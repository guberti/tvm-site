{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%%bash\n# Installs the latest dev build of TVM from PyPI. If you wish to build\n# from source, see https://tvm.apache.org/docs/install/from_source.html\npip install apache-tvm --pre"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Optimizing Operators with Auto-scheduling\n=========================================\n\n**Author**: [Lianmin Zheng](https://github.com/merrymercy), [Chengfan\nJia](https://github.com/jcf94/)\n\nIn this tutorial, we will show how TVM\\'s Auto Scheduling feature can\nfind optimal schedules without the need for writing a custom template.\n\nDifferent from the template-based\n`AutoTVM <autotvm_matmul_x86>`{.interpreted-text role=\"doc\"} which\nrelies on manual templates to define the search space, the\nauto-scheduler does not require any templates. Users only need to write\nthe computation declaration without any schedule commands or templates.\nThe auto-scheduler can automatically generate a large search space and\nfind a good schedule in the space.\n\nWe use matrix multiplication as an example in this tutorial.\n\n::: {.note}\n::: {.admonition-title}\nNote\n:::\n\nNote that this tutorial will not run on Windows or recent versions of\nmacOS. To get it to run, you will need to wrap the body of this tutorial\nin a `if\n__name__ == \"__main__\":`{.sourceCode} block.\n:::\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import numpy as np\nimport tvm\nfrom tvm import te, auto_scheduler"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Defining the Matrix Multiplication\n==================================\n\nTo start, we define a matrix multiplication with a bias addition. Note\nthat this uses standard operations available in TVMs Tensor Expression\nlanguage. The major difference is the use of the\n`register_workload`{.interpreted-text role=\"any\"} decorator at the top\nof the function definition. The function should return a list of\ninput/output tensors. From these tensors, the auto-scheduler can get the\nwhole computational graph.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "@auto_scheduler.register_workload  # Note the auto_scheduler decorator\ndef matmul_add(N, L, M, dtype):\n    A = te.placeholder((N, L), name=\"A\", dtype=dtype)\n    B = te.placeholder((L, M), name=\"B\", dtype=dtype)\n    C = te.placeholder((N, M), name=\"C\", dtype=dtype)\n\n    k = te.reduce_axis((0, L), name=\"k\")\n    matmul = te.compute(\n        (N, M),\n        lambda i, j: te.sum(A[i, k] * B[k, j], axis=k),\n        name=\"matmul\",\n        attrs={\"layout_free_placeholders\": [B]},  # enable automatic layout transform for tensor B\n    )\n    out = te.compute((N, M), lambda i, j: matmul[i, j] + C[i, j], name=\"out\")\n\n    return [A, B, C, out]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Create the search task\n======================\n\nWith the function defined, we can now create the task for the\nauto\\_scheduler to search against. We specify the particular parameters\nfor this matrix multiplication, in this case a multiplication of two\nsquare matrices of size 1024x1024. We then create a search task with\nN=L=M=1024 and dtype=\\\"float32\\\"\n\n::: {.admonition}\nImprove performance with custom targets\n\nIn order for TVM to take full advantage of specific hardware platforms,\nyou will want to manually specify your CPU capabilities. For example:\n\n> -   replace `llvm` below with `llvm -mcpu=core-avx2` to enable AVX2\n> -   replace `llvm` below with `llvm -mcpu=skylake-avx512` to enable\n>     AVX-512\n:::\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "target = tvm.target.Target(\"llvm\")\nN = L = M = 1024\ntask = tvm.auto_scheduler.SearchTask(func=matmul_add, args=(N, L, M, \"float32\"), target=target)\n\n# Inspect the computational graph\nprint(\"Computational DAG:\")\nprint(task.compute_dag)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Set Parameters for Auto-Scheduler\n=================================\n\nNext, we set parameters for the auto-scheduler.\n\n-   `num_measure_trials`{.sourceCode} is the number of measurement\n    trials we can use during the search. We only make 10 trials in this\n    tutorial for a fast demonstration. In practice, 1000 is a good value\n    for the search to converge. You can do more trials according to your\n    time budget.\n-   In addition, we use\n    `RecordToFile <auto_scheduler.RecordToFile>`{.interpreted-text\n    role=\"any\"} to log measurement records into a file `matmul.json`.\n    The measurement records can be used to query the history best,\n    resume the search, and do more analyses later.\n-   see `TuningOptions <auto_scheduler.TuningOptions>`{.interpreted-text\n    role=\"any\"} for more parameters\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "log_file = \"matmul.json\"\ntune_option = auto_scheduler.TuningOptions(\n    num_measure_trials=10,\n    measure_callbacks=[auto_scheduler.RecordToFile(log_file)],\n    verbose=2,\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Run the search\n==============\n\nNow we get all inputs ready. Pretty simple, isn\\'t it? We can kick off\nthe search and let the auto-scheduler do its magic. After some\nmeasurement trials, we can load the best schedule from the log file and\napply it.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Run auto-tuning (search)\ntask.tune(tune_option)\n# Apply the best schedule\nsch, args = task.apply_best(log_file)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Inspecting the Optimized Schedule\n=================================\n\nWe can lower the schedule to see the IR after auto-scheduling. The\nauto-scheduler correctly performs optimizations including multi-level\ntiling, layout transformation, parallelization, vectorization,\nunrolling, and operator fusion.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(\"Lowered TIR:\")\nprint(tvm.lower(sch, args, simple_mode=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Check correctness and evaluate performance\n==========================================\n\nWe build the binary and check its correctness and performance.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "func = tvm.build(sch, args, target)\na_np = np.random.uniform(size=(N, L)).astype(np.float32)\nb_np = np.random.uniform(size=(L, M)).astype(np.float32)\nc_np = np.random.uniform(size=(N, M)).astype(np.float32)\nout_np = a_np.dot(b_np) + c_np\n\ndev = tvm.cpu()\na_tvm = tvm.nd.array(a_np, device=dev)\nb_tvm = tvm.nd.array(b_np, device=dev)\nc_tvm = tvm.nd.array(c_np, device=dev)\nout_tvm = tvm.nd.empty(out_np.shape, device=dev)\nfunc(a_tvm, b_tvm, c_tvm, out_tvm)\n\n# Check results\nnp.testing.assert_allclose(out_np, out_tvm.numpy(), rtol=1e-3)\n\n# Evaluate execution time.\nevaluator = func.time_evaluator(func.entry_name, dev, min_repeat_ms=500)\nprint(\n    \"Execution time of this operator: %.3f ms\"\n    % (np.median(evaluator(a_tvm, b_tvm, c_tvm, out_tvm).results) * 1000)\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Using the record file\n=====================\n\nDuring the search, all measurement records are logged into the record\nfile `matmul.json`\\`. The measurement records can be used to re-apply\nsearch results, resume the search, and perform other analyses.\n\nHere is an example where we load the best schedule from a file, and\nprint the equivalent python schedule API. This can be used for debugging\nand learning the behavior of the auto-scheduler.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(\"Equivalent python schedule:\")\nprint(task.print_best(log_file))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "A more complicated example is to resume the search. In this case, we\nneed to create the search policy and cost model by ourselves and resume\nthe status of search policy and cost model with the log file. In the\nexample below we resume the status and do more 5 trials.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def resume_search(task, log_file):\n    print(\"Resume search:\")\n    cost_model = auto_scheduler.XGBModel()\n    cost_model.update_from_file(log_file)\n    search_policy = auto_scheduler.SketchPolicy(\n        task, cost_model, init_search_callbacks=[auto_scheduler.PreloadMeasuredStates(log_file)]\n    )\n    tune_option = auto_scheduler.TuningOptions(\n        num_measure_trials=5, measure_callbacks=[auto_scheduler.RecordToFile(log_file)]\n    )\n    task.tune(tune_option, search_policy=search_policy)\n\n\nresume_search(task, log_file)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Final Notes and Summary\n=======================\n\nIn this tutorial, we have shown how to use the TVM Auto-Scheduler to\nautomatically optimize a matrix multiplication, without the need to\nspecify a search template. It ends a series of examples that starts from\nthe Tensor Expression (TE) language that demonstrates how TVM can\noptimize computational operations.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}