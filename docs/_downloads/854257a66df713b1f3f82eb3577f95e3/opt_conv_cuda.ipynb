{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%%bash\n# Installs the latest dev build of TVM from PyPI, with CUDA enabled. To use this,\n# you must request a Google Colab instance with a GPU by going to Runtime ->\n# Change runtime type -> Hardware accelerator -> GPU. If you wish to build from\n# source, see see https://tvm.apache.org/docs/install/from_source.html\npip install tlcpack-nightly-cu113 --pre -f https://tlcpack.ai/wheels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "How to optimize convolution on GPU {#opt-conv-gpu}\n==================================\n\n**Author**: [Haichen Shen](https://homes.cs.washington.edu/~haichen/)\n\nIn this tutorial, we will demonstrate how to write a high performance\nconvolution implementation in TVM. We use square size input tensors and\nfilters as an example, and assume the input to convolution has a large\nbatch. In this example, we use a different layout to store the data in\norder to achieve better data locality. The buffer layout is HWCN, which\nstands for height, width, channel, batch.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Preparation and Algorithm\n=========================\n\nWe use the fixed size for input tensors with 256 channels and 14 x 14\ndimensions. The batch size is 256. Convolution filters contain 512\nfilters of size 3 x 3. We use stride size 1 and padding size 1 for the\nconvolution. The following code defines the convolution algorithm in\nTVM.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import numpy as np\nimport tvm\nfrom tvm import te\n\n# The sizes of inputs and filters\nbatch = 256\nin_channel = 256\nout_channel = 512\nin_size = 14\nkernel = 3\npad = 1\nstride = 1\n\n# Algorithm\nA = te.placeholder((in_size, in_size, in_channel, batch), name=\"A\")\nW = te.placeholder((kernel, kernel, in_channel, out_channel), name=\"W\")\nout_size = (in_size - kernel + 2 * pad) // stride + 1\n# Pad input\nApad = te.compute(\n    (in_size + 2 * pad, in_size + 2 * pad, in_channel, batch),\n    lambda yy, xx, cc, nn: tvm.tir.if_then_else(\n        tvm.tir.all(yy >= pad, yy - pad < in_size, xx >= pad, xx - pad < in_size),\n        A[yy - pad, xx - pad, cc, nn],\n        tvm.tir.const(0.0, \"float32\"),\n    ),\n    name=\"Apad\",\n)\n# Create reduction variables\nrc = te.reduce_axis((0, in_channel), name=\"rc\")\nry = te.reduce_axis((0, kernel), name=\"ry\")\nrx = te.reduce_axis((0, kernel), name=\"rx\")\n# Compute the convolution\nB = te.compute(\n    (out_size, out_size, out_channel, batch),\n    lambda yy, xx, ff, nn: te.sum(\n        Apad[yy * stride + ry, xx * stride + rx, rc, nn] * W[ry, rx, rc, ff], axis=[ry, rx, rc]\n    ),\n    name=\"B\",\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Memory Hierarchy\n================\n\nWe first specify the memory hierarchy for buffers. The figure below\nshows the GPU memory hierarchy. One important difference from CPU memory\nhierarchy is that GPU provides a cache buffer called shared memory,\nwhich is managed by programmers. Thus how to maximize the data reuse in\nthe shared memory is critical to achieve high performance in GPU\nkernels.\n\n![image](https://github.com/dmlc/web-data/raw/main/tvm/tutorial/gpu_memory_hierarchy.png){.align-center\nwidth=\"271px\" height=\"319px\"}\n\nIn this example, we load both Apad and W into buffer AA and WW, which\nare stored in the shared memory. These buffers will be later shared by\nall threads within the same thread block to compute the convolution.\nEach thread then loads its own part from shared buffer into their local\nregisters, AL and WL. BL is a local cache of output B, which is also\nstored in the thread local registers.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Designate the memory hierarchy\ns = te.create_schedule(B.op)\ns[Apad].compute_inline()  # compute Apad inline\nAA = s.cache_read(Apad, \"shared\", [B])\nWW = s.cache_read(W, \"shared\", [B])\nAL = s.cache_read(AA, \"local\", [B])\nWL = s.cache_read(WW, \"local\", [B])\nBL = s.cache_write(B, \"local\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Blocking\n========\n\nThe following code splits the workload into thread blocks and individual\nthreads. We follow the blocking scheme in the matrix multiply. As shown\nin the figure below, given a pixel coordinate (y, x), a thread block is\nresponsible for computing a region of block\\_factor x block\\_factor (64\nx 64) for output channels and batch. Due to the limit of shared memory\nspace, we only load step x block\\_factor (8 x 64) data from Apad and B\neach time to buffers in the shared memory.\n\n![image](https://github.com/dmlc/web-data/raw/main/tvm/tutorial/conv_gpu_blocking.png){.align-center\nwidth=\"317px\" height=\"308px\"}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# tile consts\ntile = 8\nnum_thread = 8\nblock_factor = tile * num_thread\nstep = 8\nvthread = 2\n\n# Get the GPU thread indices\nblock_x = te.thread_axis(\"blockIdx.x\")\nblock_y = te.thread_axis(\"blockIdx.y\")\nblock_z = te.thread_axis(\"blockIdx.z\")\nthread_x = te.thread_axis((0, num_thread), \"threadIdx.x\")\nthread_y = te.thread_axis((0, num_thread), \"threadIdx.y\")\nthread_xz = te.thread_axis((0, vthread), \"vthread\", name=\"vx\")\nthread_yz = te.thread_axis((0, vthread), \"vthread\", name=\"vy\")\n\n# Split the workloads\nhi, wi, fi, ni = s[B].op.axis\nbz = s[B].fuse(hi, wi)\nby, fi = s[B].split(fi, factor=block_factor)\nbx, ni = s[B].split(ni, factor=block_factor)\n\n# Bind the iteration variables to GPU thread indices\ns[B].bind(bz, block_z)\ns[B].bind(by, block_y)\ns[B].bind(bx, block_x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Virtual Thread Split\n====================\n\nWe further split the workload from a thread block to individual threads.\nTo avoid *memory bank conflict*, we use virtual thread to split the area\ninto 4 parts, and then tile into 8x8 grids. Therefore, shown in the\nfigure below, each thread computes 4 strided grids, where size of each\ngrid is 4 x 4.\n\n![image](https://github.com/dmlc/web-data/raw/main/tvm/tutorial/conv_gpu_vthread.png){.align-center\nwidth=\"268px\" height=\"188px\"}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "tyz, fi = s[B].split(fi, nparts=vthread)  # virtual thread split\ntxz, ni = s[B].split(ni, nparts=vthread)  # virtual thread split\nty, fi = s[B].split(fi, nparts=num_thread)\ntx, ni = s[B].split(ni, nparts=num_thread)\ns[B].reorder(bz, by, bx, tyz, txz, ty, tx, fi, ni)\n\ns[B].bind(tyz, thread_yz)\ns[B].bind(txz, thread_xz)\ns[B].bind(ty, thread_y)\ns[B].bind(tx, thread_x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Cooperative Fetching\n====================\n\nAs mentioned before, each time step we need to transfer step x\nblock\\_factor data from GPU global memory to shared memory. In order to\nreduce the memory transfer per thread, the following code lets threads\nin the same thread block coopertively fetch dependent data from global\nmemory.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Schedule BL local write\ns[BL].compute_at(s[B], tx)\nyi, xi, fi, ni = s[BL].op.axis\nry, rx, rc = s[BL].op.reduce_axis\nrco, rci = s[BL].split(rc, factor=step)\ns[BL].reorder(rco, ry, rx, rci, fi, ni)\n\n# Attach computation to iteration variables\ns[AA].compute_at(s[BL], rx)\ns[WW].compute_at(s[BL], rx)\ns[AL].compute_at(s[BL], rci)\ns[WL].compute_at(s[BL], rci)\n\n# Schedule for A's shared memory load\nyi, xi, ci, ni = s[AA].op.axis\nty, ci = s[AA].split(ci, nparts=num_thread)\ntx, ni = s[AA].split(ni, nparts=num_thread)\n_, ni = s[AA].split(ni, factor=4)\ns[AA].reorder(ty, tx, yi, xi, ci, ni)\ns[AA].bind(ty, thread_y)\ns[AA].bind(tx, thread_x)\ns[AA].vectorize(ni)  # vectorize memory load\n\n# Schedule for W's shared memory load\nyi, xi, ci, fi = s[WW].op.axis\nty, ci = s[WW].split(ci, nparts=num_thread)\ntx, fi = s[WW].split(fi, nparts=num_thread)\n_, fi = s[WW].split(fi, factor=4)\ns[WW].reorder(ty, tx, yi, xi, ci, fi)\ns[WW].bind(ty, thread_y)\ns[WW].bind(tx, thread_x)\ns[WW].vectorize(fi)  # vectorize memory load"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Generate CUDA Kernel\n====================\n\nFinally we use TVM to generate and compile the CUDA kernel, and evaluate\nthe latency of convolution.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "func = tvm.build(s, [A, W, B], \"cuda\")\ndev = tvm.cuda(0)\na_np = np.random.uniform(size=(in_size, in_size, in_channel, batch)).astype(A.dtype)\nw_np = np.random.uniform(size=(kernel, kernel, in_channel, out_channel)).astype(W.dtype)\na = tvm.nd.array(a_np, dev)\nw = tvm.nd.array(w_np, dev)\nb = tvm.nd.array(np.zeros((out_size, out_size, out_channel, batch), dtype=B.dtype), dev)\nfunc(a, w, b)\nevaluator = func.time_evaluator(func.entry_name, dev, number=1)\nprint(\"Convolution: %f ms\" % (evaluator(a, w, b).mean * 1e3))"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}