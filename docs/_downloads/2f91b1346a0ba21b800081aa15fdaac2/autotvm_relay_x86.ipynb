{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%%bash\n# Installs the latest dev build of TVM from PyPI. If you wish to build\n# from source, see https://tvm.apache.org/docs/install/from_source.html\npip install apache-tvm --pre"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Compiling and Optimizing a Model with the Python Interface (AutoTVM)\n====================================================================\n\n**Author**: [Chris Hoge](https://github.com/hogepodge)\n\nIn the [TVMC Tutorial](tvmc_command_line_driver), we covered how to\ncompile, run, and tune a pre-trained vision model, ResNet-50 v2 using\nthe command line interface for TVM, TVMC. TVM is more that just a\ncommand-line tool though, it is an optimizing framework with APIs\navailable for a number of different languages that gives you tremendous\nflexibility in working with machine learning models.\n\nIn this tutorial we will cover the same ground we did with TVMC, but\nshow how it is done with the Python API. Upon completion of this\nsection, we will have used the Python API for TVM to accomplish the\nfollowing tasks:\n\n-   Compile a pre-trained ResNet-50 v2 model for the TVM runtime.\n-   Run a real image through the compiled model, and interpret the\n    output and model performance.\n-   Tune the model that model on a CPU using TVM.\n-   Re-compile an optimized model using the tuning data collected by\n    TVM.\n-   Run the image through the optimized model, and compare the output\n    and model performance.\n\nThe goal of this section is to give you an overview of TVM\\'s\ncapabilites and how to use them through the Python API.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "TVM is a deep learning compiler framework, with a number of different\nmodules available for working with deep learning models and operators.\nIn this tutorial we will work through how to load, compile, and optimize\na model using the Python API.\n\nWe begin by importing a number of dependencies, including `onnx` for\nloading and converting the model, helper utilities for downloading test\ndata, the Python Image Library for working with the image data, `numpy`\nfor pre and post-processing of the image data, the TVM Relay framework,\nand the TVM Graph Executor.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import onnx\nfrom tvm.contrib.download import download_testdata\nfrom PIL import Image\nimport numpy as np\nimport tvm.relay as relay\nimport tvm\nfrom tvm.contrib import graph_executor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Downloading and Loading the ONNX Model\n======================================\n\nFor this tutorial, we will be working with ResNet-50 v2. ResNet-50 is a\nconvolutional neural network that is 50 layers deep and designed to\nclassify images. The model we will be using has been pre-trained on more\nthan a million images with 1000 different classifications. The network\nhas an input image size of 224x224. If you are interested exploring more\nof how the ResNet-50 model is structured, we recommend downloading\n[Netron](https://netron.app), a freely available ML model viewer.\n\nTVM provides a helper library to download pre-trained models. By\nproviding a model URL, file name, and model type through the module, TVM\nwill download the model and save it to disk. For the instance of an ONNX\nmodel, you can then load it into memory using the ONNX runtime.\n\n::: {.admonition}\nWorking with Other Model Formats\n\nTVM supports many popular model formats. A list can be found in the\n`Compile Deep Learning Models <tutorial-frontend>`{.interpreted-text\nrole=\"ref\"} section of the TVM Documentation.\n:::\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "model_url = (\n    \"https://github.com/onnx/models/raw/main/\"\n    \"vision/classification/resnet/model/\"\n    \"resnet50-v2-7.onnx\"\n)\n\nmodel_path = download_testdata(model_url, \"resnet50-v2-7.onnx\", module=\"onnx\")\nonnx_model = onnx.load(model_path)\n\n# Seed numpy's RNG to get consistent results\nnp.random.seed(0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Downloading, Preprocessing, and Loading the Test Image\n======================================================\n\nEach model is particular when it comes to expected tensor shapes,\nformats and data types. For this reason, most models require some pre\nand post-processing, to ensure the input is valid and to interpret the\noutput. TVMC has adopted NumPy\\'s `.npz` format for both input and\noutput data.\n\nAs input for this tutorial, we will use the image of a cat, but you can\nfeel free to substitute this image for any of your choosing.\n\n![image](https://s3.amazonaws.com/model-server/inputs/kitten.jpg){.align-center\nwidth=\"224px\" height=\"224px\"}\n\nDownload the image data, then convert it to a numpy array to use as an\ninput to the model.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "img_url = \"https://s3.amazonaws.com/model-server/inputs/kitten.jpg\"\nimg_path = download_testdata(img_url, \"imagenet_cat.png\", module=\"data\")\n\n# Resize it to 224x224\nresized_image = Image.open(img_path).resize((224, 224))\nimg_data = np.asarray(resized_image).astype(\"float32\")\n\n# Our input image is in HWC layout while ONNX expects CHW input, so convert the array\nimg_data = np.transpose(img_data, (2, 0, 1))\n\n# Normalize according to the ImageNet input specification\nimagenet_mean = np.array([0.485, 0.456, 0.406]).reshape((3, 1, 1))\nimagenet_stddev = np.array([0.229, 0.224, 0.225]).reshape((3, 1, 1))\nnorm_img_data = (img_data / 255 - imagenet_mean) / imagenet_stddev\n\n# Add the batch dimension, as we are expecting 4-dimensional input: NCHW.\nimg_data = np.expand_dims(norm_img_data, axis=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Compile the Model With Relay\n============================\n\nThe next step is to compile the ResNet model. We begin by importing the\nmodel to relay using the [from\\_onnx]{.title-ref} importer. We then\nbuild the model, with standard optimizations, into a TVM library.\nFinally, we create a TVM graph runtime module from the library.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "target = \"llvm\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "::: {.admonition}\nDefining the Correct Target\n\nSpecifying the correct target can have a huge impact on the performance\nof the compiled module, as it can take advantage of hardware features\navailable on the target. For more information, please refer to\n`Auto-tuning a convolutional network for x86 CPU <tune_relay_x86>`{.interpreted-text\nrole=\"ref\"}. We recommend identifying which CPU you are running, along\nwith optional features, and set the target appropriately. For example,\nfor some processors `target = \"llvm -mcpu=skylake\"`, or\n`target = \"llvm -mcpu=skylake-avx512\"` for processors with the AVX-512\nvector instruction set.\n:::\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# The input name may vary across model types. You can use a tool\n# like Netron to check input names\ninput_name = \"data\"\nshape_dict = {input_name: img_data.shape}\n\nmod, params = relay.frontend.from_onnx(onnx_model, shape_dict)\n\nwith tvm.transform.PassContext(opt_level=3):\n    lib = relay.build(mod, target=target, params=params)\n\ndev = tvm.device(str(target), 0)\nmodule = graph_executor.GraphModule(lib[\"default\"](dev))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Execute on the TVM Runtime\n==========================\n\nNow that we\\'ve compiled the model, we can use the TVM runtime to make\npredictions with it. To use TVM to run the model and make predictions,\nwe need two things:\n\n-   The compiled model, which we just produced.\n-   Valid input to the model to make predictions on.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "dtype = \"float32\"\nmodule.set_input(input_name, img_data)\nmodule.run()\noutput_shape = (1, 1000)\ntvm_output = module.get_output(0, tvm.nd.empty(output_shape)).numpy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Collect Basic Performance Data\n==============================\n\nWe want to collect some basic performance data associated with this\nunoptimized model and compare it to a tuned model later. To help account\nfor CPU noise, we run the computation in multiple batches in multiple\nrepetitions, then gather some basis statistics on the mean, median, and\nstandard deviation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import timeit\n\ntiming_number = 10\ntiming_repeat = 10\nunoptimized = (\n    np.array(timeit.Timer(lambda: module.run()).repeat(repeat=timing_repeat, number=timing_number))\n    * 1000\n    / timing_number\n)\nunoptimized = {\n    \"mean\": np.mean(unoptimized),\n    \"median\": np.median(unoptimized),\n    \"std\": np.std(unoptimized),\n}\n\nprint(unoptimized)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Postprocess the output\n======================\n\nAs previously mentioned, each model will have its own particular way of\nproviding output tensors.\n\nIn our case, we need to run some post-processing to render the outputs\nfrom ResNet-50 v2 into a more human-readable form, using the\nlookup-table provided for the model.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from scipy.special import softmax\n\n# Download a list of labels\nlabels_url = \"https://s3.amazonaws.com/onnx-model-zoo/synset.txt\"\nlabels_path = download_testdata(labels_url, \"synset.txt\", module=\"data\")\n\nwith open(labels_path, \"r\") as f:\n    labels = [l.rstrip() for l in f]\n\n# Open the output and read the output tensor\nscores = softmax(tvm_output)\nscores = np.squeeze(scores)\nranks = np.argsort(scores)[::-1]\nfor rank in ranks[0:5]:\n    print(\"class='%s' with probability=%f\" % (labels[rank], scores[rank]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This should produce the following output:\n\n``` {.sourceCode .bash}\n# class='n02123045 tabby, tabby cat' with probability=0.610553\n# class='n02123159 tiger cat' with probability=0.367179\n# class='n02124075 Egyptian cat' with probability=0.019365\n# class='n02129604 tiger, Panthera tigris' with probability=0.001273\n# class='n04040759 radiator' with probability=0.000261\n```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Tune the model\n==============\n\nThe previous model was compiled to work on the TVM runtime, but did not\ninclude any platform specific optimization. In this section, we will\nshow you how to build an optimized model using TVM to target your\nworking platform.\n\nIn some cases, we might not get the expected performance when running\ninferences using our compiled module. In cases like this, we can make\nuse of the auto-tuner, to find a better configuration for our model and\nget a boost in performance. Tuning in TVM refers to the process by which\na model is optimized to run faster on a given target. This differs from\ntraining or fine-tuning in that it does not affect the accuracy of the\nmodel, but only the runtime performance. As part of the tuning process,\nTVM will try running many different operator implementation variants to\nsee which perform best. The results of these runs are stored in a tuning\nrecords file.\n\nIn the simplest form, tuning requires you to provide three things:\n\n-   the target specification of the device you intend to run this model\n    on\n-   the path to an output file in which the tuning records will be\n    stored\n-   a path to the model to be tuned.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import tvm.auto_scheduler as auto_scheduler\nfrom tvm.autotvm.tuner import XGBTuner\nfrom tvm import autotvm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Set up some basic parameters for the runner. The runner takes compiled\ncode that is generated with a specific set of parameters and measures\nthe performance of it. `number` specifies the number of different\nconfigurations that we will test, while `repeat` specifies how many\nmeasurements we will take of each configuration. `min_repeat_ms` is a\nvalue that specifies how long need to run configuration test. If the\nnumber of repeats falls under this time, it will be increased. This\noption is necessary for accurate tuning on GPUs, and is not required for\nCPU tuning. Setting this value to 0 disables it. The `timeout` places an\nupper limit on how long to run training code for each tested\nconfiguration.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "number = 10\nrepeat = 1\nmin_repeat_ms = 0  # since we're tuning on a CPU, can be set to 0\ntimeout = 10  # in seconds\n\n# create a TVM runner\nrunner = autotvm.LocalRunner(\n    number=number,\n    repeat=repeat,\n    timeout=timeout,\n    min_repeat_ms=min_repeat_ms,\n    enable_cpu_cache_flush=True,\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Create a simple structure for holding tuning options. We use an XGBoost\nalgorithim for guiding the search. For a production job, you will want\nto set the number of trials to be larger than the value of 20 used here.\nFor CPU we recommend 1500, for GPU 3000-4000. The number of trials\nrequired can depend on the particular model and processor, so it\\'s\nworth spending some time evaluating performance across a range of values\nto find the best balance between tuning time and model optimization.\nBecause running tuning is time intensive we set number of trials to 10,\nbut do not recommend a value this small. The `early_stopping` parameter\nis the minimum number of trails to run before a condition that stops the\nsearch early can be applied. The measure option indicates where trial\ncode will be built, and where it will be run. In this case, we\\'re using\nthe `LocalRunner` we just created and a `LocalBuilder`. The\n`tuning_records` option specifies a file to write the tuning data to.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "tuning_option = {\n    \"tuner\": \"xgb\",\n    \"trials\": 20,\n    \"early_stopping\": 100,\n    \"measure_option\": autotvm.measure_option(\n        builder=autotvm.LocalBuilder(build_func=\"default\"), runner=runner\n    ),\n    \"tuning_records\": \"resnet-50-v2-autotuning.json\",\n}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "::: {.admonition}\nDefining the Tuning Search Algorithm\n\nBy default this search is guided using an [XGBoost Grid]{.title-ref}\nalgorithm. Depending on your model complexity and amount of time\navailable, you might want to choose a different algorithm.\n:::\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "::: {.admonition}\nSetting Tuning Parameters\n\nIn this example, in the interest of time, we set the number of trials\nand early stopping to 10. You will likely see more performance\nimprovements if you set these values to be higher but this comes at the\nexpense of time spent tuning. The number of trials required for\nconvergence will vary depending on the specifics of the model and the\ntarget platform.\n:::\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# begin by extracting the tasks from the onnx model\ntasks = autotvm.task.extract_from_program(mod[\"main\"], target=target, params=params)\n\n# Tune the extracted tasks sequentially.\nfor i, task in enumerate(tasks):\n    prefix = \"[Task %2d/%2d] \" % (i + 1, len(tasks))\n    tuner_obj = XGBTuner(task, loss_type=\"rank\")\n    tuner_obj.tune(\n        n_trial=min(tuning_option[\"trials\"], len(task.config_space)),\n        early_stopping=tuning_option[\"early_stopping\"],\n        measure_option=tuning_option[\"measure_option\"],\n        callbacks=[\n            autotvm.callback.progress_bar(tuning_option[\"trials\"], prefix=prefix),\n            autotvm.callback.log_to_file(tuning_option[\"tuning_records\"]),\n        ],\n    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The output from this tuning process will look something like this:\n\n``` {.sourceCode .bash}\n# [Task  1/24]  Current/Best:   10.71/  21.08 GFLOPS | Progress: (60/1000) | 111.77 s Done.\n# [Task  1/24]  Current/Best:    9.32/  24.18 GFLOPS | Progress: (192/1000) | 365.02 s Done.\n# [Task  2/24]  Current/Best:   22.39/ 177.59 GFLOPS | Progress: (960/1000) | 976.17 s Done.\n# [Task  3/24]  Current/Best:   32.03/ 153.34 GFLOPS | Progress: (800/1000) | 776.84 s Done.\n# [Task  4/24]  Current/Best:   11.96/ 156.49 GFLOPS | Progress: (960/1000) | 632.26 s Done.\n# [Task  5/24]  Current/Best:   23.75/ 130.78 GFLOPS | Progress: (800/1000) | 739.29 s Done.\n# [Task  6/24]  Current/Best:   38.29/ 198.31 GFLOPS | Progress: (1000/1000) | 624.51 s Done.\n# [Task  7/24]  Current/Best:    4.31/ 210.78 GFLOPS | Progress: (1000/1000) | 701.03 s Done.\n# [Task  8/24]  Current/Best:   50.25/ 185.35 GFLOPS | Progress: (972/1000) | 538.55 s Done.\n# [Task  9/24]  Current/Best:   50.19/ 194.42 GFLOPS | Progress: (1000/1000) | 487.30 s Done.\n# [Task 10/24]  Current/Best:   12.90/ 172.60 GFLOPS | Progress: (972/1000) | 607.32 s Done.\n# [Task 11/24]  Current/Best:   62.71/ 203.46 GFLOPS | Progress: (1000/1000) | 581.92 s Done.\n# [Task 12/24]  Current/Best:   36.79/ 224.71 GFLOPS | Progress: (1000/1000) | 675.13 s Done.\n# [Task 13/24]  Current/Best:    7.76/ 219.72 GFLOPS | Progress: (1000/1000) | 519.06 s Done.\n# [Task 14/24]  Current/Best:   12.26/ 202.42 GFLOPS | Progress: (1000/1000) | 514.30 s Done.\n# [Task 15/24]  Current/Best:   31.59/ 197.61 GFLOPS | Progress: (1000/1000) | 558.54 s Done.\n# [Task 16/24]  Current/Best:   31.63/ 206.08 GFLOPS | Progress: (1000/1000) | 708.36 s Done.\n# [Task 17/24]  Current/Best:   41.18/ 204.45 GFLOPS | Progress: (1000/1000) | 736.08 s Done.\n# [Task 18/24]  Current/Best:   15.85/ 222.38 GFLOPS | Progress: (980/1000) | 516.73 s Done.\n# [Task 19/24]  Current/Best:   15.78/ 203.41 GFLOPS | Progress: (1000/1000) | 587.13 s Done.\n# [Task 20/24]  Current/Best:   30.47/ 205.92 GFLOPS | Progress: (980/1000) | 471.00 s Done.\n# [Task 21/24]  Current/Best:   46.91/ 227.99 GFLOPS | Progress: (308/1000) | 219.18 s Done.\n# [Task 22/24]  Current/Best:   13.33/ 207.66 GFLOPS | Progress: (1000/1000) | 761.74 s Done.\n# [Task 23/24]  Current/Best:   53.29/ 192.98 GFLOPS | Progress: (1000/1000) | 799.90 s Done.\n# [Task 24/24]  Current/Best:   25.03/ 146.14 GFLOPS | Progress: (1000/1000) | 1112.55 s Done.\n```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Compiling an Optimized Model with Tuning Data\n=============================================\n\nAs an output of the tuning process above, we obtained the tuning records\nstored in `resnet-50-v2-autotuning.json`. The compiler will use the\nresults to generate high performance code for the model on your\nspecified target.\n\nNow that tuning data for the model has been collected, we can re-compile\nthe model using optimized operators to speed up our computations.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "with autotvm.apply_history_best(tuning_option[\"tuning_records\"]):\n    with tvm.transform.PassContext(opt_level=3, config={}):\n        lib = relay.build(mod, target=target, params=params)\n\ndev = tvm.device(str(target), 0)\nmodule = graph_executor.GraphModule(lib[\"default\"](dev))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Verify that the optimized model runs and produces the same results:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "dtype = \"float32\"\nmodule.set_input(input_name, img_data)\nmodule.run()\noutput_shape = (1, 1000)\ntvm_output = module.get_output(0, tvm.nd.empty(output_shape)).numpy()\n\nscores = softmax(tvm_output)\nscores = np.squeeze(scores)\nranks = np.argsort(scores)[::-1]\nfor rank in ranks[0:5]:\n    print(\"class='%s' with probability=%f\" % (labels[rank], scores[rank]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Verifying that the predictions are the same:\n\n``` {.sourceCode .bash}\n# class='n02123045 tabby, tabby cat' with probability=0.610550\n# class='n02123159 tiger cat' with probability=0.367181\n# class='n02124075 Egyptian cat' with probability=0.019365\n# class='n02129604 tiger, Panthera tigris' with probability=0.001273\n# class='n04040759 radiator' with probability=0.000261\n```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Comparing the Tuned and Untuned Models\n======================================\n\nWe want to collect some basic performance data associated with this\noptimized model to compare it to the unoptimized model. Depending on\nyour underlying hardware, number of iterations, and other factors, you\nshould see a performance improvement in comparing the optimized model to\nthe unoptimized model.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import timeit\n\ntiming_number = 10\ntiming_repeat = 10\noptimized = (\n    np.array(timeit.Timer(lambda: module.run()).repeat(repeat=timing_repeat, number=timing_number))\n    * 1000\n    / timing_number\n)\noptimized = {\"mean\": np.mean(optimized), \"median\": np.median(optimized), \"std\": np.std(optimized)}\n\n\nprint(\"optimized: %s\" % (optimized))\nprint(\"unoptimized: %s\" % (unoptimized))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Final Remarks\n=============\n\nIn this tutorial, we gave a short example of how to use the TVM Python\nAPI to compile, run, and tune a model. We also discussed the need for\npre and post-processing of inputs and outputs. After the tuning process,\nwe demonstrated how to compare the performance of the unoptimized and\noptimize models.\n\nHere we presented a simple example using ResNet-50 v2 locally. However,\nTVM supports many more features including cross-compilation, remote\nexecution and profiling/benchmarking.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}