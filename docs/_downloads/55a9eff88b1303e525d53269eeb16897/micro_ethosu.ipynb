{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%%bash\n# Installs the latest dev build of TVM from PyPI. If you wish to build\n# from source, see https://tvm.apache.org/docs/install/from_source.html\npip install apache-tvm --pre"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Running TVM on bare metal Arm(R) Cortex(R)-M55 CPU and Ethos(TM)-U55 NPU with CMSIS-NN\n======================================================================================\n\n**Author**: [Grant Watson](https://github.com/grant-arm)\n\nThis section contains an example of how to use TVM to run a model on an\nArm(R) Cortex(R)-M55 CPU and Ethos(TM)-U55 NPU with CMSIS-NN, using bare\nmetal. The Cortex(R)-M55 is a small, low-power CPU designed for use in\nembedded devices. CMSIS-NN is a collection of kernels optimized for\nArm(R) Cortex(R)-M CPUs. The Ethos(TM)-U55 is a microNPU, specifically\ndesigned to accelerate ML inference in resource-constrained embedded\ndevices.\n\nIn order to run the demo application without having access to a\nCortex(R)-M55 and Ethos(TM)-U55 development board, we will be running\nour sample application on a Fixed Virtual Platform (FVP). The FVP based\non Arm(R) Corstone(TM)-300 software, models a hardware system containing\na Cortex(R)-M55 and Ethos(TM)-U55. It provides a programmer\\'s view that\nis suitable for software development.\n\nIn this tutorial, we will be compiling a MobileNet v1 model and\ninstructing TVM to offload operators to the Ethos(TM)-U55 where\npossible.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Obtaining TVM\n=============\n\nTo obtain TVM for you platform, please visit <https://tlcpack.ai/> and\nfollow the instructions. Once TVM has been installed correctly, you\nshould have access to `tvmc` from the command line.\n\nTyping `tvmc` on the command line should display the following:\n\n``` {.sourceCode .text}\nusage: tvmc [-h] [-v] [--version] {tune,compile,run} ...\n\nTVM compiler driver\n\noptional arguments:\n  -h, --help          show this help message and exit\n  -v, --verbose       increase verbosity\n  --version           print the version and exit\n\ncommands:\n  {tune,compile,run}\n    tune              auto-tune a model\n    compile           compile a model.\n    run               run a compiled module\n\nTVMC - TVM driver command-line interface\n```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Installing additional python dependencies\n=========================================\n\nIn order to run the demo, you will need some additional python packages.\nThese can be installed by using the requirements.txt file below:\n\n``` {#requirements.txt .sourceCode .text}\nattrs==21.2.0\ncloudpickle==2.0.0\ndecorator==5.1.0\nethos-u-vela==3.5.0\nflatbuffers==1.12\nlxml==4.6.3\nnose==1.3.7\nnumpy==1.19.5\nPillow==8.3.2\npsutil==5.8.0\nscipy==1.5.4\nsynr==0.6\ntflite==2.4.0\ntornado==6.1\n```\n\nThese packages can be installed by running the following from the\ncommand line:\n\n``` {.sourceCode .bash}\npip install -r requirements.txt\n```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Obtaining the Model\n===================\n\nFor this tutorial, we will be working with MobileNet v1. MobileNet v1 is\na convolutional neural network designed to classify images, that has\nbeen optimized for edge devices. The model we will be using has been\npre-trained to classify images into one of 1001 different categories.\nThe network has an input image size of 224x224 so any input images will\nneed to be resized to those dimensions before being used.\n\nFor this tutorial we will be using the model in Tflite format.\n\n``` {.sourceCode .bash}\nmkdir -p ./build\ncd build\nwget https://storage.googleapis.com/download.tensorflow.org/models/mobilenet_v1_2018_08_02/mobilenet_v1_1.0_224_quant.tgz\ngunzip mobilenet_v1_1.0_224_quant.tgz\ntar xvf mobilenet_v1_1.0_224_quant.tar\n```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Compiling the model for Arm(R) Cortex(R)-M55 CPU and Ethos(TM)-U55 NPU with CMSIS-NN\n====================================================================================\n\nOnce we\\'ve downloaded the MobileNet v1 model, the next step is to\ncompile it. To accomplish that, we are going to use `tvmc compile`. The\noutput we get from the compilation process is a TAR package of the model\ncompiled to the Model Library Format (MLF) for our target platform. We\nwill be able to run that model on our target device using the TVM\nruntime.\n\n``` {.sourceCode .bash}\ntvmc compile --target=ethos-u,cmsis-nn,c \\\n             --target-ethos-u-accelerator_config=ethos-u55-256 \\\n             --target-cmsis-nn-mcpu=cortex-m55 \\\n             --target-c-mcpu=cortex-m55 \\\n             --runtime=crt \\\n             --executor=aot \\\n             --executor-aot-interface-api=c \\\n             --executor-aot-unpacked-api=1 \\\n             --pass-config tir.usmp.enable=1 \\\n             --pass-config tir.usmp.algorithm=hill_climb \\\n             --pass-config tir.disable_storage_rewrite=1 \\\n             --pass-config tir.disable_vectorize=1 \\\n             ./mobilenet_v1_1.0_224_quant.tflite \\\n             --output-format=mlf\n```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "::: {.note}\n::: {.admonition-title}\nNote\n:::\n\nExplanation of tvmc compile arguments:\n\n-   `--target=ethos-u,cmsis-nn,c` : offload operators to the microNPU\n    where possible, falling back to CMSIS-NN and finally generated C\n    code where an operator is not supported on the microNPU..\n-   `--target-ethos-u-accelerator_config=ethos-u55-256` : specifies the\n    microNPU configuration\n-   `--target-c-mcpu=cortex-m55` : Cross-compile for the Cortex(R)-M55.\n-   `--runtime=crt` : Generate glue code to allow operators to work with\n    C runtime.\n-   `--executor=aot` : Use Ahead Of Time compiltaion instead of the\n    Graph Executor.\n-   `--executor-aot-interface-api=c` : Generate a C-style interface with\n    structures designed for integrating into C apps at the boundary.\n-   `--executor-aot-unpacked-api=1` : Use the unpacked API internally.\n-   `--pass-config tir.usmp.enable=1` : Enable Unified Static Memory\n    Planning\n-   `--pass-config tir.usmp.algorithm=hill_climb` : Use the hill-climb\n    algorithm for USMP\n-   `--pass-config tir.disable_storage_rewrite=1` : Disable storage\n    rewrite\n-   `--pass-config tir.disable_vectorize=1` : Disable vectorize since\n    there are no standard vectorized types in C.\n-   `./mobilenet_v1_1.0_224_quant.tflite` : The TFLite model that is\n    being compiled.\n-   `--output-format=mlf` : Output should be generated in the Model\n    Library Format.\n:::\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "::: {.note}\n::: {.admonition-title}\nNote\n:::\n\nIf you don\\'t want to make use of the microNPU and want to offload\noperators to CMSIS-NN only:\n\n-   Use `--target=cmsis-nn,c` in place of `--target=ethos-u,cmsis-nn,c`\n-   Remove the microNPU config parameter\n    `--target-ethos-u-accelerator_config=ethos-u55-256`\n:::\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Extracting the generated code into the current directory\n========================================================\n\n``` {.sourceCode .bash}\ntar xvf module.tar\n```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Getting ImageNet labels\n=======================\n\nWhen running MobileNet v1 on an image, the result is an index in the\nrange 0 to 1000. In order to make our application a little more user\nfriendly, instead of just displaying the category index, we will display\nthe associated label. We will download these image labels into a text\nfile now and use a python script to include them in our C application\nlater.\n\n``` {.sourceCode .bash}\ncurl -sS  https://raw.githubusercontent.com/tensorflow/tensorflow/master/tensorflow/lite/java/demo/app/src/main/assets/labels_mobilenet_quant_v1_224.txt \\\n-o ./labels_mobilenet_quant_v1_224.txt\n```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Getting the input image\n=======================\n\nAs input for this tutorial, we will use the image of a cat, but you can\nsubstitute an image of your choosing.\n\n![image](https://s3.amazonaws.com/model-server/inputs/kitten.jpg){.align-center\nwidth=\"224px\" height=\"224px\"}\n\nWe download the image into the build directory and we will use a python\nscript in the next step to convert the image into an array of bytes in a\nC header file.\n\n``` {.sourceCode .bash}\ncurl -sS https://s3.amazonaws.com/model-server/inputs/kitten.jpg -o ./kitten.jpg\n```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Pre-processing the image\n========================\n\nThe following script will create 2 C header files in the src directory:\n\n-   `inputs.h` - The image supplied as an argument to the script will be\n    converted to an array of integers for input to our MobileNet v1\n    model.\n-   `outputs.h` - An integer array of zeroes will reserve 1001 integer\n    values for the output of inference.\n\n``` {#convert_image.py .sourceCode .python}\n#!python ./convert_image.py\nimport os\nimport pathlib\nimport re\nimport sys\nfrom PIL import Image\nimport numpy as np\n\n\ndef create_header_file(name, section, tensor_name, tensor_data, output_path):\n    \"\"\"\n    This function generates a header file containing the data from the numpy array provided.\n    \"\"\"\n    file_path = pathlib.Path(f\"{output_path}/\" + name).resolve()\n    # Create header file with npy_data as a C array\n    raw_path = file_path.with_suffix(\".h\").resolve()\n    with open(raw_path, \"w\") as header_file:\n        header_file.write(\n            \"#include <tvmgen_default.h>\\n\"\n            + f\"const size_t {tensor_name}_len = {tensor_data.size};\\n\"\n            + f'uint8_t {tensor_name}[] __attribute__((section(\"{section}\"), aligned(16))) = \"'\n        )\n        data_hexstr = tensor_data.tobytes().hex()\n        for i in range(0, len(data_hexstr), 2):\n            header_file.write(f\"\\\\x{data_hexstr[i:i+2]}\")\n        header_file.write('\";\\n\\n')\n\n\ndef create_headers(image_name):\n    \"\"\"\n    This function generates C header files for the input and output arrays required to run inferences\n    \"\"\"\n    img_path = os.path.join(\"./\", f\"{image_name}\")\n\n    # Resize image to 224x224\n    resized_image = Image.open(img_path).resize((224, 224))\n    img_data = np.asarray(resized_image).astype(\"float32\")\n\n    # Convert input to NCHW\n    img_data = np.transpose(img_data, (2, 0, 1))\n\n    # Create input header file\n    input_data = img_data.astype(np.uint8)\n    create_header_file(\"inputs\", \"ethosu_scratch\", \"input\", input_data, \"./include\")\n    # Create output header file\n    output_data = np.zeros([1001], np.uint8)\n    create_header_file(\n        \"outputs\",\n        \"output_data_sec\",\n        \"output\",\n        output_data,\n        \"./include\",\n    )\n\n\nif __name__ == \"__main__\":\n    create_headers(sys.argv[1])\n```\n\nRun the script from the command line:\n\n``` {.sourceCode .bash}\npython convert_image.py ./kitten.jpg\n```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Pre-processing the labels\n=========================\n\nThe following script will create a `labels.h` header file in the src\ndirectory. The labels.txt file that we downloaded previously will be\nturned into an array of strings. This array will be used to display the\nlabel that our image has been classified as.\n\n``` {#convert_labels.py .sourceCode .python}\n#!python ./convert_labels.py\nimport os\nimport pathlib\nimport sys\n\n\ndef create_labels_header(labels_file, section, output_path):\n    \"\"\"\n    This function generates a header file containing the ImageNet labels as an array of strings\n    \"\"\"\n    labels_path = pathlib.Path(labels_file).resolve()\n    file_path = pathlib.Path(f\"{output_path}/labels.h\").resolve()\n\n    with open(labels_path) as f:\n        labels = f.readlines()\n\n    with open(file_path, \"w\") as header_file:\n        header_file.write(f'char* labels[] __attribute__((section(\"{section}\"), aligned(16))) = {{')\n\n        for _, label in enumerate(labels):\n            header_file.write(f'\"{label.rstrip()}\",')\n\n        header_file.write(\"};\\n\")\n\n\nif __name__ == \"__main__\":\n    create_labels_header(sys.argv[1], \"ethosu_scratch\", \"./include\")\n```\n\nRun the script from the command line:\n\n``` {.sourceCode .bash}\npython convert_labels.py\n```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Writing the demo application\n============================\n\nThe following C application will run a single inference of the MobileNet\nv1 model on the image that we downloaded and converted to an array of\nintegers previously. Since the model was compiled with a target of\n\\\"ethos-u \\...\\\", operators supported by the Ethos(TM)-U55 NPU will be\noffloaded for acceleration. Once the application is built and run, our\ntest image should be correctly classied as a \\\"tabby\\\" and the result\nshould be displayed on the console. This file should be placed in\n`./src`\n\n``` {#demo.c .sourceCode .c}\n#include <stdio.h>\n#include <tvm_runtime.h>\n\n#include \"ethosu_mod.h\"\n#include \"uart.h\"\n\n// Header files generated by convert_image.py and convert_labels.py\n#include \"inputs.h\"\n#include \"labels.h\"\n#include \"outputs.h\"\n\nint abs(int v) { return v * ((v > 0) - (v < 0)); }\n\nint main(int argc, char** argv) {\n  uart_init();\n  printf(\"Starting Demo\\n\");\n  EthosuInit();\n\n  printf(\"Allocating memory\\n\");\n  StackMemoryManager_Init(&app_workspace, g_aot_memory, WORKSPACE_SIZE);\n\n  printf(\"Running inference\\n\");\n  struct tvmgen_default_outputs outputs = {\n      .output = output,\n  };\n  struct tvmgen_default_inputs inputs = {\n      .input = input,\n  };\n  struct ethosu_driver* driver = ethosu_reserve_driver();\n  struct tvmgen_default_devices devices = {\n      .ethos_u = driver,\n  };\n  tvmgen_default_run(&inputs, &outputs, &devices);\n  ethosu_release_driver(driver);\n\n  // Calculate index of max value\n  uint8_t max_value = 0;\n  int32_t max_index = -1;\n  for (unsigned int i = 0; i < output_len; ++i) {\n    if (output[i] > max_value) {\n      max_value = output[i];\n      max_index = i;\n    }\n  }\n  printf(\"The image has been classified as '%s'\\n\", labels[max_index]);\n\n  // The FVP will shut down when it receives \"EXITTHESIM\" on the UART\n  printf(\"EXITTHESIM\\n\");\n  while (1 == 1)\n    ;\n  return 0;\n}\n```\n\nIn addition, you will need these header files from github in your\n`./include` directory:\n\n[include\nfiles](https://github.com/apache/tvm/tree/main/apps/microtvm/ethosu/include)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "::: {.note}\n::: {.admonition-title}\nNote\n:::\n\nIf you\\'d like to use FreeRTOS for task scheduling and queues, a sample\napplication can be found here [demo\\_freertos.c\n\\<https://github.com/apache/tvm/blob/main/apps/microtvm/ethosu/src/demo\\_freertos.c\\>]{.title-ref}\n:::\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Creating the linker script\n==========================\n\nWe need to create a linker script that will be used when we build our\napplication in the following section. The linker script tells the linker\nwhere everything should be placed in memory. The corstone300.ld linker\nscript below should be placed in your working directory.\n\nAn example linker script for the FVP can be found here\n[corstone300.ld](https://github.com/apache/tvm/blob/main/apps/microtvm/ethosu/corstone300.ld)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "::: {.note}\n::: {.admonition-title}\nNote\n:::\n\nThe code generated by TVM will place the model weights and the Arm(R)\nEthos(TM)-U55 command stream in a section named `ethosu_scratch`. For a\nmodel the size of MobileNet v1, the weights and command stream will not\nfit into the limited SRAM available. For this reason it\\'s important\nthat the linker script places the `ethosu_scratch` section into DRAM\n(DDR).\n:::\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "::: {.note}\n::: {.admonition-title}\nNote\n:::\n\nBefore building and running the application, you will need to update\nyour PATH environment variable to include the path to cmake 3.19.5 and\nthe FVP. For example if you\\'ve installed these in `/opt/arm` , then you\nwould do the following:\n\n`export PATH=/opt/arm/FVP_Corstone_SSE-300_Ethos-U55/models/Linux64_GCC-6.4:/opt/arm/cmake/bin:$PATH`\n:::\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Building the demo application using make\n========================================\n\nWe can now build the demo application using make. The Makefile should be\nplaced in your working directory before running `make` on the command\nline:\n\nAn example Makefile can be found here:\n[Makefile](https://github.com/apache/tvm/blob/main/apps/microtvm/ethosu/Makefile)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "::: {.note}\n::: {.admonition-title}\nNote\n:::\n\nIf you\\'re using FreeRTOS, the Makefile builds it from the specified FREERTOS\\_PATH:\n\n:   `make FREERTOS_PATH=<FreeRTOS directory>`\n:::\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Running the demo application\n============================\n\nFinally, we can run our demo appliction on the Fixed Virtual Platform\n(FVP), by using the following command:\n\n``` {.sourceCode .bash}\nFVP_Corstone_SSE-300_Ethos-U55 -C cpu0.CFGDTCMSZ=15 \\\n-C cpu0.CFGITCMSZ=15 -C mps3_board.uart0.out_file=\\\"-\\\" -C mps3_board.uart0.shutdown_tag=\\\"EXITTHESIM\\\" \\\n-C mps3_board.visualisation.disable-visualisation=1 -C mps3_board.telnetterminal0.start_telnet=0 \\\n-C mps3_board.telnetterminal1.start_telnet=0 -C mps3_board.telnetterminal2.start_telnet=0 -C mps3_board.telnetterminal5.start_telnet=0 \\\n-C ethosu.extra_args=\"--fast\" \\\n-C ethosu.num_macs=256 ./build/demo\n```\n\nYou should see the following output displayed in your console window:\n\n``` {.sourceCode .text}\ntelnetterminal0: Listening for serial connection on port 5000\ntelnetterminal1: Listening for serial connection on port 5001\ntelnetterminal2: Listening for serial connection on port 5002\ntelnetterminal5: Listening for serial connection on port 5003\n\n    Ethos-U rev dedfa618 --- Jan 12 2021 23:03:55\n    (C) COPYRIGHT 2019-2021 Arm Limited\n    ALL RIGHTS RESERVED\n\nStarting Demo\nethosu_init. base_address=0x48102000, fast_memory=0x0, fast_memory_size=0, secure=1, privileged=1\nethosu_register_driver: New NPU driver at address 0x20000de8 is registered.\nCMD=0x00000000\nSoft reset NPU\nAllocating memory\nRunning inference\nethosu_find_and_reserve_driver - Driver 0x20000de8 reserved.\nethosu_invoke\nCMD=0x00000004\nQCONFIG=0x00000002\nREGIONCFG0=0x00000003\nREGIONCFG1=0x00000003\nREGIONCFG2=0x00000013\nREGIONCFG3=0x00000053\nREGIONCFG4=0x00000153\nREGIONCFG5=0x00000553\nREGIONCFG6=0x00001553\nREGIONCFG7=0x00005553\nAXI_LIMIT0=0x0f1f0000\nAXI_LIMIT1=0x0f1f0000\nAXI_LIMIT2=0x0f1f0000\nAXI_LIMIT3=0x0f1f0000\nethosu_invoke OPTIMIZER_CONFIG\nhandle_optimizer_config:\nOptimizer release nbr: 0 patch: 1\nOptimizer config cmd_stream_version: 0 macs_per_cc: 8 shram_size: 48 custom_dma: 0\nOptimizer config Ethos-U version: 1.0.6\nEthos-U config cmd_stream_version: 0 macs_per_cc: 8 shram_size: 48 custom_dma: 0\nEthos-U version: 1.0.6\nethosu_invoke NOP\nethosu_invoke NOP\nethosu_invoke NOP\nethosu_invoke COMMAND_STREAM\nhandle_command_stream: cmd_stream=0x61025be0, cms_length 1181\nQBASE=0x0000000061025be0, QSIZE=4724, base_pointer_offset=0x00000000\nBASEP0=0x0000000061026e60\nBASEP1=0x0000000060002f10\nBASEP2=0x0000000060002f10\nBASEP3=0x0000000061000fb0\nBASEP4=0x0000000060000fb0\nCMD=0x000Interrupt. status=0xffff0022, qread=4724\nCMD=0x00000006\n00006\nCMD=0x0000000c\nethosu_release_driver - Driver 0x20000de8 released\nThe image has been classified as 'tabby'\nEXITTHESIM\nInfo: /OSCI/SystemC: Simulation stopped by user.\n```\n\nYou should see near the end of the output that the image has been\ncorrectly classified as \\'tabby\\'.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}