{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%%bash\n# Installs the latest dev build of TVM from PyPI, with CUDA enabled. To use this,\n# you must request a Google Colab instance with a GPU by going to Runtime ->\n# Change runtime type -> Hardware accelerator -> GPU. If you wish to build from\n# source, see see https://tvm.apache.org/docs/install/from_source.html\npip install tlcpack-nightly-cu113 --pre -f https://tlcpack.ai/wheels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Working with Operators Using Tensor Expression {#tutorial-tensor-expr-get-started}\n==============================================\n\n**Author**: [Tianqi Chen](https://tqchen.github.io)\n\nIn this tutorial we will turn our attention to how TVM works with Tensor\nExpression (TE) to define tensor computations and apply loop\noptimizations. TE describes tensor computations in a pure functional\nlanguage (that is each expression has no side effects). When viewed in\ncontext of the TVM as a whole, Relay describes a computation as a set of\noperators, and each of these operators can be represented as a TE\nexpression where each TE expression takes input tensors and produces an\noutput tensor.\n\nThis is an introductory tutorial to the Tensor Expression language in\nTVM. TVM uses a domain specific tensor expression for efficient kernel\nconstruction. We will demonstrate the basic workflow with two examples\nof using the tensor expression language. The first example introduces TE\nand scheduling with vector addition. The second expands on these\nconcepts with a step-by-step optimization of a matrix multiplication\nwith TE. This matrix multiplication example will serve as the\ncomparative basis for future tutorials covering more advanced features\nof TVM.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Example 1: Writing and Scheduling Vector Addition in TE for CPU\n===============================================================\n\nLet\\'s look at an example in Python in which we will implement a TE for\nvector addition, followed by a schedule targeted towards a CPU. We begin\nby initializing a TVM environment.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import tvm\nimport tvm.testing\nfrom tvm import te\nimport numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "You will get better performance if you can identify the CPU you are\ntargeting and specify it. If you\\'re using LLVM, you can get this\ninformation from the command `llc --version` to get the CPU type, and\nyou can check `/proc/cpuinfo` for additional extensions that your\nprocessor might support. For example, you can use\n`llvm -mcpu=skylake-avx512` for CPUs with AVX-512 instructions.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "tgt = tvm.target.Target(target=\"llvm\", host=\"llvm\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Describing the Vector Computation\n=================================\n\nWe describe a vector addition computation. TVM adopts tensor semantics,\nwith each intermediate result represented as a multi-dimensional array.\nThe user needs to describe the computation rule that generates the\ntensors. We first define a symbolic variable `n` to represent the shape.\nWe then define two placeholder Tensors, `A` and `B`, with given shape\n`(n,)`. We then describe the result tensor `C`, with a `compute`\noperation. The `compute` defines a computation, with the output\nconforming to the specified tensor shape and the computation to be\nperformed at each position in the tensor defined by the lambda function.\nNote that while `n` is a variable, it defines a consistent shape between\nthe `A`, `B` and `C` tensors. Remember, no actual computation happens\nduring this phase, as we are only declaring how the computation should\nbe done.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "n = te.var(\"n\")\nA = te.placeholder((n,), name=\"A\")\nB = te.placeholder((n,), name=\"B\")\nC = te.compute(A.shape, lambda i: A[i] + B[i], name=\"C\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "::: {.admonition}\nLambda Functions\n\nThe second argument to the `te.compute` method is the function that\nperforms the computation. In this example, we\\'re using an anonymous\nfunction, also known as a `lambda` function, to define the computation,\nin this case addition on the `i`th element of `A` and `B`.\n:::\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Create a Default Schedule for the Computation\n=============================================\n\nWhile the above lines describe the computation rule, we can compute `C`\nin many different ways to fit different devices. For a tensor with\nmultiple axes, you can choose which axis to iterate over first, or\ncomputations can be split across different threads. TVM requires that\nthe user to provide a schedule, which is a description of how the\ncomputation should be performed. Scheduling operations within TE can\nchange loop orders, split computations across different threads, and\ngroup blocks of data together, amongst other operations. An important\nconcept behind schedules is that they only describe how the computation\nis performed, so different schedules for the same TE will produce the\nsame result.\n\nTVM allows you to create a naive schedule that will compute `C` in by\niterating in row major order.\n\n``` {.sourceCode .c}\nfor (int i = 0; i < n; ++i) {\n  C[i] = A[i] + B[i];\n}\n```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "s = te.create_schedule(C.op)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Compile and Evaluate the Default Schedule\n=========================================\n\nWith the TE expression and a schedule, we can produce runnable code for\nour target language and architecture, in this case LLVM and a CPU. We\nprovide TVM with the schedule, a list of the TE expressions that are in\nthe schedule, the target and host, and the name of the function we are\nproducing. The result of the output is a type-erased function that can\nbe called directly from Python.\n\nIn the following line, we use `tvm.build` to create a function. The\nbuild function takes the schedule, the desired signature of the function\n(including the inputs and outputs) as well as target language we want to\ncompile to.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fadd = tvm.build(s, [A, B, C], tgt, name=\"myadd\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let\\'s run the function, and compare the output to the same computation\nin numpy. The compiled TVM function exposes a concise C API that can be\ninvoked from any language. We begin by creating a device, which is a\ndevice (CPU in this example) that TVM can compile the schedule to. In\nthis case the device is an LLVM CPU target. We can then initialize the\ntensors in our device and perform the custom addition operation. To\nverify that the computation is correct, we can compare the result of the\noutput of the c tensor to the same computation performed by numpy.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "dev = tvm.device(tgt.kind.name, 0)\n\nn = 1024\na = tvm.nd.array(np.random.uniform(size=n).astype(A.dtype), dev)\nb = tvm.nd.array(np.random.uniform(size=n).astype(B.dtype), dev)\nc = tvm.nd.array(np.zeros(n, dtype=C.dtype), dev)\nfadd(a, b, c)\ntvm.testing.assert_allclose(c.numpy(), a.numpy() + b.numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To get a comparison of how fast this version is compared to numpy,\ncreate a helper function to run a profile of the TVM generated code.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import timeit\n\nnp_repeat = 100\nnp_running_time = timeit.timeit(\n    setup=\"import numpy\\n\"\n    \"n = 32768\\n\"\n    'dtype = \"float32\"\\n'\n    \"a = numpy.random.rand(n, 1).astype(dtype)\\n\"\n    \"b = numpy.random.rand(n, 1).astype(dtype)\\n\",\n    stmt=\"answer = a + b\",\n    number=np_repeat,\n)\nprint(\"Numpy running time: %f\" % (np_running_time / np_repeat))\n\n\ndef evaluate_addition(func, target, optimization, log):\n    dev = tvm.device(target.kind.name, 0)\n    n = 32768\n    a = tvm.nd.array(np.random.uniform(size=n).astype(A.dtype), dev)\n    b = tvm.nd.array(np.random.uniform(size=n).astype(B.dtype), dev)\n    c = tvm.nd.array(np.zeros(n, dtype=C.dtype), dev)\n\n    evaluator = func.time_evaluator(func.entry_name, dev, number=10)\n    mean_time = evaluator(a, b, c).mean\n    print(\"%s: %f\" % (optimization, mean_time))\n\n    log.append((optimization, mean_time))\n\n\nlog = [(\"numpy\", np_running_time / np_repeat)]\nevaluate_addition(fadd, tgt, \"naive\", log=log)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Updating the Schedule to Use Parallelism\n========================================\n\nNow that we\\'ve illustrated the fundamentals of TE, let\\'s go deeper\ninto what schedules do, and how they can be used to optimize tensor\nexpressions for different architectures. A schedule is a series of steps\nthat are applied to an expression to transform it in a number of\ndifferent ways. When a schedule is applied to an expression in TE, the\ninputs and outputs remain the same, but when compiled the implementation\nof the expression can change. This tensor addition, in the default\nschedule, is run serially but is easy to parallelize across all of the\nprocessor threads. We can apply the parallel schedule operation to our\ncomputation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "s[C].parallel(C.op.axis[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The `tvm.lower` command will generate the Intermediate Representation\n(IR) of the TE, with the corresponding schedule. By lowering the\nexpression as we apply different schedule operations, we can see the\neffect of scheduling on the ordering of the computation. We use the flag\n`simple_mode=True` to return a readable C-style statement.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(tvm.lower(s, [A, B, C], simple_mode=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "It\\'s now possible for TVM to run these blocks on independent threads.\nLet\\'s compile and run this new schedule with the parallel operation\napplied:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fadd_parallel = tvm.build(s, [A, B, C], tgt, name=\"myadd_parallel\")\nfadd_parallel(a, b, c)\n\ntvm.testing.assert_allclose(c.numpy(), a.numpy() + b.numpy())\n\nevaluate_addition(fadd_parallel, tgt, \"parallel\", log=log)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Updating the Schedule to Use Vectorization\n==========================================\n\nModern CPUs also have the ability to perform SIMD operations on floating\npoint values, and we can apply another schedule to our computation\nexpression to take advantage of this. Accomplishing this requires\nmultiple steps: first we have to split the schedule into inner and outer\nloops using the split scheduling primitive. The inner loops can use\nvectorization to use SIMD instructions using the vectorize scheduling\nprimitive, then the outer loops can be parallelized using the parallel\nscheduling primitive. Choose the split factor to be the number of\nthreads on your CPU.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Recreate the schedule, since we modified it with the parallel operation in\n# the previous example\nn = te.var(\"n\")\nA = te.placeholder((n,), name=\"A\")\nB = te.placeholder((n,), name=\"B\")\nC = te.compute(A.shape, lambda i: A[i] + B[i], name=\"C\")\n\ns = te.create_schedule(C.op)\n\n# This factor should be chosen to match the number of threads appropriate for\n# your CPU. This will vary depending on architecture, but a good rule is\n# setting this factor to equal the number of available CPU cores.\nfactor = 4\n\nouter, inner = s[C].split(C.op.axis[0], factor=factor)\ns[C].parallel(outer)\ns[C].vectorize(inner)\n\nfadd_vector = tvm.build(s, [A, B, C], tgt, name=\"myadd_parallel\")\n\nevaluate_addition(fadd_vector, tgt, \"vector\", log=log)\n\nprint(tvm.lower(s, [A, B, C], simple_mode=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Comparing the Different Schedules\n=================================\n\nWe can now compare the different schedules\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "baseline = log[0][1]\nprint(\"%s\\t%s\\t%s\" % (\"Operator\".rjust(20), \"Timing\".rjust(20), \"Performance\".rjust(20)))\nfor result in log:\n    print(\n        \"%s\\t%s\\t%s\"\n        % (result[0].rjust(20), str(result[1]).rjust(20), str(result[1] / baseline).rjust(20))\n    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "::: {.admonition}\nCode Specialization\n\nAs you may have noticed, the declarations of `A`, `B` and `C` all take\nthe same shape argument, `n`. TVM will take advantage of this to pass\nonly a single shape argument to the kernel, as you will find in the\nprinted device code. This is one form of specialization.\n\nOn the host side, TVM will automatically generate check code that checks\nthe constraints in the parameters. So if you pass arrays with different\nshapes into fadd, an error will be raised.\n\nWe can do more specializations. For example, we can write `n =\ntvm.runtime.convert(1024)`{.sourceCode} instead of\n`n = te.var(\"n\")`{.sourceCode}, in the computation declaration. The\ngenerated function will only take vectors with length 1024.\n:::\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We\\'ve defined, scheduled, and compiled a vector addition operator,\nwhich we were then able to execute on the TVM runtime. We can save the\noperator as a library, which we can then load later using the TVM\nruntime.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Targeting Vector Addition for GPUs (Optional)\n=============================================\n\nTVM is capable of targeting multiple architectures. In the next example,\nwe will target compilation of the vector addition to GPUs.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# If you want to run this code, change ``run_cuda = True``\n# Note that by default this example is not run in the docs CI.\n\nrun_cuda = False\nif run_cuda:\n    # Change this target to the correct backend for you gpu. For example: cuda (NVIDIA GPUs),\n    # rocm (Radeon GPUS), OpenCL (opencl).\n    tgt_gpu = tvm.target.Target(target=\"cuda\", host=\"llvm\")\n\n    # Recreate the schedule\n    n = te.var(\"n\")\n    A = te.placeholder((n,), name=\"A\")\n    B = te.placeholder((n,), name=\"B\")\n    C = te.compute(A.shape, lambda i: A[i] + B[i], name=\"C\")\n    print(type(C))\n\n    s = te.create_schedule(C.op)\n\n    bx, tx = s[C].split(C.op.axis[0], factor=64)\n\n    ################################################################################\n    # Finally we must bind the iteration axis bx and tx to threads in the GPU\n    # compute grid. The naive schedule is not valid for GPUs, and these are\n    # specific constructs that allow us to generate code that runs on a GPU.\n\n    s[C].bind(bx, te.thread_axis(\"blockIdx.x\"))\n    s[C].bind(tx, te.thread_axis(\"threadIdx.x\"))\n\n    ######################################################################\n    # Compilation\n    # -----------\n    # After we have finished specifying the schedule, we can compile it\n    # into a TVM function. By default TVM compiles into a type-erased\n    # function that can be directly called from the python side.\n    #\n    # In the following line, we use tvm.build to create a function.\n    # The build function takes the schedule, the desired signature of the\n    # function (including the inputs and outputs) as well as target language\n    # we want to compile to.\n    #\n    # The result of compilation fadd is a GPU device function (if GPU is\n    # involved) as well as a host wrapper that calls into the GPU\n    # function. fadd is the generated host wrapper function, it contains\n    # a reference to the generated device function internally.\n\n    fadd = tvm.build(s, [A, B, C], target=tgt_gpu, name=\"myadd\")\n\n    ################################################################################\n    # The compiled TVM function exposes a concise C API that can be invoked from\n    # any language.\n    #\n    # We provide a minimal array API in python to aid quick testing and prototyping.\n    # The array API is based on the `DLPack <https://github.com/dmlc/dlpack>`_ standard.\n    #\n    # - We first create a GPU device.\n    # - Then tvm.nd.array copies the data to the GPU.\n    # - ``fadd`` runs the actual computation\n    # - ``numpy()`` copies the GPU array back to the CPU (so we can verify correctness).\n    #\n    # Note that copying the data to and from the memory on the GPU is a required step.\n\n    dev = tvm.device(tgt_gpu.kind.name, 0)\n\n    n = 1024\n    a = tvm.nd.array(np.random.uniform(size=n).astype(A.dtype), dev)\n    b = tvm.nd.array(np.random.uniform(size=n).astype(B.dtype), dev)\n    c = tvm.nd.array(np.zeros(n, dtype=C.dtype), dev)\n    fadd(a, b, c)\n    tvm.testing.assert_allclose(c.numpy(), a.numpy() + b.numpy())\n\n    ################################################################################\n    # Inspect the Generated GPU Code\n    # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n    # You can inspect the generated code in TVM. The result of tvm.build is a TVM\n    # Module. fadd is the host module that contains the host wrapper, it also\n    # contains a device module for the CUDA (GPU) function.\n    #\n    # The following code fetches the device module and prints the content code.\n\n    if (\n        tgt_gpu.kind.name == \"cuda\"\n        or tgt_gpu.kind.name == \"rocm\"\n        or tgt_gpu.kind.name.startswith(\"opencl\")\n    ):\n        dev_module = fadd.imported_modules[0]\n        print(\"-----GPU code-----\")\n        print(dev_module.get_source())\n    else:\n        print(fadd.get_source())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Saving and Loading Compiled Modules\n===================================\n\nBesides runtime compilation, we can save the compiled modules into a\nfile and load them back later.\n\nThe following code first performs the following steps:\n\n-   It saves the compiled host module into an object file.\n-   Then it saves the device module into a ptx file.\n-   cc.create\\_shared calls a compiler (gcc) to create a shared library\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from tvm.contrib import cc\nfrom tvm.contrib import utils\n\ntemp = utils.tempdir()\nfadd.save(temp.relpath(\"myadd.o\"))\nif tgt.kind.name == \"cuda\":\n    fadd.imported_modules[0].save(temp.relpath(\"myadd.ptx\"))\nif tgt.kind.name == \"rocm\":\n    fadd.imported_modules[0].save(temp.relpath(\"myadd.hsaco\"))\nif tgt.kind.name.startswith(\"opencl\"):\n    fadd.imported_modules[0].save(temp.relpath(\"myadd.cl\"))\ncc.create_shared(temp.relpath(\"myadd.so\"), [temp.relpath(\"myadd.o\")])\nprint(temp.listdir())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "::: {.admonition}\nModule Storage Format\n\nThe CPU (host) module is directly saved as a shared library (.so). There\ncan be multiple customized formats of the device code. In our example,\nthe device code is stored in ptx, as well as a meta data json file. They\ncan be loaded and linked separately via import.\n:::\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Load Compiled Module\n====================\n\nWe can load the compiled module from the file system and run the code.\nThe following code loads the host and device module separately and links\nthem together. We can verify that the newly loaded function works.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fadd1 = tvm.runtime.load_module(temp.relpath(\"myadd.so\"))\nif tgt.kind.name == \"cuda\":\n    fadd1_dev = tvm.runtime.load_module(temp.relpath(\"myadd.ptx\"))\n    fadd1.import_module(fadd1_dev)\n\nif tgt.kind.name == \"rocm\":\n    fadd1_dev = tvm.runtime.load_module(temp.relpath(\"myadd.hsaco\"))\n    fadd1.import_module(fadd1_dev)\n\nif tgt.kind.name.startswith(\"opencl\"):\n    fadd1_dev = tvm.runtime.load_module(temp.relpath(\"myadd.cl\"))\n    fadd1.import_module(fadd1_dev)\n\nfadd1(a, b, c)\ntvm.testing.assert_allclose(c.numpy(), a.numpy() + b.numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Pack Everything into One Library\n================================\n\nIn the above example, we store the device and host code separately. TVM\nalso supports export everything as one shared library. Under the hood,\nwe pack the device modules into binary blobs and link them together with\nthe host code. Currently we support packing of Metal, OpenCL and CUDA\nmodules.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fadd.export_library(temp.relpath(\"myadd_pack.so\"))\nfadd2 = tvm.runtime.load_module(temp.relpath(\"myadd_pack.so\"))\nfadd2(a, b, c)\ntvm.testing.assert_allclose(c.numpy(), a.numpy() + b.numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "::: {.admonition}\nRuntime API and Thread-Safety\n\nThe compiled modules of TVM do not depend on the TVM compiler. Instead,\nthey only depend on a minimum runtime library. The TVM runtime library\nwraps the device drivers and provides thread-safe and device agnostic\ncalls into the compiled functions.\n\nThis means that you can call the compiled TVM functions from any thread,\non any GPUs, provided that you have compiled the code for that GPU.\n:::\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Generate OpenCL Code\n====================\n\nTVM provides code generation features into multiple backends. We can\nalso generate OpenCL code or LLVM code that runs on CPU backends.\n\nThe following code blocks generate OpenCL code, creates array on an\nOpenCL device, and verifies the correctness of the code.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "if tgt.kind.name.startswith(\"opencl\"):\n    fadd_cl = tvm.build(s, [A, B, C], tgt, name=\"myadd\")\n    print(\"------opencl code------\")\n    print(fadd_cl.imported_modules[0].get_source())\n    dev = tvm.cl(0)\n    n = 1024\n    a = tvm.nd.array(np.random.uniform(size=n).astype(A.dtype), dev)\n    b = tvm.nd.array(np.random.uniform(size=n).astype(B.dtype), dev)\n    c = tvm.nd.array(np.zeros(n, dtype=C.dtype), dev)\n    fadd_cl(a, b, c)\n    tvm.testing.assert_allclose(c.numpy(), a.numpy() + b.numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "::: {.admonition}\nTE Scheduling Primitives\n\nTVM includes a number of different scheduling primitives:\n\n-   split: splits a specified axis into two axises by the defined\n    factor.\n-   tile: tiles will split a computation across two axes by the defined\n    factors.\n-   fuse: fuses two consecutive axises of one computation.\n-   reorder: can reorder the axises of a computation into a defined\n    order.\n-   bind: can bind a computation to a specific thread, useful in GPU\n    programming.\n-   compute\\_at: by default, TVM will compute tensors at the outermost\n    level of the function, or the root, by default. compute\\_at\n    specifies that one tensor should be computed at the first axis of\n    computation for another operator.\n-   compute\\_inline: when marked inline, a computation will be expanded\n    then inserted into the address where the tensor is required.\n-   compute\\_root: moves a computation to the outermost layer, or root,\n    of the function. This means that stage of the computation will be\n    fully computed before it moves on to the next stage.\n\nA complete description of these primitives can be found in the\n`Schedule Primitives <schedule_primitives>`{.interpreted-text\nrole=\"ref\"} docs page.\n:::\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Example 2: Manually Optimizing Matrix Multiplication with TE\n============================================================\n\nNow we will consider a second, more advanced example, demonstrating how\nwith just 18 lines of python code TVM speeds up a common matrix\nmultiplication operation by 18x.\n\n**Matrix multiplication is a compute intensive operation. There are two\nimportant optimizations for good CPU performance:**\n\n1.  Increase the cache hit rate of memory access. Both complex numerical\n    computation and hot-spot memory access can be accelerated by a high\n    cache hit rate. This requires us to transform the origin memory\n    access pattern to a pattern that fits the cache policy.\n2.  SIMD (Single instruction multi-data), also known as the vector\n    processing unit. On each cycle instead of processing a single value,\n    SIMD can process a small batch of data. This requires us to\n    transform the data access pattern in the loop body in uniform\n    pattern so that the LLVM backend can lower it to SIMD.\n\nThe techniques used in this tutorial are a subset of tricks mentioned in\nthis [repository](https://github.com/flame/how-to-optimize-gemm). Some\nof them have been applied by TVM abstraction automatically, but some of\nthem cannot be automatically applied due to TVM constraints.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Preparation and Performance Baseline\n====================================\n\nWe begin by collecting performance data on the [numpy]{.title-ref}\nimplementation of matrix multiplication.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import tvm\nimport tvm.testing\nfrom tvm import te\nimport numpy\n\n# The size of the matrix\n# (M, K) x (K, N)\n# You are free to try out different shapes, sometimes TVM optimization outperforms numpy with MKL.\nM = 1024\nK = 1024\nN = 1024\n\n# The default tensor data type in tvm\ndtype = \"float32\"\n\n# You will want to adjust the target to match any CPU vector extensions you\n# might have. For example, if you're using using Intel AVX2 (Advanced Vector\n# Extensions) ISA for SIMD, you can get the best performance by changing the\n# following line to ``llvm -mcpu=core-avx2``, or specific type of CPU you use.\n# Recall that you're using llvm, you can get this information from the command\n# ``llc --version`` to get the CPU type, and you can check ``/proc/cpuinfo``\n# for additional extensions that your processor might support.\n\ntarget = tvm.target.Target(target=\"llvm\", host=\"llvm\")\ndev = tvm.device(target.kind.name, 0)\n\n# Random generated tensor for testing\na = tvm.nd.array(numpy.random.rand(M, K).astype(dtype), dev)\nb = tvm.nd.array(numpy.random.rand(K, N).astype(dtype), dev)\n\n# Repeatedly perform a matrix multiplication to get a performance baseline\n# for the default numpy implementation\nnp_repeat = 100\nnp_running_time = timeit.timeit(\n    setup=\"import numpy\\n\"\n    \"M = \" + str(M) + \"\\n\"\n    \"K = \" + str(K) + \"\\n\"\n    \"N = \" + str(N) + \"\\n\"\n    'dtype = \"float32\"\\n'\n    \"a = numpy.random.rand(M, K).astype(dtype)\\n\"\n    \"b = numpy.random.rand(K, N).astype(dtype)\\n\",\n    stmt=\"answer = numpy.dot(a, b)\",\n    number=np_repeat,\n)\nprint(\"Numpy running time: %f\" % (np_running_time / np_repeat))\n\nanswer = numpy.dot(a.numpy(), b.numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we write a basic matrix multiplication using TVM TE and verify that\nit produces the same results as the numpy implementation. We also write\na function that will help us measure the performance of the schedule\noptimizations.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# TVM Matrix Multiplication using TE\nk = te.reduce_axis((0, K), \"k\")\nA = te.placeholder((M, K), name=\"A\")\nB = te.placeholder((K, N), name=\"B\")\nC = te.compute((M, N), lambda x, y: te.sum(A[x, k] * B[k, y], axis=k), name=\"C\")\n\n# Default schedule\ns = te.create_schedule(C.op)\nfunc = tvm.build(s, [A, B, C], target=target, name=\"mmult\")\n\nc = tvm.nd.array(numpy.zeros((M, N), dtype=dtype), dev)\nfunc(a, b, c)\ntvm.testing.assert_allclose(c.numpy(), answer, rtol=1e-5)\n\n\ndef evaluate_operation(s, vars, target, name, optimization, log):\n    func = tvm.build(s, [A, B, C], target=target, name=\"mmult\")\n    assert func\n\n    c = tvm.nd.array(numpy.zeros((M, N), dtype=dtype), dev)\n    func(a, b, c)\n    tvm.testing.assert_allclose(c.numpy(), answer, rtol=1e-5)\n\n    evaluator = func.time_evaluator(func.entry_name, dev, number=10)\n    mean_time = evaluator(a, b, c).mean\n    print(\"%s: %f\" % (optimization, mean_time))\n    log.append((optimization, mean_time))\n\n\nlog = []\n\nevaluate_operation(s, [A, B, C], target=target, name=\"mmult\", optimization=\"none\", log=log)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let\\'s take a look at the intermediate representation of the operator\nand default schedule using the TVM lower function. Note how the\nimplementation is essentially a naive implementation of a matrix\nmultiplication, using three nested loops over the indices of the A and B\nmatrices.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(tvm.lower(s, [A, B, C], simple_mode=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Optimization 1: Blocking\n========================\n\nA important trick to enhance the cache hit rate is blocking, where you\nstructure memory access such that the inside a block is a small\nneighborhood that has high memory locality. In this tutorial, we pick a\nblock factor of 32. This will result in a block that will fill a 32 \\*\n32 \\* sizeof(float) area of memory. This corresponds to a cache size of\n4KB, in relation to a reference cache size of 32 KB for L1 cache.\n\nWe begin by creating a default schedule for the `C` operation, then\napply a `tile` scheduling primitive to it with the specified block\nfactor, with the scheduling primitive returning the resulting loop order\nfrom outermost to innermost, as a vector\n`[x_outer, y_outer, x_inner, y_inner]`. We then get the reduction axis\nfor output of the operation, and perform a split operation on it using a\nfactor of 4. This factor doesn\\'t directly impact the blocking\noptimization we\\'re working on right now, but will be useful later when\nwe apply vectorization.\n\nNow that the operation has been blocked, we can reorder the computation\nto put the reduction operation into the outermost loop of the\ncomputation, helping to guarantee that the blocked data remains in\ncache. This completes the schedule, and we can build and test the\nperformance compared to the naive schedule.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "bn = 32\n\n# Blocking by loop tiling\nxo, yo, xi, yi = s[C].tile(C.op.axis[0], C.op.axis[1], bn, bn)\n(k,) = s[C].op.reduce_axis\nko, ki = s[C].split(k, factor=4)\n\n# Hoist reduction domain outside the blocking loop\ns[C].reorder(xo, yo, ko, ki, xi, yi)\n\nevaluate_operation(s, [A, B, C], target=target, name=\"mmult\", optimization=\"blocking\", log=log)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "By reordering the computation to take advantage of caching, you should\nsee a significant improvement in the performance of the computation.\nNow, print the internal representation and compare it to the original:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(tvm.lower(s, [A, B, C], simple_mode=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Optimization 2: Vectorization\n=============================\n\nAnother important optimization trick is vectorization. When the memory\naccess pattern is uniform, the compiler can detect this pattern and pass\nthe continuous memory to the SIMD vector processor. In TVM, we can use\nthe `vectorize` interface to hint the compiler this pattern, taking\nadvantage of this hardware feature.\n\nIn this tutorial, we chose to vectorize the inner loop row data since it\nis already cache friendly from our previous optimizations.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Apply the vectorization optimization\ns[C].vectorize(yi)\n\nevaluate_operation(s, [A, B, C], target=target, name=\"mmult\", optimization=\"vectorization\", log=log)\n\n# The generalized IR after vectorization\nprint(tvm.lower(s, [A, B, C], simple_mode=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Optimization 3: Loop Permutation\n================================\n\nIf we look at the above IR, we can see the inner loop row data is\nvectorized and B is transformed into PackedB (this is evident by the\n[(float32x32\\*)B2]{.title-ref} portion of the inner loop). The traversal\nof PackedB is sequential now. So we will look at the access pattern of\nA. In current schedule, A is accessed column by column which is not\ncache friendly. If we change the nested loop order of [ki]{.title-ref}\nand inner axes [xi]{.title-ref}, the access pattern for A matrix will be\nmore cache friendly.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "s = te.create_schedule(C.op)\nxo, yo, xi, yi = s[C].tile(C.op.axis[0], C.op.axis[1], bn, bn)\n(k,) = s[C].op.reduce_axis\nko, ki = s[C].split(k, factor=4)\n\n# re-ordering\ns[C].reorder(xo, yo, ko, xi, ki, yi)\ns[C].vectorize(yi)\n\nevaluate_operation(\n    s, [A, B, C], target=target, name=\"mmult\", optimization=\"loop permutation\", log=log\n)\n\n# Again, print the new generalized IR\nprint(tvm.lower(s, [A, B, C], simple_mode=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Optimization 4: Array Packing\n=============================\n\nAnother important trick is array packing. This trick is to reorder the\nstorage dimension of the array to convert the continuous access pattern\non certain dimension to a sequential pattern after flattening.\n\n![image](https://github.com/dmlc/web-data/raw/main/tvm/tutorial/array-packing.png){.align-center}\n\nJust as it is shown in the figure above, after blocking the\ncomputations, we can observe the array access pattern of B (after\nflattening), which is regular but discontinuous. We expect that after\nsome transformation we can get a continuous access pattern. By\nreordering a `[16][16]` array to a `[16/4][16][4]` array the access\npattern of B will be sequential when grabbing the corresponding value\nfrom the packed array.\n\nTo accomplish this, we are going to have to start with a new default\nschedule, taking into account the new packing of B. It\\'s worth taking a\nmoment to comment on this: TE is a powerful and expressive language for\nwriting optimized operators, but it often requires some knowledge of the\nunderlying algorithm, data structures, and hardware target that you are\nwriting for. Later in the tutorial, we will discuss some of the options\nfor letting TVM take that burden. Regardless, let\\'s move on with the\nnew optimized schedule.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# We have to re-write the algorithm slightly.\npackedB = te.compute((N / bn, K, bn), lambda x, y, z: B[y, x * bn + z], name=\"packedB\")\nC = te.compute(\n    (M, N),\n    lambda x, y: te.sum(A[x, k] * packedB[y // bn, k, tvm.tir.indexmod(y, bn)], axis=k),\n    name=\"C\",\n)\n\ns = te.create_schedule(C.op)\n\nxo, yo, xi, yi = s[C].tile(C.op.axis[0], C.op.axis[1], bn, bn)\n(k,) = s[C].op.reduce_axis\nko, ki = s[C].split(k, factor=4)\n\ns[C].reorder(xo, yo, ko, xi, ki, yi)\ns[C].vectorize(yi)\n\nx, y, z = s[packedB].op.axis\ns[packedB].vectorize(z)\ns[packedB].parallel(x)\n\nevaluate_operation(s, [A, B, C], target=target, name=\"mmult\", optimization=\"array packing\", log=log)\n\n# Here is the generated IR after array packing.\nprint(tvm.lower(s, [A, B, C], simple_mode=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Optimization 5: Optimizing Block Writing Through Caching\n========================================================\n\nUp to this point all of our optimizations have focused on efficiently\naccessing and computing the data from the [A]{.title-ref} and\n[B]{.title-ref} matrices to compute the [C]{.title-ref} matrix. After\nthe blocking optimization, the operator will write result to\n[C]{.title-ref} block by block, and the access pattern is not\nsequential. We can address this by using a sequential cache array, using\na combination of [cache\\_write]{.title-ref}, [compute\\_at]{.title-ref},\nand [unroll\\`to hold the block results and write to \\`C]{.title-ref}\nwhen all the block results are ready.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "s = te.create_schedule(C.op)\n\n# Allocate write cache\nCC = s.cache_write(C, \"global\")\n\nxo, yo, xi, yi = s[C].tile(C.op.axis[0], C.op.axis[1], bn, bn)\n\n# Write cache is computed at yo\ns[CC].compute_at(s[C], yo)\n\n# New inner axes\nxc, yc = s[CC].op.axis\n\n(k,) = s[CC].op.reduce_axis\nko, ki = s[CC].split(k, factor=4)\ns[CC].reorder(ko, xc, ki, yc)\ns[CC].unroll(ki)\ns[CC].vectorize(yc)\n\nx, y, z = s[packedB].op.axis\ns[packedB].vectorize(z)\ns[packedB].parallel(x)\n\nevaluate_operation(s, [A, B, C], target=target, name=\"mmult\", optimization=\"block caching\", log=log)\n\n# Here is the generated IR after write cache blocking.\nprint(tvm.lower(s, [A, B, C], simple_mode=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Optimization 6: Parallelization\n===============================\n\nSo far, our computation is only designed to use a single core. Nearly\nall modern processors have multiple cores, and computation can benefit\nfrom running computations in parallel. The final optimization is to take\nadvantage of thread-level parallelization.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# parallel\ns[C].parallel(xo)\n\nx, y, z = s[packedB].op.axis\ns[packedB].vectorize(z)\ns[packedB].parallel(x)\n\nevaluate_operation(\n    s, [A, B, C], target=target, name=\"mmult\", optimization=\"parallelization\", log=log\n)\n\n# Here is the generated IR after parallelization.\nprint(tvm.lower(s, [A, B, C], simple_mode=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Summary of Matrix Multiplication Example\n========================================\n\nAfter applying the above simple optimizations with only 18 lines of\ncode, our generated code can begin to approach the performance of\n[numpy]{.title-ref} with the Math Kernel Library (MKL). Since we\\'ve\nbeen logging the performance as we\\'ve been working, we can compare the\nresults.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "baseline = log[0][1]\nprint(\"%s\\t%s\\t%s\" % (\"Operator\".rjust(20), \"Timing\".rjust(20), \"Performance\".rjust(20)))\nfor result in log:\n    print(\n        \"%s\\t%s\\t%s\"\n        % (result[0].rjust(20), str(result[1]).rjust(20), str(result[1] / baseline).rjust(20))\n    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Note that the outputs on the web page reflect the running times on a\nnon-exclusive Docker container, and should be considered unreliable. It\nis highly encouraged to run the tutorial by yourself to observe the\nperformance gain achieved by TVM, and to carefully work through each\nexample to understand the iterative improvements that are made to the\nmatrix multiplication operation.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Final Notes and Summary\n=======================\n\nAs mentioned earlier, how to apply optimizations using TE and scheduling\nprimitives can require some knowledge of the underlying architecture and\nalgorithms. However, TE was designed to act as a foundation for more\ncomplex algorithms that can search the potential optimization. With the\nknowledge you have from this introduction to TE, we can now begin to\nexplore how TVM can automate the schedule optimization process.\n\nThis tutorial provided a walk-through of TVM Tensor Expression (TE)\nworkflow using a vector add and a matrix multiplication examples. The\ngeneral workflow is\n\n-   Describe your computation via a series of operations.\n-   Describe how we want to compute use schedule primitives.\n-   Compile to the target function we want.\n-   Optionally, save the function to be loaded later.\n\nUpcoming tutorials expand on the matrix multiplication example, and show\nhow you can build generic templates of the matrix multiplication and\nother operations with tunable parameters that allows you to\nautomatically optimize the computation for specific platforms.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}