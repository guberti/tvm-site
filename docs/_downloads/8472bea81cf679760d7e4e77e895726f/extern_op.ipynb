{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%%bash\n# Installs the latest dev build of TVM from PyPI. If you wish to build\n# from source, see https://tvm.apache.org/docs/install/from_source.html\npip install apache-tvm --pre"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "External Tensor Functions\n=========================\n\n**Author**: [Tianqi Chen](https://tqchen.github.io)\n\nWhile TVM supports transparent code generation, sometimes it is also\nhelpful to incorporate manual written code into the pipeline. For\nexample, we might want to use cuDNN for some of the convolution kernels\nand define the rest of the stages.\n\nTVM supports these black box function calls natively. Specifically, TVM\nsupport all the tensor functions that are DLPack compatible. Which means\nwe can call any function with POD types(pointer, int, float) or pointer\nto DLTensor as argument.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from __future__ import absolute_import, print_function\n\n\nimport tvm\nfrom tvm import te\nimport numpy as np\nfrom tvm.contrib import cblas\nimport tvm.testing\n\nif not tvm.get_global_func(\"tvm.contrib.cblas.matmul\", allow_missing=True):\n    raise Exception(\"Not compiled with cblas support; can't build this tutorial\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Use Extern Tensor Function\n==========================\n\nIn the example below, we use `te.extern`{.interpreted-text role=\"any\"}\nto add an extern array function call. In the extern call, we declare the\nshape of output tensors. In the second argument we provide the list of\ninputs.\n\nUser will need to provide a function describing how to compute the\nresult. The compute function takes list of symbolic placeholder for the\ninputs, list of symbolic placeholder for the outputs and returns the\nexecuting statement.\n\nIn this case we simply call a registered TVM function, which invokes a\nCBLAS call. TVM does not control internal of the extern array function\nand treats it as black-box. We can further mix schedulable TVM calls\nthat add a bias term to the result.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "n = 1024\nl = 128\nm = 235\nbias = te.var(\"bias\", dtype=\"float32\")\nA = te.placeholder((n, l), name=\"A\")\nB = te.placeholder((l, m), name=\"B\")\nC = te.extern(\n    (n, m),\n    [A, B],\n    lambda ins, outs: tvm.tir.call_packed(\n        \"tvm.contrib.cblas.matmul\", ins[0], ins[1], outs[0], False, False\n    ),\n    name=\"C\",\n)\nD = te.compute(C.shape, lambda i, j: C[i, j] + bias, name=\"D\")\ns = te.create_schedule(D.op)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Verify the Result\n=================\n\nWe can verify that the result matches what we expected.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "dev = tvm.cpu(0)\nf = tvm.build(s, [A, B, D, bias], \"llvm\")\na = tvm.nd.array(np.random.uniform(size=(n, l)).astype(A.dtype), dev)\nb = tvm.nd.array(np.random.uniform(size=(l, m)).astype(B.dtype), dev)\nd = tvm.nd.array(np.zeros((n, m), dtype=D.dtype), dev)\nbb = 10.0\nf(a, b, d, bb)\ntvm.testing.assert_allclose(d.numpy(), np.dot(a.numpy(), b.numpy()) + 10, rtol=1e-5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Extern Contrib Wrappers\n=======================\n\nTVM also provide extern contrib wrappers to useful extern calls, the\nfollowing line is equivalent to the previous example.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from tvm.contrib import cblas\n\nC = cblas.matmul(A, B)\nD = te.compute(C.shape, lambda i, j: C[i, j] + bias, name=\"D\")\ns = te.create_schedule(D.op)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Hook Python Function as Extern\n==============================\n\nSince we can call into any PackedFunc in TVM. We can use the extern\nfunction to callback into python.\n\nThe following example registers a python function into TVM runtime\nsystem and use it to complete one stage of the computation. This makes\nTVM much more flexible. For example, we can insert front-end callbacks\nto inspect the intermediate results or mix customized code with TVM.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "@tvm.register_func(\"tvm.contrib.my_tvm_addone\")\ndef my_tvm_addone(x, y):\n    print(\"my_tvm_addone signatures: %s, %s\" % (type(x), type(y)))\n    tvm.nd.array(x.numpy() + 1).copyto(y)\n\n\nA = te.placeholder((n,), name=\"A\")\nB = te.extern(\n    A.shape,\n    [A],\n    lambda ins, outs: tvm.tir.call_packed(\"tvm.contrib.my_tvm_addone\", ins[0], outs[0]),\n    name=\"C\",\n)\ns = te.create_schedule(B.op)\nf = tvm.build(s, [A, B], \"llvm\")\na = tvm.nd.array(np.random.uniform(size=(n,)).astype(A.dtype), dev)\nb = tvm.nd.array(np.random.uniform(size=(n,)).astype(B.dtype), dev)\nf(a, b)\ntvm.testing.assert_allclose(b.numpy(), a.numpy() + 1, rtol=1e-5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Summary\n=======\n\n-   TVM calls extern tensor function via `te.extern`{.interpreted-text\n    role=\"any\"}\n-   Use contrib wrappers for short sugars of extern tensor calls.\n-   We can hook front-end function as extern tensor callbacks.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}