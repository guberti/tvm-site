{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%%bash\n# Installs the latest dev build of TVM from PyPI. If you wish to build\n# from source, see https://tvm.apache.org/docs/install/from_source.html\npip install apache-tvm --pre"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Use Tensor Expression Debug Display (TEDD) for Visualization\n============================================================\n\n**Author**: [Yongfeng Gu](https://github.com/yongfeng-nv)\n\nThis is an introduction about using TEDD to visualize tensor\nexpressions.\n\nTensor Expressions are scheduled with primitives. Although individual\nprimitives are usually easy to understand, they become complicated\nquickly when you put them together. We have introduced an operational\nmodel of schedule primitives in Tensor Expression.\n\n-   the interactions between different schedule primitives,\n-   the impact of the schedule primitives on the final code generation.\n\nThe operational model is based on a Dataflow Graph, a Schedule Tree and\nan IterVar Relationship Graph. Schedule primitives perform operations on\nthese graphs.\n\nTEDD renders these three graphs from a given schedule. This tutorial\ndemonstrates how to use TEDD and how to interpret the rendered graphs.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import tvm\nfrom tvm import te\nfrom tvm import topi\nfrom tvm.contrib import tedd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Define and Schedule Convolution with Bias and ReLU\n==================================================\n\nLet\\'s build an example Tensor Expression for a convolution followed by\nBias and ReLU. We first connect conv2d, add, and relu TOPIs. Then, we\ncreate a TOPI generic schedule.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "batch = 1\nin_channel = 256\nin_size = 32\nnum_filter = 256\nkernel = 3\nstride = 1\npadding = \"SAME\"\ndilation = 1\n\nA = te.placeholder((in_size, in_size, in_channel, batch), name=\"A\")\nW = te.placeholder((kernel, kernel, in_channel, num_filter), name=\"W\")\nB = te.placeholder((1, num_filter, 1), name=\"bias\")\n\nwith tvm.target.Target(\"llvm\"):\n    t_conv = topi.nn.conv2d_hwcn(A, W, stride, padding, dilation)\n    t_bias = topi.add(t_conv, B)\n    t_relu = topi.nn.relu(t_bias)\n    s = topi.generic.schedule_conv2d_hwcn([t_relu])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Render Graphs with TEDD\n=======================\n\nWe render graphs to see the computation and how it is scheduled. If you\nrun the tutorial in a Jupyter notebook, you can use the following\ncommented lines to render SVG figures showing in notebook directly.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "tedd.viz_dataflow_graph(s, dot_file_path=\"/tmp/dfg.dot\")\n# tedd.viz_dataflow_graph(s, show_svg = True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![image](https://github.com/dmlc/web-data/raw/main/tvm/tutorial/tedd_dfg.png){.align-center}\n\nThe first one is a dataflow graph. Every node represents a stage with\nname and memory scope shown in the middle and inputs/outputs information\non the sides. Edges show nodes\\' dependency.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "tedd.viz_schedule_tree(s, dot_file_path=\"/tmp/scheduletree.dot\")\n# tedd.viz_schedule_tree(s, show_svg = True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We just rendered the schedule tree graph. You may notice an warning\nabout ranges not available. The message also suggests to call\nnormalize() to infer range information. We will skip inspecting the\nfirst schedule tree and encourage you to compare the graphs before and\nafter normalize() for its impact.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "s = s.normalize()\ntedd.viz_schedule_tree(s, dot_file_path=\"/tmp/scheduletree2.dot\")\n# tedd.viz_schedule_tree(s, show_svg = True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![image](https://github.com/dmlc/web-data/raw/main/tvm/tutorial/tedd_st.png){.align-center}\n\nNow, let us take a close look at the second schedule tree. Every block\nunder ROOT represents a stage. Stage name shows in the top row and\ncompute shows in the bottom row. The middle rows are for IterVars, the\nhigher the outer, the lower the inner. An IterVar row contains its\nindex, name, type, and other optional information. Let\\'s use the\nW.shared stage as an example. The top row tells its name, \\\"W.shared\\\",\nand memory scope, \\\"Shared\\\". Its compute is\n`W(ax0, ax1, ax2, ax3)`{.sourceCode}. Its outer most loop IterVar is\nax0.ax1.fused.ax2.fused.ax3.fused.outer, indexed with 0, of kDataPar,\nbound to threadIdx.y, and with range(min=0, ext=8). You can also tell\nIterVar type with the index box color, shown in the legend.\n\nIf a stage doesn\\'t compute\\_at any other stage, it has an edge directly\nto the ROOT node. Otherwise, it has an edge pointing to the IterVar it\nattaches to, such as W.shared attaches to rx.outer in the middle compute\nstage.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "::: {.note}\n::: {.admonition-title}\nNote\n:::\n\nBy definition, IterVars are internal nodes and computes are leaf nodes\nin a schedule tree. The edges among IterVars and compute within one\nstage are omitted, making every stage a block, for better readability.\n:::\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "tedd.viz_itervar_relationship_graph(s, dot_file_path=\"/tmp/itervar.dot\")\n# tedd.viz_itervar_relationship_graph(s, show_svg = True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![image](https://github.com/dmlc/web-data/raw/main/tvm/tutorial/tedd_itervar_rel.png){.align-center}\n\nThe last one is an IterVar Relationship Graph. Every subgraph represents\na stage and contains IterVar nodes and transformation nodes. For\nexample, W.shared has three split nodes and three fuse nodes. The rest\nare IterVar nodes of the same format as the IterVar rows in Schedule\nTrees. Root IterVars are those not driven by any transformation node,\nsuch as ax0; leaf IterVars don\\'t drive any transformation node and have\nnon-negative indices, such as ax0.ax1.fused.ax2.fused.ax3.fused.outer\nwith index of 0.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Summary\n=======\n\nThis tutorial demonstrates the usage of TEDD. We use an example built\nwith TOPI to show the schedules under the hood. You can also use it\nbefore and after any schedule primitive to inspect its effect.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}