{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%%bash\n# Installs the latest dev build of TVM from PyPI. If you wish to build\n# from source, see https://tvm.apache.org/docs/install/from_source.html\npip install apache-tvm --pre"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Deploy a Framework-prequantized Model with TVM\n==============================================\n\n**Author**: [Masahiro Masuda](https://github.com/masahi)\n\nThis is a tutorial on loading models quantized by deep learning\nframeworks into TVM. Pre-quantized model import is one of the\nquantization support we have in TVM. More details on the quantization\nstory in TVM can be found\n[here](https://discuss.tvm.apache.org/t/quantization-story/3920).\n\nHere, we demonstrate how to load and run models quantized by PyTorch,\nMXNet, and TFLite. Once loaded, we can run compiled, quantized models on\nany hardware TVM supports.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "First, necessary imports\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n\nimport numpy as np\n\nimport torch\nfrom torchvision.models.quantization import mobilenet as qmobilenet\n\nimport tvm\nfrom tvm import relay\nfrom tvm.contrib.download import download_testdata"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Helper functions to run the demo\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def get_transform():\n    import torchvision.transforms as transforms\n\n    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n    return transforms.Compose(\n        [\n            transforms.Resize(256),\n            transforms.CenterCrop(224),\n            transforms.ToTensor(),\n            normalize,\n        ]\n    )\n\n\ndef get_real_image(im_height, im_width):\n    img_url = \"https://github.com/dmlc/mxnet.js/blob/main/data/cat.png?raw=true\"\n    img_path = download_testdata(img_url, \"cat.png\", module=\"data\")\n    return Image.open(img_path).resize((im_height, im_width))\n\n\ndef get_imagenet_input():\n    im = get_real_image(224, 224)\n    preprocess = get_transform()\n    pt_tensor = preprocess(im)\n    return np.expand_dims(pt_tensor.numpy(), 0)\n\n\ndef get_synset():\n    synset_url = \"\".join(\n        [\n            \"https://gist.githubusercontent.com/zhreshold/\",\n            \"4d0b62f3d01426887599d4f7ede23ee5/raw/\",\n            \"596b27d23537e5a1b5751d2b0481ef172f58b539/\",\n            \"imagenet1000_clsid_to_human.txt\",\n        ]\n    )\n    synset_name = \"imagenet1000_clsid_to_human.txt\"\n    synset_path = download_testdata(synset_url, synset_name, module=\"data\")\n    with open(synset_path) as f:\n        return eval(f.read())\n\n\ndef run_tvm_model(mod, params, input_name, inp, target=\"llvm\"):\n    with tvm.transform.PassContext(opt_level=3):\n        lib = relay.build(mod, target=target, params=params)\n\n    runtime = tvm.contrib.graph_executor.GraphModule(lib[\"default\"](tvm.device(target, 0)))\n\n    runtime.set_input(input_name, inp)\n    runtime.run()\n    return runtime.get_output(0).numpy(), runtime"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "A mapping from label to class name, to verify that the outputs from\nmodels below are reasonable\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "synset = get_synset()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Everyone\\'s favorite cat image for demonstration\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "inp = get_imagenet_input()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Deploy a quantized PyTorch Model\n================================\n\nFirst, we demonstrate how to load deep learning models quantized by\nPyTorch, using our PyTorch frontend.\n\nPlease refer to the PyTorch static quantization tutorial below to learn\nabout their quantization workflow.\n<https://pytorch.org/tutorials/advanced/static_quantization_tutorial.html>\n\nWe use this function to quantize PyTorch models. In short, this function\ntakes a floating point model and converts it to uint8. The model is\nper-channel quantized.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def quantize_model(model, inp):\n    model.fuse_model()\n    model.qconfig = torch.quantization.get_default_qconfig(\"fbgemm\")\n    torch.quantization.prepare(model, inplace=True)\n    # Dummy calibration\n    model(inp)\n    torch.quantization.convert(model, inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Load quantization-ready, pretrained Mobilenet v2 model from torchvision\n=======================================================================\n\nWe choose mobilenet v2 because this model was trained with quantization\naware training. Other models require a full post training calibration.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "qmodel = qmobilenet.mobilenet_v2(pretrained=True).eval()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Quantize, trace and run the PyTorch Mobilenet v2 model\n======================================================\n\nThe details are out of scope for this tutorial. Please refer to the\ntutorials on the PyTorch website to learn about quantization and jit.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "pt_inp = torch.from_numpy(inp)\nquantize_model(qmodel, pt_inp)\nscript_module = torch.jit.trace(qmodel, pt_inp).eval()\n\nwith torch.no_grad():\n    pt_result = script_module(pt_inp).numpy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Convert quantized Mobilenet v2 to Relay-QNN using the PyTorch frontend\n======================================================================\n\nThe PyTorch frontend has support for converting a quantized PyTorch\nmodel to an equivalent Relay module enriched with quantization-aware\noperators. We call this representation Relay QNN dialect.\n\nYou can print the output from the frontend to see how quantized models\nare represented.\n\nYou would see operators specific to quantization such as qnn.quantize,\nqnn.dequantize, qnn.requantize, and qnn.conv2d etc.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "input_name = \"input\"  # the input name can be be arbitrary for PyTorch frontend.\ninput_shapes = [(input_name, (1, 3, 224, 224))]\nmod, params = relay.frontend.from_pytorch(script_module, input_shapes)\n# print(mod) # comment in to see the QNN IR dump"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Compile and run the Relay module\n================================\n\nOnce we obtained the quantized Relay module, the rest of the workflow is\nthe same as running floating point models. Please refer to other\ntutorials for more details.\n\nUnder the hood, quantization specific operators are lowered to a\nsequence of standard Relay operators before compilation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "target = \"llvm\"\ntvm_result, rt_mod = run_tvm_model(mod, params, input_name, inp, target=target)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Compare the output labels\n=========================\n\nWe should see identical labels printed.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "pt_top3_labels = np.argsort(pt_result[0])[::-1][:3]\ntvm_top3_labels = np.argsort(tvm_result[0])[::-1][:3]\n\nprint(\"PyTorch top3 labels:\", [synset[label] for label in pt_top3_labels])\nprint(\"TVM top3 labels:\", [synset[label] for label in tvm_top3_labels])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "However, due to the difference in numerics, in general the raw floating\npoint outputs are not expected to be identical. Here, we print how many\nfloating point output values are identical out of 1000 outputs from\nmobilenet v2.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(\"%d in 1000 raw floating outputs identical.\" % np.sum(tvm_result[0] == pt_result[0]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Measure performance\n===================\n\nHere we give an example of how to measure performance of TVM compiled\nmodels.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "n_repeat = 100  # should be bigger to make the measurement more accurate\ndev = tvm.cpu(0)\nprint(rt_mod.benchmark(dev, number=1, repeat=n_repeat))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "::: {.note}\n::: {.admonition-title}\nNote\n:::\n\nWe recommend this method for the following reasons:\n\n> -   Measurements are done in C++, so there is no Python overhead\n> -   It includes several warm up runs\n> -   The same method can be used to profile on remote devices (android\n>     etc.).\n:::\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "::: {.note}\n::: {.admonition-title}\nNote\n:::\n\nUnless the hardware has special support for fast 8 bit instructions,\nquantized models are not expected to be any faster than FP32 models.\nWithout fast 8 bit instructions, TVM does quantized convolution in 16\nbit, even if the model itself is 8 bit.\n\nFor x86, the best performance can be achieved on CPUs with AVX512\ninstructions set. In this case, TVM utilizes the fastest available 8 bit\ninstructions for the given target. This includes support for the VNNI 8\nbit dot product instruction (CascadeLake or newer).\n\nMoreover, the following general tips for CPU performance equally\napplies:\n\n> -   Set the environment variable TVM\\_NUM\\_THREADS to the number of\n>     physical cores\n> -   Choose the best target for your hardware, such as \\\"llvm\n>     -mcpu=skylake-avx512\\\" or \\\"llvm -mcpu=cascadelake\\\" (more CPUs\n>     with AVX512 would come in the future)\n:::\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Deploy a quantized MXNet Model\n==============================\n\nTODO\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Deploy a quantized TFLite Model\n===============================\n\nTODO\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}