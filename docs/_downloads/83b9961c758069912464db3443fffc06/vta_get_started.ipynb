{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%%bash\n# Installs the latest dev build of TVM from PyPI. If you wish to build\n# from source, see https://tvm.apache.org/docs/install/from_source.html\npip install apache-tvm --pre"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Get Started with VTA {#vta-get-started}\n====================\n\n**Author**: [Thierry Moreau](https://homes.cs.washington.edu/~moreau/)\n\nThis is an introduction tutorial on how to use TVM to program the VTA\ndesign.\n\nIn this tutorial, we will demonstrate the basic TVM workflow to\nimplement a vector addition on the VTA design\\'s vector ALU. This\nprocess includes specific scheduling transformations necessary to lower\ncomputation down to low-level accelerator operations.\n\nTo begin, we need to import TVM which is our deep learning optimizing\ncompiler. We also need to import the VTA python package which contains\nVTA specific extensions for TVM to target the VTA design.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from __future__ import absolute_import, print_function\n\nimport os\nimport tvm\nfrom tvm import te\nimport vta\nimport numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Loading in VTA Parameters\n=========================\n\nVTA is a modular and customizable design. Consequently, the user is free\nto modify high-level hardware parameters that affect the hardware design\nlayout. These parameters are specified in the\n`vta_config.json`{.sourceCode} file by their `log2`{.sourceCode} values.\nThese VTA parameters can be loaded with the `vta.get_env`{.sourceCode}\nfunction.\n\nFinally, the TVM target is also specified in the\n`vta_config.json`{.sourceCode} file. When set to *sim*, execution will\ntake place inside of a behavioral VTA simulator. If you want to run this\ntutorial on the Pynq FPGA development platform, follow the *VTA\nPynq-Based Testing Setup* guide.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "env = vta.get_env()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "FPGA Programming\n================\n\nWhen targeting the Pynq FPGA development board, we need to configure the\nboard with a VTA bitstream.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# We'll need the TVM RPC module and the VTA simulator module\nfrom tvm import rpc\nfrom tvm.contrib import utils\nfrom vta.testing import simulator\n\n# We read the Pynq RPC host IP address and port number from the OS environment\nhost = os.environ.get(\"VTA_RPC_HOST\", \"192.168.2.99\")\nport = int(os.environ.get(\"VTA_RPC_PORT\", \"9091\"))\n\n# We configure both the bitstream and the runtime system on the Pynq\n# to match the VTA configuration specified by the vta_config.json file.\nif env.TARGET == \"pynq\" or env.TARGET == \"de10nano\":\n\n    # Make sure that TVM was compiled with RPC=1\n    assert tvm.runtime.enabled(\"rpc\")\n    remote = rpc.connect(host, port)\n\n    # Reconfigure the JIT runtime\n    vta.reconfig_runtime(remote)\n\n    # Program the FPGA with a pre-compiled VTA bitstream.\n    # You can program the FPGA with your own custom bitstream\n    # by passing the path to the bitstream file instead of None.\n    vta.program_fpga(remote, bitstream=None)\n\n# In simulation mode, host the RPC server locally.\nelif env.TARGET in (\"sim\", \"tsim\", \"intelfocl\"):\n    remote = rpc.LocalSession()\n\n    if env.TARGET in [\"intelfocl\"]:\n        # program intelfocl aocx\n        vta.program_fpga(remote, bitstream=\"vta.bitstream\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Computation Declaration\n=======================\n\nAs a first step, we need to describe our computation. TVM adopts tensor\nsemantics, with each intermediate result represented as\nmulti-dimensional array. The user needs to describe the computation rule\nthat generates the output tensors.\n\nIn this example we describe a vector addition, which requires multiple\ncomputation stages, as shown in the dataflow diagram below. First we\ndescribe the input tensors `A`{.sourceCode} and `B`{.sourceCode} that\nare living in main memory. Second, we need to declare intermediate\ntensors `A_buf`{.sourceCode} and `B_buf`{.sourceCode}, which will live\nin VTA\\'s on-chip buffers. Having this extra computational stage allows\nus to explicitly stage cached reads and writes. Third, we describe the\nvector addition computation which will add `A_buf`{.sourceCode} to\n`B_buf`{.sourceCode} to produce `C_buf`{.sourceCode}. The last operation\nis a cast and copy back to DRAM, into results tensor `C`{.sourceCode}.\n\n![image](https://raw.githubusercontent.com/uwsampl/web-data/main/vta/tutorial/vadd_dataflow.png){.align-center}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Input Placeholders\n==================\n\nWe describe the placeholder tensors `A`{.sourceCode}, and\n`B`{.sourceCode} in a tiled data format to match the data layout\nrequirements imposed by the VTA vector ALU.\n\nFor VTA\\'s general purpose operations such as vector adds, the tile size\nis `(env.BATCH, env.BLOCK_OUT)`{.sourceCode}. The dimensions are\nspecified in the `vta_config.json`{.sourceCode} configuration file and\nare set by default to a (1, 16) vector.\n\nIn addition, A and B\\'s data types also needs to match the\n`env.acc_dtype`{.sourceCode} which is set by the\n`vta_config.json`{.sourceCode} file to be a 32-bit integer.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Output channel factor m - total 64 x 16 = 1024 output channels\nm = 64\n# Batch factor o - total 1 x 1 = 1\no = 1\n# A placeholder tensor in tiled data format\nA = te.placeholder((o, m, env.BATCH, env.BLOCK_OUT), name=\"A\", dtype=env.acc_dtype)\n# B placeholder tensor in tiled data format\nB = te.placeholder((o, m, env.BATCH, env.BLOCK_OUT), name=\"B\", dtype=env.acc_dtype)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Copy Buffers\n============\n\nOne specificity of hardware accelerators, is that on-chip memory has to\nbe explicitly managed. This means that we\\'ll need to describe\nintermediate tensors `A_buf`{.sourceCode} and `B_buf`{.sourceCode} that\ncan have a different memory scope than the original placeholder tensors\n`A`{.sourceCode} and `B`{.sourceCode}.\n\nLater in the scheduling phase, we can tell the compiler that\n`A_buf`{.sourceCode} and `B_buf`{.sourceCode} will live in the VTA\\'s\non-chip buffers (SRAM), while `A`{.sourceCode} and `B`{.sourceCode} will\nlive in main memory (DRAM). We describe A\\_buf and B\\_buf as the result\nof a compute operation that is the identity function. This can later be\ninterpreted by the compiler as a cached read operation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# A copy buffer\nA_buf = te.compute((o, m, env.BATCH, env.BLOCK_OUT), lambda *i: A(*i), \"A_buf\")\n# B copy buffer\nB_buf = te.compute((o, m, env.BATCH, env.BLOCK_OUT), lambda *i: B(*i), \"B_buf\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Vector Addition\n===============\n\nNow we\\'re ready to describe the vector addition result tensor\n`C`{.sourceCode}, with another compute operation. The compute function\ntakes the shape of the tensor, as well as a lambda function that\ndescribes the computation rule for each position of the tensor.\n\nNo computation happens during this phase, as we are only declaring how\nthe computation should be done.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Describe the in-VTA vector addition\nC_buf = te.compute(\n    (o, m, env.BATCH, env.BLOCK_OUT),\n    lambda *i: A_buf(*i).astype(env.acc_dtype) + B_buf(*i).astype(env.acc_dtype),\n    name=\"C_buf\",\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Casting the Results\n===================\n\nAfter the computation is done, we\\'ll need to send the results computed\nby VTA back to main memory.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "::: {.note}\n::: {.admonition-title}\nNote\n:::\n\n**Memory Store Restrictions**\n\nOne specificity of VTA is that it only supports DRAM stores in the\nnarrow `env.inp_dtype`{.sourceCode} data type format. This lets us\nreduce the data footprint for memory transfers (more on this in the\nbasic matrix multiply example).\n:::\n\nWe perform one last typecast operation to the narrow input activation\ndata format.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Cast to output type, and send to main memory\nC = te.compute(\n    (o, m, env.BATCH, env.BLOCK_OUT), lambda *i: C_buf(*i).astype(env.inp_dtype), name=\"C\"\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This concludes the computation declaration part of this tutorial.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Scheduling the Computation\n==========================\n\nWhile the above lines describes the computation rule, we can obtain\n`C`{.sourceCode} in many ways. TVM asks the user to provide an\nimplementation of the computation called *schedule*.\n\nA schedule is a set of transformations to an original computation that\ntransforms the implementation of the computation without affecting\ncorrectness. This simple VTA programming tutorial aims to demonstrate\nbasic schedule transformations that will map the original schedule down\nto VTA hardware primitives.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Default Schedule\n================\n\nAfter we construct the schedule, by default the schedule computes\n`C`{.sourceCode} in the following way:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Let's take a look at the generated schedule\ns = te.create_schedule(C.op)\n\nprint(tvm.lower(s, [A, B, C], simple_mode=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Although this schedule makes sense, it won\\'t compile to VTA. In order\nto obtain correct code generation, we need to apply scheduling\nprimitives and code annotation that will transform the schedule into one\nthat can be directly lowered onto VTA hardware intrinsics. Those\ninclude:\n\n> -   DMA copy operations which will take globally-scoped tensors and\n>     copy those into locally-scoped tensors.\n> -   Vector ALU operations that will perform the vector add.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Buffer Scopes\n=============\n\nFirst, we set the scope of the copy buffers to indicate to TVM that\nthese intermediate tensors will be stored in the VTA\\'s on-chip SRAM\nbuffers. Below, we tell TVM that `A_buf`{.sourceCode},\n`B_buf`{.sourceCode}, `C_buf`{.sourceCode} will live in VTA\\'s on-chip\n*accumulator buffer* which serves as VTA\\'s general purpose register\nfile.\n\nSet the intermediate tensors\\' scope to VTA\\'s on-chip accumulator\nbuffer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "s[A_buf].set_scope(env.acc_scope)\ns[B_buf].set_scope(env.acc_scope)\ns[C_buf].set_scope(env.acc_scope)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "DMA Transfers\n=============\n\nWe need to schedule DMA transfers to move data living in DRAM to and\nfrom the VTA on-chip buffers. We insert `dma_copy`{.sourceCode} pragmas\nto indicate to the compiler that the copy operations will be performed\nin bulk via DMA, which is common in hardware accelerators.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Tag the buffer copies with the DMA pragma to map a copy loop to a\n# DMA transfer operation\ns[A_buf].pragma(s[A_buf].op.axis[0], env.dma_copy)\ns[B_buf].pragma(s[B_buf].op.axis[0], env.dma_copy)\ns[C].pragma(s[C].op.axis[0], env.dma_copy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "ALU Operations\n==============\n\nVTA has a vector ALU that can perform vector operations on tensors in\nthe accumulator buffer. In order to tell TVM that a given operation\nneeds to be mapped to the VTA\\'s vector ALU, we need to explicitly tag\nthe vector addition loop with an `env.alu`{.sourceCode} pragma.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Tell TVM that the computation needs to be performed\n# on VTA's vector ALU\ns[C_buf].pragma(C_buf.op.axis[0], env.alu)\n\n# Let's take a look at the finalized schedule\nprint(vta.lower(s, [A, B, C], simple_mode=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This concludes the scheduling portion of this tutorial.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "TVM Compilation\n===============\n\nAfter we have finished specifying the schedule, we can compile it into a\nTVM function. By default TVM compiles into a type-erased function that\ncan be directly called from python side.\n\nIn the following line, we use `tvm.build`{.sourceCode} to create a\nfunction. The build function takes the schedule, the desired signature\nof the function(including the inputs and outputs) as well as target\nlanguage we want to compile to.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "my_vadd = vta.build(\n    s, [A, B, C], tvm.target.Target(\"ext_dev\", host=env.target_host), name=\"my_vadd\"\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Saving the Module\n=================\n\nTVM lets us save our module into a file so it can loaded back later.\nThis is called ahead-of-time compilation and allows us to save some\ncompilation time. More importantly, this allows us to cross-compile the\nexecutable on our development machine and send it over to the Pynq FPGA\nboard over RPC for execution.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Write the compiled module into an object file.\ntemp = utils.tempdir()\nmy_vadd.save(temp.relpath(\"vadd.o\"))\n\n# Send the executable over RPC\nremote.upload(temp.relpath(\"vadd.o\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Loading the Module\n==================\n\nWe can load the compiled module from the file system to run the code.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "f = remote.load_module(\"vadd.o\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Running the Function\n====================\n\nThe compiled TVM function uses a concise C API and can be invoked from\nany language.\n\nTVM provides an array API in python to aid quick testing and\nprototyping. The array API is based on\n[DLPack](https://github.com/dmlc/dlpack) standard.\n\n-   We first create a remote context (for remote execution on the Pynq).\n-   Then `tvm.nd.array`{.sourceCode} formats the data accordingly.\n-   `f()`{.sourceCode} runs the actual computation.\n-   `numpy()`{.sourceCode} copies the result array back in a format that\n    can be interpreted.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Get the remote device context\nctx = remote.ext_dev(0)\n\n# Initialize the A and B arrays randomly in the int range of (-128, 128]\nA_orig = np.random.randint(-128, 128, size=(o * env.BATCH, m * env.BLOCK_OUT)).astype(A.dtype)\nB_orig = np.random.randint(-128, 128, size=(o * env.BATCH, m * env.BLOCK_OUT)).astype(B.dtype)\n\n# Apply packing to the A and B arrays from a 2D to a 4D packed layout\nA_packed = A_orig.reshape(o, env.BATCH, m, env.BLOCK_OUT).transpose((0, 2, 1, 3))\nB_packed = B_orig.reshape(o, env.BATCH, m, env.BLOCK_OUT).transpose((0, 2, 1, 3))\n\n# Format the input/output arrays with tvm.nd.array to the DLPack standard\nA_nd = tvm.nd.array(A_packed, ctx)\nB_nd = tvm.nd.array(B_packed, ctx)\nC_nd = tvm.nd.array(np.zeros((o, m, env.BATCH, env.BLOCK_OUT)).astype(C.dtype), ctx)\n\n# Invoke the module to perform the computation\nf(A_nd, B_nd, C_nd)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Verifying Correctness\n=====================\n\nCompute the reference result with numpy and assert that the output of\nthe matrix multiplication indeed is correct\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Compute reference result with numpy\nC_ref = (A_orig.astype(env.acc_dtype) + B_orig.astype(env.acc_dtype)).astype(C.dtype)\nC_ref = C_ref.reshape(o, env.BATCH, m, env.BLOCK_OUT).transpose((0, 2, 1, 3))\nnp.testing.assert_equal(C_ref, C_nd.numpy())\nprint(\"Successful vector add test!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Summary\n=======\n\nThis tutorial provides a walk-through of TVM for programming the deep\nlearning accelerator VTA with a simple vector addition example. The\ngeneral workflow includes:\n\n-   Programming the FPGA with the VTA bitstream over RPC.\n-   Describing the vector add computation via a series of computations.\n-   Describing how we want to perform the computation using schedule\n    primitives.\n-   Compiling the function to the VTA target.\n-   Running the compiled module and verifying it against a numpy\n    implementation.\n\nYou are more than welcome to check other examples out and tutorials to\nlearn more about the supported operations, schedule primitives and other\nfeatures supported by TVM to program VTA.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}