{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%%bash\n# Installs the latest dev build of TVM from PyPI. If you wish to build\n# from source, see https://tvm.apache.org/docs/install/from_source.html\npip install apache-tvm --pre"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Training Vision Models for microTVM on Arduino {#microtvm-train-arduino}\n==============================================\n\n**Author**: [Gavin Uberti](https://github.com/guberti)\n\nThis tutorial shows how MobileNetV1 models can be trained to fit on\nembedded devices, and how those models can be deployed to Arduino using\nTVM.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Motivation\n==========\n\nWhen building IOT devices, we often want them to **see and understand**\nthe world around them. This can take many forms, but often times a\ndevice will want to know if a certain **kind of object** is in its field\nof vision.\n\nFor example, a security camera might look for **people**, so it can\ndecide whether to save a video to memory. A traffic light might look for\n**cars**, so it can judge which lights should change first. Or a forest\ncamera might look for a **kind of animal**, so they can estimate how\nlarge the animal population is.\n\nTo make these devices affordable, we would like them to need only a\nlow-cost processor like the\n[nRF52840](https://www.nordicsemi.com/Products/nRF52840) (costing five\ndollars each on Mouser) or the\n[RP2040](https://www.raspberrypi.com/products/rp2040/) (just \\$1.45\neach!).\n\nThese devices have very little memory (\\~250 KB RAM), meaning that no\nconventional edge AI vision model (like MobileNet or EfficientNet) will\nbe able to run. In this tutorial, we will show how these models can be\nmodified to work around this requirement. Then, we will use TVM to\ncompile and deploy it for an Arduino that uses one of these processors.\n\nInstalling the Prerequisites\n----------------------------\n\nThis tutorial will use TensorFlow to train the model - a widely used\nmachine learning library created by Google. TensorFlow is a very\nlow-level library, however, so we will the Keras interface to talk to\nTensorFlow. We will also use TensorFlow Lite to perform quantization on\nour model, as TensorFlow by itself does not support this.\n\nOnce we have our generated model, we will use TVM to compile and test\nit. To avoid having to build from source, we\\'ll install `tlcpack` - a\ncommunity build of TVM. Lastly, we\\'ll also install `imagemagick` and\n`curl` to preprocess data:\n\n> ``` {.sourceCode .bash}\n> %%shell\n> pip install -q tensorflow tflite\n> pip install -q tlcpack-nightly -f https://tlcpack.ai/wheels\n> apt-get -qq install imagemagick curl\n>\n> # Install Arduino CLI and library for Nano 33 BLE\n> curl -fsSL https://raw.githubusercontent.com/arduino/arduino-cli/master/install.sh | sh\n> /content/bin/arduino-cli core update-index\n> /content/bin/arduino-cli core install arduino:mbed_nano\n> ```\n\nUsing the GPU\n-------------\n\nThis tutorial demonstrates training a neural network, which is requires\na lot of computing power and will go much faster if you have a GPU. If\nyou are viewing this tutorial on Google Colab, you can enable a GPU by\ngoing to **Runtime-\\>Change runtime type** and selecting \\\"GPU\\\" as our\nhardware accelerator. If you are running locally, you can [follow\nTensorFlow\\'s guide](https://www.tensorflow.org/guide/gpu) instead.\n\nWe can test our GPU installation with the following code:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n\nif not tf.test.gpu_device_name():\n    print(\"No GPU was detected!\")\n    print(\"Model training will take much longer (~30 minutes instead of ~5)\")\nelse:\n    print(\"GPU detected - you're good to go.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Choosing Our Work Dir\n=====================\n\nWe need to pick a directory where our image datasets, trained model, and\neventual Arduino sketch will all live. If running on Google Colab,\nwe\\'ll save everything in `/root` (aka `~`) but you\\'ll probably want to\nstore it elsewhere if running locally. Note that this variable only\naffects Python scripts - you\\'ll have to adjust the Bash commands too.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import os\n\nFOLDER = \"/root\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Downloading the Data\n====================\n\nConvolutional neural networks usually learn by looking at many images,\nalong with labels telling the network what those images are. To get\nthese images, we\\'ll need a publicly available dataset with thousands of\nimages of all sorts of objects and labels of what\\'s in each image.\nWe\\'ll also need a bunch of images that **aren\\'t** of cars, as we\\'re\ntrying to distinguish these two classes.\n\nIn this tutorial, we\\'ll create a model to detect if an image contains a\n**car**, but you can use whatever category you like! Just change the\nsource URL below to one containing images of another type of object.\n\nTo get our car images, we\\'ll be downloading the [Stanford Cars\ndataset](http://ai.stanford.edu/~jkrause/cars/car_dataset.html), which\ncontains 16,185 full color images of cars. We\\'ll also need images of\nrandom things that aren\\'t cars, so we\\'ll use the [COCO\n2017](https://cocodataset.org/#home) validation set (it\\'s smaller, and\nthus faster to download than the full training set. Training on the full\ndata set would yield better results). Note that there are some cars in\nthe COCO 2017 data set, but it\\'s a small enough fraction not to matter\n- just keep in mind that this will drive down our percieved accuracy\nslightly.\n\nWe could use the TensorFlow dataloader utilities, but we\\'ll instead do\nit manually to make sure it\\'s easy to change the datasets being used.\nWe\\'ll end up with the following file hierarchy:\n\n> ``` {.sourceCode .}\n> /root\n> \u251c\u2500\u2500 images\n> \u2502   \u251c\u2500\u2500 object\n> \u2502   \u2502   \u251c\u2500\u2500 000001.jpg\n> \u2502   \u2502   \u2502 ...\n> \u2502   \u2502   \u2514\u2500\u2500 016185.jpg\n> \u2502   \u251c\u2500\u2500 object.tgz\n> \u2502   \u251c\u2500\u2500 random\n> \u2502   \u2502   \u251c\u2500\u2500 000000000139.jpg\n> \u2502   \u2502   \u2502 ...\n> \u2502   \u2502   \u2514\u2500\u2500 000000581781.jpg\n> \u2502   \u2514\u2500\u2500 random.zip\n> ```\n\nWe should also note that Stanford cars has 8k images, while the COCO\n2017 validation set is 5k images - it is not a 50/50 split! If we wanted\nto, we could weight these classes differently during training to correct\nfor this, but training will still work if we ignore it. It should take\nabout **2 minutes** to download the Stanford Cars, while COCO 2017\nvalidation will take **1 minute**.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import os\nimport shutil\nimport urllib.request\n\n# Download datasets\nos.makedirs(f\"{FOLDER}/downloads\")\nos.makedirs(f\"{FOLDER}/images\")\nurllib.request.urlretrieve(\n    \"https://data.deepai.org/stanfordcars.zip\", f\"{FOLDER}/downloads/target.zip\"\n)\nurllib.request.urlretrieve(\n    \"http://images.cocodataset.org/zips/val2017.zip\", f\"{FOLDER}/downloads/random.zip\"\n)\n\n# Extract them and rename their folders\nshutil.unpack_archive(f\"{FOLDER}/downloads/target.zip\", f\"{FOLDER}/downloads\")\nshutil.unpack_archive(f\"{FOLDER}/downloads/random.zip\", f\"{FOLDER}/downloads\")\nshutil.move(f\"{FOLDER}/downloads/cars_train/cars_train\", f\"{FOLDER}/images/target\")\nshutil.move(f\"{FOLDER}/downloads/val2017\", f\"{FOLDER}/images/random\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Loading the Data\n================\n\nCurrently, our data is stored on-disk as JPG files of various sizes. To\ntrain with it, we\\'ll have to load the images into memory, resize them\nto be 64x64, and convert them to raw, uncompressed data. Keras\\'s\n`image_dataset_from_directory` will take care of most of this, though it\nloads images such that each pixel value is a float from 0 to 255.\n\nWe\\'ll also need to load labels, though Keras will help with this. From\nour subdirectory structure, it knows the images in `/objects` are one\nclass, and those in `/random` another. Setting\n`label_mode='categorical'` tells Keras to convert these into\n**categorical labels** - a 2x1 vector that\\'s either `[1, 0]` for an\nobject of our target class, or `[0, 1]` vector for anything else. We\\'ll\nalso set `shuffle=True` to randomize the order of our examples.\n\nWe will also **batch** the data - grouping samples into clumps to make\nour training go faster. Setting `batch_size = 32` is a decent number.\n\nLastly, in machine learning we generally want our inputs to be small\nnumbers. We\\'ll thus use a `Rescaling` layer to change our images such\nthat each pixel is a float between `0.0` and `1.0`, instead of `0` to\n`255`. We need to be careful not to rescale our categorical labels\nthough, so we\\'ll use a `lambda` function.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "IMAGE_SIZE = (64, 64, 3)\nunscaled_dataset = tf.keras.utils.image_dataset_from_directory(\n    f\"{FOLDER}/images\",\n    batch_size=32,\n    shuffle=True,\n    label_mode=\"categorical\",\n    image_size=IMAGE_SIZE[0:2],\n)\nrescale = tf.keras.layers.Rescaling(scale=1.0 / 255)\nfull_dataset = unscaled_dataset.map(lambda im, lbl: (rescale(im), lbl))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "What\\'s Inside Our Dataset?\n===========================\n\nBefore giving this data set to our neural network, we ought to give it a\nquick visual inspection. Does the data look properly transformed? Do the\nlabels seem appropriate? And what\\'s our ratio of objects to other\nstuff? We can display some examples from our datasets using\n`matplotlib`:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n\nnum_target_class = len(os.listdir(f\"{FOLDER}/images/target/\"))\nnum_random_class = len(os.listdir(f\"{FOLDER}/images/random/\"))\nprint(f\"{FOLDER}/images/target contains {num_target_class} images\")\nprint(f\"{FOLDER}/images/random contains {num_random_class} images\")\n\n# Show some samples and their labels\nSAMPLES_TO_SHOW = 10\nplt.figure(figsize=(20, 10))\nfor i, (image, label) in enumerate(unscaled_dataset.unbatch()):\n    if i >= SAMPLES_TO_SHOW:\n        break\n    ax = plt.subplot(1, SAMPLES_TO_SHOW, i + 1)\n    plt.imshow(image.numpy().astype(\"uint8\"))\n    plt.title(list(label.numpy()))\n    plt.axis(\"off\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Validating our Accuracy\n=======================\n\nWhile developing our model, we\\'ll often want to check how accurate it\nis (e.g. to see if it improves during training). How do we do this? We\ncould just train it on *all* of the data, and then ask it to classify\nthat same data. However, our model could cheat by just memorizing all of\nthe samples, which would make it *appear* to have very high accuracy,\nbut perform very badly in reality. In practice, this \\\"memorizing\\\" is\ncalled **overfitting**.\n\nTo prevent this, we will set aside some of the data (we\\'ll use 20%) as\na **validation set**. Our model will never be trained on validation data\n- we\\'ll only use it to check our model\\'s accuracy.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "num_batches = len(full_dataset)\ntrain_dataset = full_dataset.take(int(num_batches * 0.8))\nvalidation_dataset = full_dataset.skip(len(train_dataset))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Loading the Data\n================\n\nIn the past decade, [convolutional neural\nnetworks](https://en.wikipedia.org/wiki/Convolutional_neural_network)\nhave been widely adopted for image classification tasks.\nState-of-the-art models like [EfficientNet\nV2](https://arxiv.org/abs/2104.00298) are able to perform image\nclassification better than even humans! Unfortunately, these models have\ntens of millions of parameters, and thus won\\'t fit on cheap security\ncamera computers.\n\nOur applications generally don\\'t need perfect accuracy - 90% is good\nenough. We can thus use the older and smaller MobileNet V1 architecture.\nBut this *still* won\\'t be small enough - by default, MobileNet V1 with\n224x224 inputs and alpha 1.0 takes \\~50 MB to just **store**. To reduce\nthe size of the model, there are three knobs we can turn. First, we can\nreduce the size of the input images from 224x224 to 96x96 or 64x64, and\nKeras makes it easy to do this. We can also reduce the **alpha** of the\nmodel, from 1.0 to 0.25, which downscales the width of the network (and\nthe number of filters) by a factor of four. And if we were really\nstrapped for space, we could reduce the number of **channels** by making\nour model take grayscale images instead of RGB ones.\n\nIn this tutorial, we will use an RGB 64x64 input image and alpha 0.25.\nThis is not quite ideal, but it allows the finished model to fit in 192\nKB of RAM, while still letting us perform transfer learning using the\nofficial TensorFlow source models (if we used alpha \\<0.25 or a\ngrayscale input, we wouldn\\'t be able to do this).\n\nWhat is Transfer Learning?\n--------------------------\n\nDeep learning has [dominated image\nclassification](https://paperswithcode.com/sota/image-classification-on-imagenet)\nfor a long time, but training neural networks takes a lot of time. When\na neural network is trained \\\"from scratch\\\", its parameters start out\nrandomly initialized, forcing it to learn very slowly how to tell images\napart.\n\nWith transfer learning, we instead start with a neural network that\\'s\n**already** good at a specific task. In this example, that task is\nclassifying images from [the ImageNet\ndatabase](https://www.image-net.org/). This means the network already\nhas some object detection capabilities, and is likely closer to what you\nwant then a random model would be.\n\nThis works especially well with image processing neural networks like\nMobileNet. In practice, it turns out the convolutional layers of the\nmodel (i.e. the first 90% of the layers) are used for identifying\nlow-level features like lines and shapes - only the last few fully\nconnected layers are used to determine how those shapes make up the\nobjects the network is trying to detect.\n\nWe can take advantage of this by starting training with a MobileNet\nmodel that was trained on ImageNet, and already knows how to identify\nthose lines and shapes. We can then just remove the last few layers from\nthis pretrained model, and add our own final layers. We\\'ll then train\nthis conglomerate model for a few epochs on our cars vs non-cars\ndataset, to adjust the first layers and train from scratch the last\nlayers. This process of training an already-partially-trained model is\ncalled *fine-tuning*.\n\nSource MobileNets for transfer learning have been [pretrained by the\nTensorFlow\nfolks](https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet_v1.md),\nso we can just download the one closest to what we want (the 128x128\ninput model with 0.25 depth scale).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "os.makedirs(f\"{FOLDER}/models\")\nWEIGHTS_PATH = f\"{FOLDER}/models/mobilenet_2_5_128_tf.h5\"\nurllib.request.urlretrieve(\n    \"https://storage.googleapis.com/tensorflow/keras-applications/mobilenet/mobilenet_2_5_128_tf.h5\",\n    WEIGHTS_PATH,\n)\n\npretrained = tf.keras.applications.MobileNet(\n    input_shape=IMAGE_SIZE, weights=WEIGHTS_PATH, alpha=0.25\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Modifying Our Network\n=====================\n\nAs mentioned above, our pretrained model is designed to classify the\n1,000 ImageNet categories, but we want to convert it to classify cars.\nSince only the bottom few layers are task-specific, we\\'ll **cut off the\nlast five layers** of our original model. In their place we\\'ll build\nour own \\\"tail\\\" to the model by performing respape, dropout, flatten,\nand softmax operations.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "model = tf.keras.models.Sequential()\n\nmodel.add(tf.keras.layers.InputLayer(input_shape=IMAGE_SIZE))\nmodel.add(tf.keras.Model(inputs=pretrained.inputs, outputs=pretrained.layers[-5].output))\n\nmodel.add(tf.keras.layers.Reshape((-1,)))\nmodel.add(tf.keras.layers.Dropout(0.1))\nmodel.add(tf.keras.layers.Flatten())\nmodel.add(tf.keras.layers.Dense(2, activation=\"softmax\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Fine Tuning Our Network\n=======================\n\nWhen training neural networks, we must set a parameter called the\n**learning rate** that controls how fast our network learns. It must be\nset carefully - too slow, and our network will take forever to train;\ntoo fast, and our network won\\'t be able to learn some fine details.\nGenerally for Adam (the optimizer we\\'re using), `0.001` is a pretty\ngood learning rate (and is what\\'s recommended in the [original\npaper](https://arxiv.org/abs/1412.6980)). However, in this case `0.0005`\nseems to work a little better.\n\nWe\\'ll also pass the validation set from earlier to `model.fit`. This\nwill evaluate how good our model is each time we train it, and let us\ntrack how our model is improving. Once training is finished, the model\nshould have a validation accuracy around `0.98` (meaning it was right\n98% of the time on our validation set).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "model.compile(\n    optimizer=tf.keras.optimizers.Adam(learning_rate=0.0005),\n    loss=\"categorical_crossentropy\",\n    metrics=[\"accuracy\"],\n)\nmodel.fit(train_dataset, validation_data=validation_dataset, epochs=3, verbose=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Quantization\n============\n\nWe\\'ve done a decent job of reducing our model\\'s size so far - changing\nthe input dimension, along with removing the bottom layers reduced the\nmodel to just 219k parameters. However, each of these parameters is a\n`float32` that takes four bytes, so our model will take up almost one\nMB!\n\nAdditionally, it might be the case that our hardware doesn\\'t have\nbuilt-in support for floating point numbers. While most high-memory\nArduinos (like the Nano 33 BLE) do have hardware support, some others\n(like the Arduino Due) do not. On any boards *without* dedicated\nhardware support, floating point multiplication will be extremely slow.\n\nTo address both issues we will **quantize** the model - representing the\nweights as eight bit integers. It\\'s more complex than just rounding,\nthough - to get the best performance, TensorFlow tracks how each neuron\nin our model activates, so we can figure out how most accurately\nsimulate the neuron\\'s original activations with integer operations.\n\nWe will help TensorFlow do this by creating a representative dataset - a\nsubset of the original that is used for tracking how those neurons\nactivate. We\\'ll then pass this into a `TFLiteConverter` (Keras itself\ndoes not have quantization support) with an `Optimize` flag to tell\nTFLite to perform the conversion. By default, TFLite keeps the inputs\nand outputs of our model as floats, so we must explicitly tell it to\navoid this behavior.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def representative_dataset():\n    for image_batch, label_batch in full_dataset.take(10):\n        yield [image_batch]\n\n\nconverter = tf.lite.TFLiteConverter.from_keras_model(model)\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\nconverter.representative_dataset = representative_dataset\nconverter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\nconverter.inference_input_type = tf.uint8\nconverter.inference_output_type = tf.uint8\n\nquantized_model = converter.convert()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Download the Model if Desired\n=============================\n\nWe\\'ve now got a finished model that you can use locally or in other\ntutorials (try autotuning this model or viewing it on\n<https://netron.app/>). But before we do those things, we\\'ll have to\nwrite it to a file (`quantized.tflite`). If you\\'re running this\ntutorial on Google Colab, you\\'ll have to uncomment the last two lines\nto download the file after writing it.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "QUANTIZED_MODEL_PATH = f\"{FOLDER}/models/quantized.tflite\"\nwith open(QUANTIZED_MODEL_PATH, \"wb\") as f:\n    f.write(quantized_model)\n# from google.colab import files\n# files.download(QUANTIZED_MODEL_PATH)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Compiling With TVM For Arduino\n==============================\n\nTensorFlow has a built-in framework for deploying to microcontrollers -\n[TFLite Micro](https://www.tensorflow.org/lite/microcontrollers).\nHowever, it\\'s poorly supported by development boards and does not\nsupport autotuning. We will use Apache TVM instead.\n\nTVM can be used either with its command line interface (`tvmc`) or with\nits Python interface. The Python interface is fully-featured and more\nstable, so we\\'ll use it here.\n\nTVM is an optimizing compiler, and optimizations to our model are\nperformed in stages via **intermediate representations**. The first of\nthese is [Relay](https://arxiv.org/abs/1810.00952) a high-level\nintermediate representation emphasizing portability. The conversion from\n`.tflite` to Relay is done without any knowledge of our \\\"end goal\\\" -\nthe fact we intend to run this model on an Arduino.\n\nChoosing an Arduino Board\n-------------------------\n\nNext, we\\'ll have to decide exactly which Arduino board to use. The\nArduino sketch that we ultimately generate should be compatible with any\nboard, but knowing which board we are using in advance allows TVM to\nadjust its compilation strategy to get better performance.\n\nThere is one catch - we need enough **memory** (flash and RAM) to be\nable to run our model. We won\\'t ever be able to run a complex vision\nmodel like a MobileNet on an Arduino Uno - that board only has 2 kB of\nRAM and 32 kB of flash! Our model has \\~200,000 parameters, so there is\njust no way it could fit.\n\nFor this tutorial, we will use the Nano 33 BLE, which has 1 MB of flash\nmemory and 256 KB of RAM. However, any other Arduino with those specs or\nbetter should also work.\n\nGenerating our project\n----------------------\n\nNext, we\\'ll compile the model to TVM\\'s MLF (model library format)\nintermediate representation, which consists of C/C++ code and is\ndesigned for autotuning. To improve performance, we\\'ll tell TVM that\nwe\\'re compiling for the `nrf52840` microprocessor (the one the Nano 33\nBLE uses). We\\'ll also tell it to use the C runtime (abbreviated `crt`)\nand to use ahead-of-time memory allocation (abbreviated `aot`, which\nhelps reduce the model\\'s memory footprint). Lastly, we will disable\nvectorization with `\"tir.disable_vectorize\": True`, as C has no native\nvectorized types.\n\nOnce we have set these configuration parameters, we will call\n`tvm.relay.build` to compile our Relay model into the MLF intermediate\nrepresentation. From here, we just need to call\n`tvm.micro.generate_project` and pass in the Arduino template project to\nfinish compilation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import shutil\nimport tflite\nimport tvm\n\n# Method to load model is different in TFLite 1 vs 2\ntry:  # TFLite 2.1 and above\n    tflite_model = tflite.Model.GetRootAsModel(quantized_model, 0)\nexcept AttributeError:  # Fall back to TFLite 1.14 method\n    tflite_model = tflite.Model.Model.GetRootAsModel(quantized_model, 0)\n\n# Convert to the Relay intermediate representation\nmod, params = tvm.relay.frontend.from_tflite(tflite_model)\n\n# Set configuration flags to improve performance\ntarget = tvm.target.target.micro(\"nrf52840\")\nruntime = tvm.relay.backend.Runtime(\"crt\")\nexecutor = tvm.relay.backend.Executor(\"aot\", {\"unpacked-api\": True})\n\n# Convert to the MLF intermediate representation\nwith tvm.transform.PassContext(opt_level=3, config={\"tir.disable_vectorize\": True}):\n    mod = tvm.relay.build(mod, target, runtime=runtime, executor=executor, params=params)\n\n# Generate an Arduino project from the MLF intermediate representation\nshutil.rmtree(f\"{FOLDER}/models/project\", ignore_errors=True)\narduino_project = tvm.micro.generate_project(\n    tvm.micro.get_microtvm_template_projects(\"arduino\"),\n    mod,\n    f\"{FOLDER}/models/project\",\n    {\n        \"board\": \"nano33ble\",\n        \"arduino_cli_cmd\": \"/content/bin/arduino-cli\",\n        \"project_type\": \"example_project\",\n    },\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Testing our Arduino Project\n===========================\n\nConsider the following two 224x224 images from the author\\'s camera roll\n- one of a car, one not. We will test our Arduino project by loading\nboth of these images and executing the compiled model on them.\n\n![image](https://raw.githubusercontent.com/tlc-pack/web-data/main/testdata/microTVM/data/model_train_images_combined.png){.align-center\nwidth=\"600px\" height=\"200px\"}\n\nCurrently, these are 224x224 PNG images we can download from Imgur.\nBefore we can feed in these images, we\\'ll need to resize and convert\nthem to raw data, which can be done with `imagemagick`.\n\nIt\\'s also challenging to load raw data onto an Arduino, as only C/CPP\nfiles (and similar) are compiled. We can work around this by embedding\nour raw data in a hard-coded C array with the built-in utility `bin2c`\nthat will output a file like below:\n\n> ``` {.sourceCode .c}\n> static const unsigned char CAR_IMAGE[] = {\n>   0x22,0x23,0x14,0x22,\n>   ...\n>   0x07,0x0e,0x08,0x08\n> };\n> ```\n\nWe can do both of these things with a few lines of Bash code:\n\n> ``` {.sourceCode .bash}\n> %%shell\n> mkdir -p ~/tests\n> curl \"https://i.imgur.com/JBbEhxN.png\" -o ~/tests/car_224.png\n> convert ~/tests/car_224.png -resize 64 ~/tests/car_64.png\n> stream ~/tests/car_64.png ~/tests/car.raw\n> bin2c -c -st ~/tests/car.raw --name CAR_IMAGE > ~/models/project/car.c\n>\n> curl \"https://i.imgur.com/wkh7Dx2.png\" -o ~/tests/catan_224.png\n> convert ~/tests/catan_224.png -resize 64 ~/tests/catan_64.png\n> stream ~/tests/catan_64.png ~/tests/catan.raw\n> bin2c -c -st ~/tests/catan.raw --name CATAN_IMAGE > ~/models/project/catan.c\n> ```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Writing our Arduino Script\n==========================\n\nWe now need a little bit of Arduino code to read the two binary arrays\nwe just generated, run the model on them, and log the output to the\nserial monitor. This file will replace `arduino_sketch.ino` as the main\nfile of our sketch. You\\'ll have to copy this code in manually..\n\n> ``` {.sourceCode .c}\n> %%writefile /root/models/project.ino\n> #include \"src/model.h\"\n> #include \"car.c\"\n> #include \"catan.c\"\n>\n> void setup() {\n>   Serial.begin(9600);\n>   TVMInitialize();\n> }\n>\n> void loop() {\n>   uint8_t result_data[2];\n>   Serial.println(\"Car results:\");\n>   TVMExecute(const_cast<uint8_t*>(CAR_IMAGE), result_data);\n>   Serial.print(result_data[0]); Serial.print(\", \");\n>   Serial.print(result_data[1]); Serial.println();\n>\n>   Serial.println(\"Other object results:\");\n>   TVMExecute(const_cast<uint8_t*>(CATAN_IMAGE), result_data);\n>   Serial.print(result_data[0]); Serial.print(\", \");\n>   Serial.print(result_data[1]); Serial.println();\n>\n>   delay(1000);\n> }\n> ```\n\nCompiling Our Code\n------------------\n\nNow that our project has been generated, TVM\\'s job is mostly done! We\ncan still call `arduino_project.build()` and `arduino_project.upload()`,\nbut these just use `arduino-cli`\\'s compile and flash commands\nunderneath. We could also begin autotuning our model, but that\\'s a\nsubject for a different tutorial. To finish up, we\\'ll verify no\ncompiler errors are thrown by our project:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "shutil.rmtree(f\"{FOLDER}/models/project/build\", ignore_errors=True)\narduino_project.build()\nprint(\"Compilation succeeded!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Uploading to Our Device\n=======================\n\nThe very last step is uploading our sketch to an Arduino to make sure\nour code works properly. Unfortunately, we can\\'t do that from Google\nColab, so we\\'ll have to download our sketch. This is simple enough to\ndo - we\\'ll just turn our project into a [.zip]{.title-ref} archive, and\ncall [files.download]{.title-ref}. If you\\'re running on Google Colab,\nyou\\'ll have to uncomment the last two lines to download the file after\nwriting it.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "ZIP_FOLDER = f\"{FOLDER}/models/project\"\nshutil.make_archive(ZIP_FOLDER, \"zip\", ZIP_FOLDER)\n# from google.colab import files\n# files.download(f\"{FOLDER}/models/project.zip\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "From here, we\\'ll need to open it in the Arduino IDE. You\\'ll have to\ndownload the IDE as well as the SDK for whichever board you are using.\nFor certain boards like the Sony SPRESENSE, you may have to change\nsettings to control how much memory you want the board to use.\n\nExpected Results\n================\n\nIf all works as expected, you should see the following output on a\nSerial monitor:\n\n> ``` {.sourceCode .}\n> Car results:\n> 255, 0\n> Other object results:\n> 0, 255\n> ```\n\nThe first number represents the model\\'s confidence that the object\n**is** a car and ranges from 0-255. The second number represents the\nmodel\\'s confidence that the object **is not** a car and is also 0-255.\nThese results mean the model is very sure that the first image is a car,\nand the second image is not (which is correct). Hence, our model is\nworking!\n\nSummary\n-------\n\nIn this tutorial, we used transfer learning to quickly train an image\nrecognition model to identify cars. We modified its input dimensions and\nlast few layers to make it better at this, and to make it faster and\nsmaller. We then quantified the model and compiled it using TVM to\ncreate an Arduino sketch. Lastly, we tested the model using two static\nimages to prove it works as intended.\n\nNext Steps\n==========\n\nFrom here, we could modify the model to read live images from the camera\n- we have another Arduino tutorial for how to do that [on\nGitHub](https://github.com/guberti/tvm-arduino-demos/tree/master/examples/person_detection).\nAlternatively, we could also [use TVM\\'s autotuning\ncapabilities](https://tvm.apache.org/docs/how_to/work_with_microtvm/micro_autotune.html)\nto dramatically improve the model\\'s performance.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}