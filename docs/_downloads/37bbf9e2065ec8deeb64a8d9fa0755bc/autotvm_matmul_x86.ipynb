{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%%bash\n# Installs the latest dev build of TVM from PyPI, with CUDA enabled. To use this,\n# you must request a Google Colab instance with a GPU by going to Runtime ->\n# Change runtime type -> Hardware accelerator -> GPU. If you wish to build from\n# source, see see https://tvm.apache.org/docs/install/from_source.html\npip install tlcpack-nightly-cu113 --pre -f https://tlcpack.ai/wheels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Optimizing Operators with Schedule Templates and AutoTVM {#tutorial-autotvm-matmul-x86}\n========================================================\n\n**Authors**: [Lianmin Zheng](https://github.com/merrymercy), [Chris\nHoge](https://github.com/hogepodge)\n\nIn this tutorial, we show how the TVM Tensor Expression (TE) language\ncan be used to write schedule templates that can be searched by AutoTVM\nto find the optimal schedule. This process is called Auto-Tuning, which\nhelps automate the process of optimizing tensor computation.\n\nThis tutorial builds on the previous `tutorial on how to write a matrix\nmultiplication using TE <tensor_expr_get_started>`{.interpreted-text\nrole=\"doc\"}.\n\nThere are two steps in auto-tuning.\n\n-   The first step is defining a search space.\n-   The second step is running a search algorithm to explore through\n    this space.\n\nIn this tutorial, you can learn how to perform these two steps in TVM.\nThe whole workflow is illustrated by a matrix multiplication example.\n\n::: {.note}\n::: {.admonition-title}\nNote\n:::\n\nNote that this tutorial will not run on Windows or recent versions of\nmacOS. To get it to run, you will need to wrap the body of this tutorial\nin a `if __name__ == \"__main__\":`{.sourceCode} block.\n:::\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Install dependencies\n====================\n\nTo use autotvm package in TVM, we need to install some extra\ndependencies.\n\n``` {.sourceCode .bash}\npip3 install --user psutil xgboost cloudpickle\n```\n\nTo make TVM run faster in tuning, it is recommended to use cython as FFI\nof TVM. In the root directory of TVM, execute:\n\n``` {.sourceCode .bash}\npip3 install --user cython\nsudo make cython3\n```\n\nNow return to python code. Begin by importing the required packages.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import logging\nimport sys\n\nimport numpy as np\nimport tvm\nfrom tvm import te\nimport tvm.testing\n\n# the module is called `autotvm`\nfrom tvm import autotvm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Basic Matrix Multiplication with TE\n===================================\n\nRecall the basic implementation of matrix multiplication using TE. We\nwrite it down here with a few changes. We will wrap the multiplication\nin a python function definition. For simplicity, we will focus our\nattention on a split optimization, using a fixed value that defines the\nblock size of the reordering.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def matmul_basic(N, L, M, dtype):\n\n    A = te.placeholder((N, L), name=\"A\", dtype=dtype)\n    B = te.placeholder((L, M), name=\"B\", dtype=dtype)\n\n    k = te.reduce_axis((0, L), name=\"k\")\n    C = te.compute((N, M), lambda i, j: te.sum(A[i, k] * B[k, j], axis=k), name=\"C\")\n    s = te.create_schedule(C.op)\n\n    # schedule\n    y, x = s[C].op.axis\n    k = s[C].op.reduce_axis[0]\n\n    yo, yi = s[C].split(y, 8)\n    xo, xi = s[C].split(x, 8)\n\n    s[C].reorder(yo, xo, k, yi, xi)\n\n    return s, [A, B, C]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Matrix Multiplication with AutoTVM\n==================================\n\nIn the previous schedule code, we use a constant \\\"8\\\" as the tiling\nfactor. However, it might not be the best one because the best tiling\nfactor depends on real hardware environment and input shape.\n\nIf you want the schedule code to be portable across a wider range of\ninput shapes and target hardware, it is better to define a set of\ncandidate values and pick the best one according to the measurement\nresults on target hardware.\n\nIn autotvm, we can define a tunable parameter, or a \\\"knob\\\" for such\nkind of value.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "A Basic Matrix Multiplication Template\n======================================\n\nWe begin with an example of how to create a tunable parameter set for\nthe block size of the [split]{.title-ref} scheduling operation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Matmul V1: List candidate values\n@autotvm.template(\"tutorial/matmul_v1\")  # 1. use a decorator\ndef matmul_v1(N, L, M, dtype):\n    A = te.placeholder((N, L), name=\"A\", dtype=dtype)\n    B = te.placeholder((L, M), name=\"B\", dtype=dtype)\n\n    k = te.reduce_axis((0, L), name=\"k\")\n    C = te.compute((N, M), lambda i, j: te.sum(A[i, k] * B[k, j], axis=k), name=\"C\")\n    s = te.create_schedule(C.op)\n\n    # schedule\n    y, x = s[C].op.axis\n    k = s[C].op.reduce_axis[0]\n\n    # 2. get the config object\n    cfg = autotvm.get_config()\n\n    # 3. define search space\n    cfg.define_knob(\"tile_y\", [1, 2, 4, 8, 16])\n    cfg.define_knob(\"tile_x\", [1, 2, 4, 8, 16])\n\n    # 4. schedule according to config\n    yo, yi = s[C].split(y, cfg[\"tile_y\"].val)\n    xo, xi = s[C].split(x, cfg[\"tile_x\"].val)\n\n    s[C].reorder(yo, xo, k, yi, xi)\n\n    return s, [A, B, C]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here we make four modifications to the previous schedule code and get a\ntunable \\\"template\\\". We can explain the modifications one by one.\n\n1.  Use a decorator to mark this function as a simple template.\n2.  Get a config object: You can regard this `cfg`{.sourceCode} as an\n    argument of this function but we obtain it in a different way. With\n    this argument, this function is no longer a deterministic schedule.\n    Instead, we can pass different configurations to this function and\n    get different schedules. A function that uses a configuration object\n    like this is called a \\\"template\\\".\n\n    To make the template function more compact, we can do two things to\n    define the parameter search space within a single function.\n\n    1.  Define a search space across a set values. This is done by\n        making `cfg`{.sourceCode} a `ConfigSpace`{.interpreted-text\n        role=\"any\"} object. It will collect all of the tunable knobs in\n        this function and build a search space from it.\n    2.  Schedule according to an entity in this space. This is done by\n        making `cfg`{.sourceCode} a `ConfigEntity`{.interpreted-text\n        role=\"any\"} object. When it is a\n        `ConfigEntity`{.interpreted-text role=\"any\"}, it will ignore all\n        space definition API (namely,\n        `cfg.define_XXXXX(...)`{.sourceCode}). Instead, it will store\n        deterministic values for all tunable knobs, and we schedule\n        according to these values.\n\n    During auto-tuning, we will first call this template with a\n    `ConfigSpace`{.interpreted-text role=\"any\"} object to build the\n    search space. Then we call this template with different\n    `ConfigEntity`{.interpreted-text role=\"any\"} in the built space to\n    get different schedules. Finally we will measure the code generated\n    by different schedules and pick the best one.\n\n3.  Define two tunable knobs. The first one is `tile_y`{.sourceCode}\n    with 5 possible values. The second one is `tile_x`{.sourceCode} with\n    a same list of possible values. These two knobs are independent, so\n    they span a search space with size 25 = 5x5.\n4.  The configuration knobs are passed to the `split`{.sourceCode}\n    schedule operation, allowing us to schedule according to the 5x5\n    deterministic values we previously defined in `cfg`{.sourceCode}.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "A Matrix Multiplication Template with the Advanced Parameter API\n================================================================\n\nIn the previous template, we manually listed all of the possible values\nfor a knob. This is the lowest level API to define the space, and gives\nan explicit enumeration of the parameter space to search. However, we\nalso provide another set of APIs that can make the definition of the\nsearch space easier and smarter. Where possible, we recommend you use\nthis higher-level API\n\nIn the following example, we use\n`ConfigSpace.define_split`{.interpreted-text role=\"any\"} to define a\nsplit knob. It will enumerate all the possible ways to split an axis and\nconstruct the space.\n\nWe also have `ConfigSpace.define_reorder`{.interpreted-text role=\"any\"}\nfor reorder knob and `ConfigSpace.define_annotate`{.interpreted-text\nrole=\"any\"} for annotation like unroll, vectorization, thread binding.\nWhen the high level API cannot meet your requirements, you can always\nfall back to using the low level API.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "@autotvm.template(\"tutorial/matmul\")\ndef matmul(N, L, M, dtype):\n    A = te.placeholder((N, L), name=\"A\", dtype=dtype)\n    B = te.placeholder((L, M), name=\"B\", dtype=dtype)\n\n    k = te.reduce_axis((0, L), name=\"k\")\n    C = te.compute((N, M), lambda i, j: te.sum(A[i, k] * B[k, j], axis=k), name=\"C\")\n    s = te.create_schedule(C.op)\n\n    # schedule\n    y, x = s[C].op.axis\n    k = s[C].op.reduce_axis[0]\n\n    ##### define space begin #####\n    cfg = autotvm.get_config()\n    cfg.define_split(\"tile_y\", y, num_outputs=2)\n    cfg.define_split(\"tile_x\", x, num_outputs=2)\n    ##### define space end #####\n\n    # schedule according to config\n    yo, yi = cfg[\"tile_y\"].apply(s, C, y)\n    xo, xi = cfg[\"tile_x\"].apply(s, C, x)\n\n    s[C].reorder(yo, xo, k, yi, xi)\n\n    return s, [A, B, C]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "::: {.admonition}\nMore Explanation on `cfg.define_split`{.sourceCode}\n\nIn this template,\n`cfg.define_split(\"tile_y\", y, num_outputs=2)`{.sourceCode} will\nenumerate all possible combinations that can split axis y into two axes\nwith factors of the length of y. For example, if the length of y is 32\nand we want to split it into two axes using factors of 32, then there\nare 6 possible values for (length of outer axis, length of inner axis)\npair, namely (32, 1), (16, 2), (8, 4), (4, 8), (2, 16) or (1, 32). These\nare all 6 possible values of [tile\\_y]{.title-ref}.\n\nDuring scheduling, `cfg[\"tile_y\"]`{.sourceCode} is a\n`SplitEntity`{.sourceCode} object. We stores the lengths of outer axes\nand inner axes in `cfg['tile_y'].size`{.sourceCode} (a tuple with two\nelements). In this template, we apply it by using\n`yo, yi = cfg['tile_y'].apply(s, C, y)`{.sourceCode}. Actually, this is\nequivalent to `yo, yi = s[C].split(y,\ncfg[\"tile_y\"].size[1])`{.sourceCode} or `yo, yi = s[C].split(y,\nnparts=cfg['tile_y\"].size[0])`{.sourceCode}\n\nThe advantage of using cfg.apply API is that it makes multi-level splits\n(that is, when num\\_outputs \\>= 3) easier.\n:::\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Step 2: Use AutoTVM to Optimize the Matrix Multiplication\n=========================================================\n\nIn Step 1, we wrote a matrix multiplication template that allowed us to\nparameterize the block size used in the [split]{.title-ref} schedule. We\ncan now conduct a search over this parameter space. The next step is to\npick a tuner to guide the exploration of this space.\n\nAuto-tuners in TVM\n------------------\n\nThe job for a tuner can be described by following pseudo code\n\n> ``` {.sourceCode .c}\n> ct = 0\n> while ct < max_number_of_trials:\n>     propose a batch of configs\n>     measure this batch of configs on real hardware and get results\n>     ct += batch_size\n> ```\n\nWhen proposing the next batch of configs, the tuner can take different\nstrategies. Some of the tuner strategies provided by TVM include:\n\n-   `tvm.autotvm.tuner.RandomTuner`{.interpreted-text role=\"any\"}:\n    Enumerate the space in a random order\n-   `tvm.autotvm.tuner.GridSearchTuner`{.interpreted-text role=\"any\"}:\n    Enumerate the space in a grid search order\n-   `tvm.autotvm.tuner.GATuner`{.interpreted-text role=\"any\"}: Using\n    genetic algorithm to search through the space\n-   `tvm.autotvm.tuner.XGBTuner`{.interpreted-text role=\"any\"}: Uses a\n    model based method. Train a XGBoost model to predict the speed of\n    lowered IR and pick the next batch according to the prediction.\n\nYou can choose the tuner according to the size of your space, your time\nbudget and other factors. For example, if your space is very small (less\nthan 1000), a grid-search tuner or a random tuner is good enough. If\nyour space is at the level of 10\\^9 (this is the space size of a conv2d\noperator on CUDA GPU), XGBoostTuner can explore more efficiently and\nfind better configs.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Begin tuning\n============\n\nHere we continue our matrix multiplication example. First we create a\ntuning task. We can also inspect the initialized search space. In this\ncase, for a 512x512 square matrix multiplication, the space size is\n10x10=100 Note that the task and search space are independent of the\ntuner picked.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "N, L, M = 512, 512, 512\ntask = autotvm.task.create(\"tutorial/matmul\", args=(N, L, M, \"float32\"), target=\"llvm\")\nprint(task.config_space)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Then we need to define how to measure the generated code and pick a\ntuner. Since our space is small, a random tuner is just okay.\n\nWe only make 10 trials in this tutorial for demonstration. In practice,\nyou can do more trials according to your time budget. We will log the\ntuning results into a log file. This file can be used to choose the best\nconfiguration discovered by the tuner later.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# logging config (for printing tuning log to the screen)\nlogging.getLogger(\"autotvm\").setLevel(logging.DEBUG)\nlogging.getLogger(\"autotvm\").addHandler(logging.StreamHandler(sys.stdout))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "There are two steps for measuring a config: build and run. By default,\nwe use all CPU cores to compile program. We then measure them\nsequentially. To help reduce variance, we take 5 measurements and\naverage them.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "measure_option = autotvm.measure_option(builder=\"local\", runner=autotvm.LocalRunner(number=5))\n\n# Begin tuning with RandomTuner, log records to file `matmul.log`\n# You can use alternatives like XGBTuner.\ntuner = autotvm.tuner.RandomTuner(task)\ntuner.tune(\n    n_trial=10,\n    measure_option=measure_option,\n    callbacks=[autotvm.callback.log_to_file(\"matmul.log\")],\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "With tuning completed, we can choose the configuration from the log file\nthat has the best measured performance and compile the schedule with the\ncorresponding parameters. We also do a quick verification that the\nschedule is producing correct answers. We can call the function\n`matmul`{.sourceCode} directly under the\n`autotvm.apply_history_best`{.interpreted-text role=\"any\"} context. When\nwe call this function, it will query the dispatch context with its\nargument and get the best config with the same argument.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# apply history best from log file\nwith autotvm.apply_history_best(\"matmul.log\"):\n    with tvm.target.Target(\"llvm\"):\n        s, arg_bufs = matmul(N, L, M, \"float32\")\n        func = tvm.build(s, arg_bufs)\n\n# check correctness\na_np = np.random.uniform(size=(N, L)).astype(np.float32)\nb_np = np.random.uniform(size=(L, M)).astype(np.float32)\nc_np = a_np.dot(b_np)\n\nc_tvm = tvm.nd.empty(c_np.shape)\nfunc(tvm.nd.array(a_np), tvm.nd.array(b_np), c_tvm)\n\ntvm.testing.assert_allclose(c_np, c_tvm.numpy(), rtol=1e-4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Final Notes and Summary\n=======================\n\nIn this tutorial, we have shown how to build operator templates that\nallow TVM to search a parameter space and choose optimized schedule\nconfigurations. To gain a deeper understanding of how this works, we\nrecommend expanding on this example by adding new search parameters to\nthe schedule based on schedule operations demonstrated in the :ref:\n[Getting Started With Tensor Expressions\n\\<tensor\\_expr\\_get\\_started\\>\\_]{.title-ref} tutorial. In the upcoming\nsections, we will demonstrate the AutoScheduler, a method for TVM to\noptimize common operators without the need for the user to provide a\nuser-defined template.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}