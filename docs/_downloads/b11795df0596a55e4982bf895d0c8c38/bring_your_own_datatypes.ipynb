{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%%bash\n# Installs the latest dev build of TVM from PyPI. If you wish to build\n# from source, see https://tvm.apache.org/docs/install/from_source.html\npip install apache-tvm --pre"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Bring Your Own Datatypes to TVM\n===============================\n\n**Authors**: [Gus Smith](https://github.com/gussmith23), [Andrew\nLiu](https://github.com/hypercubestart)\n\nIn this tutorial, we will show you how to utilize the Bring Your Own\nDatatypes framework to use your own custom datatypes in TVM. Note that\nthe Bring Your Own Datatypes framework currently only handles **software\nemulated versions of datatypes**. The framework does not support\ncompiling for custom accelerator datatypes out-of-the-box.\n\nDatatype Libraries\n------------------\n\nThe Bring Your Own Datatypes allows users to register their own datatype\nimplementations alongside TVM\\'s native datatypes (such as `float`). In\nthe wild, these datatype implementations often appear as libraries. For\nexample:\n\n-   [libposit](https://github.com/cjdelisle/libposit), a posit library\n-   [Stillwater Universal](https://github.com/stillwater-sc/universal),\n    a library with posits, fixed-point numbers, and other types\n-   [SoftFloat](https://github.com/ucb-bar/berkeley-softfloat-3),\n    Berkeley\\'s software implementation of IEEE 754 floating-point\n\nThe Bring Your Own Datatypes enables users to plug these datatype\nimplementations into TVM!\n\nIn this section, we will use an example library we have already\nimplemented, located at `3rdparty/byodt/myfloat.cc`. This datatype,\nwhich we dubbed \\\"myfloat\\\", is really just a IEE-754 float\nunder-the-hood, but it serves a useful example to show that any datatype\ncan be used in the BYODT framework.\n\nSetup\n-----\n\nSince we do not use any 3rdparty library, there is no setup needed.\n\nIf you would like to try this with your own datatype library, first\nbring the library\\'s functions into the process space with `CDLL`:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "A Simple TVM Program\n====================\n\nWe\\'ll begin by writing a simple program in TVM; afterwards, we will\nre-write it to use custom datatypes.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import tvm\nfrom tvm import relay\n\n# Our basic program: Z = X + Y\nx = relay.var(\"x\", shape=(3,), dtype=\"float32\")\ny = relay.var(\"y\", shape=(3,), dtype=\"float32\")\nz = x + y\nprogram = relay.Function([x, y], z)\nmodule = tvm.IRModule.from_expr(program)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now, we create random inputs to feed into this program using numpy:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import numpy as np\n\nnp.random.seed(23)  # for reproducibility\n\nx_input = np.random.rand(3).astype(\"float32\")\ny_input = np.random.rand(3).astype(\"float32\")\nprint(\"x: {}\".format(x_input))\nprint(\"y: {}\".format(y_input))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Finally, we\\'re ready to run the program:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "z_output = relay.create_executor(mod=module).evaluate()(x_input, y_input)\nprint(\"z: {}\".format(z_output))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Adding Custom Datatypes\n=======================\n\nNow, we will do the same, but we will use a custom datatype for our\nintermediate computation.\n\nWe use the same input variables `x` and `y` as above, but before adding\n`x + y`, we first cast both `x` and `y` to a custom datatype via the\n`relay.cast(...)` call.\n\nNote how we specify the custom datatype: we indicate it using the\nspecial `custom[...]` syntax. Additionally, note the \\\"32\\\" after the\ndatatype: this is the bitwidth of the custom datatype. This tells TVM\nthat each instance of `myfloat` is 32 bits wide.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "try:\n    with tvm.transform.PassContext(config={\"tir.disable_vectorize\": True}):\n        x_myfloat = relay.cast(x, dtype=\"custom[myfloat]32\")\n        y_myfloat = relay.cast(y, dtype=\"custom[myfloat]32\")\n        z_myfloat = x_myfloat + y_myfloat\n        z = relay.cast(z_myfloat, dtype=\"float32\")\nexcept tvm.TVMError as e:\n    # Print last line of error\n    print(str(e).split(\"\\n\")[-1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Trying to generate this program throws an error from TVM. TVM does not\nknow how to handle any custom datatype out of the box! We first have to\nregister the custom type with TVM, giving it a name and a type code:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "tvm.target.datatype.register(\"myfloat\", 150)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Note that the type code, 150, is currently chosen manually by the user.\nSee `TVMTypeCode::kCustomBegin` in\n[include/tvm/runtime/c\\_runtime\\_api.h](https://github.com/apache/tvm/blob/main/include/tvm/runtime/data_type.h).\nNow we can generate our program again:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "x_myfloat = relay.cast(x, dtype=\"custom[myfloat]32\")\ny_myfloat = relay.cast(y, dtype=\"custom[myfloat]32\")\nz_myfloat = x_myfloat + y_myfloat\nz = relay.cast(z_myfloat, dtype=\"float32\")\nprogram = relay.Function([x, y], z)\nmodule = tvm.IRModule.from_expr(program)\nmodule = relay.transform.InferType()(module)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we have a Relay program that uses myfloat!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(program)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now that we can express our program without errors, let\\'s try running\nit!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "try:\n    with tvm.transform.PassContext(config={\"tir.disable_vectorize\": True}):\n        z_output_myfloat = relay.create_executor(\"graph\", mod=module).evaluate()(x_input, y_input)\n        print(\"z: {}\".format(y_myfloat))\nexcept tvm.TVMError as e:\n    # Print last line of error\n    print(str(e).split(\"\\n\")[-1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now, trying to compile this program throws an error. Let\\'s dissect this\nerror.\n\nThe error is occurring during the process of lowering the custom\ndatatype code to code that TVM can compile and run. TVM is telling us\nthat it cannot find a *lowering function* for the `Cast` operation, when\ncasting from source type 2 (`float`, in TVM), to destination type 150\n(our custom datatype). When lowering custom datatypes, if TVM encounters\nan operation over a custom datatype, it looks for a user-registered\n*lowering function*, which tells it how to lower the operation to an\noperation over datatypes it understands. We have not told TVM how to\nlower `Cast` operations for our custom datatypes; thus, the source of\nthis error.\n\nTo fix this error, we simply need to specify a lowering function:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "tvm.target.datatype.register_op(\n    tvm.target.datatype.create_lower_func(\n        {\n            (32, 32): \"FloatToCustom32\",  # cast from float32 to myfloat32\n        }\n    ),\n    \"Cast\",\n    \"llvm\",\n    \"float\",\n    \"myfloat\",\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The `register_op(...)` call takes a lowering function, and a number of\nparameters which specify exactly the operation which should be lowered\nwith the provided lowering function. In this case, the arguments we pass\nspecify that this lowering function is for lowering a `Cast` from\n`float` to `myfloat` for target `\"llvm\"`.\n\nThe lowering function passed into this call is very general: it should\ntake an operation of the specified type (in this case,\n[Cast]{.title-ref}) and return another operation which only uses\ndatatypes which TVM understands.\n\nIn the general case, we expect users to implement operations over their\ncustom datatypes using calls to an external library. In our example, our\n`myfloat` library implements a `Cast` from `float` to 32-bit `myfloat`\nin the function `FloatToCustom32`. To provide for the general case, we\nhave made a helper function, `create_lower_func(...)`, which does just\nthis: given a dictionary, it replaces the given operation with a `Call`\nto the appropriate function name provided based on the op and the bit\nwidths. It additionally removes usages of the custom datatype by storing\nthe custom datatype in an opaque `uint` of the appropriate width; in our\ncase, a `uint32_t`. For more information, see [the source\ncode](https://github.com/apache/tvm/blob/main/python/tvm/target/datatype.py).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# We can now re-try running the program:\ntry:\n    with tvm.transform.PassContext(config={\"tir.disable_vectorize\": True}):\n        z_output_myfloat = relay.create_executor(\"graph\", mod=module).evaluate()(x_input, y_input)\n        print(\"z: {}\".format(z_output_myfloat))\nexcept tvm.TVMError as e:\n    # Print last line of error\n    print(str(e).split(\"\\n\")[-1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This new error tells us that the `Add` lowering function is not found,\nwhich is good news, as it\\'s no longer complaining about the `Cast`! We\nknow what to do from here: we just need to register the lowering\nfunctions for the other operations in our program.\n\nNote that for `Add`, `create_lower_func` takes in a dict where the key\nis an integer. For `Cast` operations, we require a 2-tuple to specify\nthe `src_bit_length` and the `dest_bit_length`, while for all other\noperations, the bit length is the same between the operands so we only\nrequire one integer to specify `bit_length`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "tvm.target.datatype.register_op(\n    tvm.target.datatype.create_lower_func({32: \"Custom32Add\"}),\n    \"Add\",\n    \"llvm\",\n    \"myfloat\",\n)\ntvm.target.datatype.register_op(\n    tvm.target.datatype.create_lower_func({(32, 32): \"Custom32ToFloat\"}),\n    \"Cast\",\n    \"llvm\",\n    \"myfloat\",\n    \"float\",\n)\n\n# Now, we can run our program without errors.\nwith tvm.transform.PassContext(config={\"tir.disable_vectorize\": True}):\n    z_output_myfloat = relay.create_executor(mod=module).evaluate()(x_input, y_input)\nprint(\"z: {}\".format(z_output_myfloat))\n\nprint(\"x:\\t\\t{}\".format(x_input))\nprint(\"y:\\t\\t{}\".format(y_input))\nprint(\"z (float32):\\t{}\".format(z_output))\nprint(\"z (myfloat32):\\t{}\".format(z_output_myfloat))\n\n# Perhaps as expected, the ``myfloat32`` results and ``float32`` are exactly the same!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Running Models With Custom Datatypes\n====================================\n\nWe will first choose the model which we would like to run with myfloat.\nIn this case we use [Mobilenet](https://arxiv.org/abs/1704.04861). We\nchoose Mobilenet due to its small size. In this alpha state of the Bring\nYour Own Datatypes framework, we have not implemented any software\noptimizations for running software emulations of custom datatypes; the\nresult is poor performance due to many calls into our datatype emulation\nlibrary.\n\nFirst let us define two helper functions to get the mobilenet model and\na cat image.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def get_mobilenet():\n    dshape = (1, 3, 224, 224)\n    from mxnet.gluon.model_zoo.vision import get_model\n\n    block = get_model(\"mobilenet0.25\", pretrained=True)\n    shape_dict = {\"data\": dshape}\n    return relay.frontend.from_mxnet(block, shape_dict)\n\n\ndef get_cat_image():\n    from tvm.contrib.download import download_testdata\n    from PIL import Image\n\n    url = \"https://gist.githubusercontent.com/zhreshold/bcda4716699ac97ea44f791c24310193/raw/fa7ef0e9c9a5daea686d6473a62aacd1a5885849/cat.png\"\n    dst = \"cat.png\"\n    real_dst = download_testdata(url, dst, module=\"data\")\n    img = Image.open(real_dst).resize((224, 224))\n    # CoreML's standard model image format is BGR\n    img_bgr = np.array(img)[:, :, ::-1]\n    img = np.transpose(img_bgr, (2, 0, 1))[np.newaxis, :]\n    return np.asarray(img, dtype=\"float32\")\n\n\nmodule, params = get_mobilenet()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "It\\'s easy to execute MobileNet with native TVM:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "ex = tvm.relay.create_executor(\"graph\", mod=module, params=params)\ninput = get_cat_image()\nresult = ex.evaluate()(input).numpy()\n# print first 10 elements\nprint(result.flatten()[:10])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now, we would like to change the model to use myfloat internally. To do\nso, we need to convert the network. To do this, we first define a\nfunction which will help us convert tensors:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def convert_ndarray(dst_dtype, array):\n    \"\"\"Converts an NDArray into the specified datatype\"\"\"\n    x = relay.var(\"x\", shape=array.shape, dtype=str(array.dtype))\n    cast = relay.Function([x], x.astype(dst_dtype))\n    with tvm.transform.PassContext(config={\"tir.disable_vectorize\": True}):\n        return relay.create_executor(\"graph\").evaluate(cast)(array)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now, to actually convert the entire network, we have written [a pass in\nRelay](https://github.com/gussmith23/tvm/blob/ea174c01c54a2529e19ca71e125f5884e728da6e/python/tvm/relay/frontend/change_datatype.py#L21)\nwhich simply converts all nodes within the model to use the new\ndatatype.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from tvm.relay.frontend.change_datatype import ChangeDatatype\n\nsrc_dtype = \"float32\"\ndst_dtype = \"custom[myfloat]32\"\n\nmodule = relay.transform.InferType()(module)\n\n# Currently, custom datatypes only work if you run simplify_inference beforehand\nmodule = tvm.relay.transform.SimplifyInference()(module)\n\n# Run type inference before changing datatype\nmodule = tvm.relay.transform.InferType()(module)\n\n# Change datatype from float to myfloat and re-infer types\ncdtype = ChangeDatatype(src_dtype, dst_dtype)\nexpr = cdtype.visit(module[\"main\"])\nmodule = tvm.relay.transform.InferType()(module)\n\n# We also convert the parameters:\nparams = {k: convert_ndarray(dst_dtype, v) for k, v in params.items()}\n\n# We also need to convert our input:\ninput = convert_ndarray(dst_dtype, input)\n\n# Finally, we can try to run the converted model:\ntry:\n    # Vectorization is not implemented with custom datatypes.\n    with tvm.transform.PassContext(config={\"tir.disable_vectorize\": True}):\n        result_myfloat = tvm.relay.create_executor(\"graph\", mod=module).evaluate(expr)(\n            input, **params\n        )\nexcept tvm.TVMError as e:\n    print(str(e).split(\"\\n\")[-1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "When we attempt to run the model, we get a familiar error telling us\nthat more functions need to be registered for myfloat.\n\nBecause this is a neural network, many more operations are required.\nHere, we register all the needed functions:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "tvm.target.datatype.register_op(\n    tvm.target.datatype.create_lower_func({32: \"FloatToCustom32\"}),\n    \"FloatImm\",\n    \"llvm\",\n    \"myfloat\",\n)\n\ntvm.target.datatype.register_op(\n    tvm.target.datatype.lower_ite, \"Call\", \"llvm\", \"myfloat\", intrinsic_name=\"tir.if_then_else\"\n)\n\ntvm.target.datatype.register_op(\n    tvm.target.datatype.lower_call_pure_extern,\n    \"Call\",\n    \"llvm\",\n    \"myfloat\",\n    intrinsic_name=\"tir.call_pure_extern\",\n)\n\ntvm.target.datatype.register_op(\n    tvm.target.datatype.create_lower_func({32: \"Custom32Mul\"}),\n    \"Mul\",\n    \"llvm\",\n    \"myfloat\",\n)\ntvm.target.datatype.register_op(\n    tvm.target.datatype.create_lower_func({32: \"Custom32Div\"}),\n    \"Div\",\n    \"llvm\",\n    \"myfloat\",\n)\n\ntvm.target.datatype.register_op(\n    tvm.target.datatype.create_lower_func({32: \"Custom32Sqrt\"}),\n    \"Call\",\n    \"llvm\",\n    \"myfloat\",\n    intrinsic_name=\"tir.sqrt\",\n)\n\ntvm.target.datatype.register_op(\n    tvm.target.datatype.create_lower_func({32: \"Custom32Sub\"}),\n    \"Sub\",\n    \"llvm\",\n    \"myfloat\",\n)\n\ntvm.target.datatype.register_op(\n    tvm.target.datatype.create_lower_func({32: \"Custom32Exp\"}),\n    \"Call\",\n    \"llvm\",\n    \"myfloat\",\n    intrinsic_name=\"tir.exp\",\n)\n\ntvm.target.datatype.register_op(\n    tvm.target.datatype.create_lower_func({32: \"Custom32Max\"}),\n    \"Max\",\n    \"llvm\",\n    \"myfloat\",\n)\n\ntvm.target.datatype.register_min_func(\n    tvm.target.datatype.create_min_lower_func({32: \"MinCustom32\"}, \"myfloat\"),\n    \"myfloat\",\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Note we are making use of two new functions: `register_min_func` and\n`create_min_lower_func`.\n\n`register_min_func` takes in an integer `num_bits` for the bit length,\nand should return an operation representing the minimum finite\nrepresentable value for the custom data type with the specified bit\nlength.\n\nSimilar to `register_op` and `create_lower_func`, the\n`create_min_lower_func` handles the general case where the minimum\nrepresentable custom datatype value is implemented using calls to an\nexternal library.\n\nNow we can finally run the model:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Vectorization is not implemented with custom datatypes.\nwith tvm.transform.PassContext(config={\"tir.disable_vectorize\": True}):\n    result_myfloat = relay.create_executor(mod=module).evaluate(expr)(input, **params)\n    result_myfloat = convert_ndarray(src_dtype, result_myfloat).numpy()\n    # print first 10 elements\n    print(result_myfloat.flatten()[:10])\n\n# Again, note that the output using 32-bit myfloat exactly the same as 32-bit floats,\n# because myfloat is exactly a float!\nnp.testing.assert_array_equal(result, result_myfloat)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}