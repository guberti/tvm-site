{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%%bash\n# Installs the latest dev build of TVM from PyPI, with CUDA enabled. To use this,\n# you must request a Google Colab instance with a GPU by going to Runtime ->\n# Change runtime type -> Hardware accelerator -> GPU. If you wish to build from\n# source, see see https://tvm.apache.org/docs/install/from_source.html\npip install tlcpack-nightly-cu113 --pre -f https://tlcpack.ai/wheels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Quick Start Tutorial for Compiling Deep Learning Models {#tutorial-relay-quick-start}\n=======================================================\n\n**Author**: [Yao Wang](https://github.com/kevinthesun), [Truman\nTian](https://github.com/SiNZeRo)\n\nThis example shows how to build a neural network with Relay python\nfrontend and generates a runtime library for Nvidia GPU with TVM. Notice\nthat you need to build TVM with cuda and llvm enabled.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Overview for Supported Hardware Backend of TVM\n==============================================\n\nThe image below shows hardware backend currently supported by TVM:\n\n![image](https://github.com/dmlc/web-data/raw/main/tvm/tutorial/tvm_support_list.png){.align-center}\n\nIn this tutorial, we\\'ll choose cuda and llvm as target backends. To\nbegin with, let\\'s import Relay and TVM.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import numpy as np\n\nfrom tvm import relay\nfrom tvm.relay import testing\nimport tvm\nfrom tvm import te\nfrom tvm.contrib import graph_executor\nimport tvm.testing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Define Neural Network in Relay\n==============================\n\nFirst, let\\'s define a neural network with relay python frontend. For\nsimplicity, we\\'ll use pre-defined resnet-18 network in Relay.\nParameters are initialized with Xavier initializer. Relay also supports\nother model formats such as MXNet, CoreML, ONNX and Tensorflow.\n\nIn this tutorial, we assume we will do inference on our device and the\nbatch size is set to be 1. Input images are RGB color images of size 224\n\\* 224. We can call the\n:py`tvm.relay.expr.TupleWrapper.astext()`{.interpreted-text role=\"meth\"}\nto show the network structure.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "batch_size = 1\nnum_class = 1000\nimage_shape = (3, 224, 224)\ndata_shape = (batch_size,) + image_shape\nout_shape = (batch_size, num_class)\n\nmod, params = relay.testing.resnet.get_workload(\n    num_layers=18, batch_size=batch_size, image_shape=image_shape\n)\n\n# set show_meta_data=True if you want to show meta data\nprint(mod.astext(show_meta_data=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Compilation\n===========\n\nNext step is to compile the model using the Relay/TVM pipeline. Users\ncan specify the optimization level of the compilation. Currently this\nvalue can be 0 to 3. The optimization passes include operator fusion,\npre-computation, layout transformation and so on.\n\n:py`relay.build`{.interpreted-text role=\"func\"} returns three\ncomponents: the execution graph in json format, the TVM module library\nof compiled functions specifically for this graph on the target\nhardware, and the parameter blobs of the model. During the compilation,\nRelay does the graph-level optimization while TVM does the tensor-level\noptimization, resulting in an optimized runtime module for model\nserving.\n\nWe\\'ll first compile for Nvidia GPU. Behind the scene,\n:py`relay.build`{.interpreted-text role=\"func\"} first does a number of\ngraph-level optimizations, e.g. pruning, fusing, etc., then registers\nthe operators (i.e. the nodes of the optimized graphs) to TVM\nimplementations to generate a [tvm.module]{.title-ref}. To generate the\nmodule library, TVM will first transfer the high level IR into the lower\nintrinsic IR of the specified target backend, which is CUDA in this\nexample. Then the machine code will be generated as the module library.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "opt_level = 3\ntarget = tvm.target.cuda()\nwith tvm.transform.PassContext(opt_level=opt_level):\n    lib = relay.build(mod, target, params=params)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Run the generate library\n========================\n\nNow we can create graph executor and run the module on Nvidia GPU.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# create random input\ndev = tvm.cuda()\ndata = np.random.uniform(-1, 1, size=data_shape).astype(\"float32\")\n# create module\nmodule = graph_executor.GraphModule(lib[\"default\"](dev))\n# set input and parameters\nmodule.set_input(\"data\", data)\n# run\nmodule.run()\n# get output\nout = module.get_output(0, tvm.nd.empty(out_shape)).numpy()\n\n# Print first 10 elements of output\nprint(out.flatten()[0:10])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Save and Load Compiled Module\n=============================\n\nWe can also save the graph, lib and parameters into files and load them\nback in deploy environment.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# save the graph, lib and params into separate files\nfrom tvm.contrib import utils\n\ntemp = utils.tempdir()\npath_lib = temp.relpath(\"deploy_lib.tar\")\nlib.export_library(path_lib)\nprint(temp.listdir())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# load the module back.\nloaded_lib = tvm.runtime.load_module(path_lib)\ninput_data = tvm.nd.array(data)\n\nmodule = graph_executor.GraphModule(loaded_lib[\"default\"](dev))\nmodule.run(data=input_data)\nout_deploy = module.get_output(0).numpy()\n\n# Print first 10 elements of output\nprint(out_deploy.flatten()[0:10])\n\n# check whether the output from deployed module is consistent with original one\ntvm.testing.assert_allclose(out_deploy, out, atol=1e-5)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}