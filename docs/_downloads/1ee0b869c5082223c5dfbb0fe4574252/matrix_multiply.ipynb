{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%%bash\n# Installs the latest dev build of TVM from PyPI. If you wish to build\n# from source, see https://tvm.apache.org/docs/install/from_source.html\npip install apache-tvm --pre"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Simple Matrix Multiply {#basic-mat-mult}\n======================\n\n**Author**: [Thierry Moreau](https://homes.cs.washington.edu/~moreau/)\n\nIn this tutorial, we will build on top of the\n`vta-get-started`{.interpreted-text role=\"ref\"} tutorial and introduce\nadditional concepts required to implement matrix multiplication on VTA\nwith the TVM workflow.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "RPC Setup\n=========\n\nWe start by programming the Pynq\\'s FPGA and building its RPC runtime as\nwe did in the VTA introductory tutorial.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from __future__ import absolute_import, print_function\n\nimport os\nimport tvm\nfrom tvm import te\nimport vta\nimport numpy as np\nfrom tvm import rpc\nfrom tvm.contrib import utils\nfrom vta.testing import simulator\n\n# Load VTA parameters from the 3rdparty/vta-hw/config/vta_config.json file\nenv = vta.get_env()\n\n# We read the Pynq RPC host IP address and port number from the OS environment\nhost = os.environ.get(\"VTA_RPC_HOST\", \"192.168.2.99\")\nport = int(os.environ.get(\"VTA_RPC_PORT\", \"9091\"))\n\n# We configure both the bitstream and the runtime system on the Pynq\n# to match the VTA configuration specified by the vta_config.json file.\nif env.TARGET == \"pynq\" or env.TARGET == \"de10nano\":\n\n    # Make sure that TVM was compiled with RPC=1\n    assert tvm.runtime.enabled(\"rpc\")\n    remote = rpc.connect(host, port)\n\n    # Reconfigure the JIT runtime\n    vta.reconfig_runtime(remote)\n\n    # Program the FPGA with a pre-compiled VTA bitstream.\n    # You can program the FPGA with your own custom bitstream\n    # by passing the path to the bitstream file instead of None.\n    vta.program_fpga(remote, bitstream=None)\n\n# In simulation mode, host the RPC server locally.\nelif env.TARGET in [\"sim\", \"tsim\"]:\n    remote = rpc.LocalSession()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Computation Declaration\n=======================\n\nIn this example we describe a simple matrix multiplication addition,\nwhich requires multiple computation stages, as shown in the dataflow\ndiagram below. First we describe the input tensors `A`{.sourceCode} and\n`B`{.sourceCode} that are living in main memory. Second, we need to\ndeclare intermediate tensors `A_buf`{.sourceCode} and\n`B_buf`{.sourceCode}, which will live in VTA\\'s on-chip buffers. Having\nthis extra computational stage allows us to explicitly stage cached\nreads and writes. Third, we describe the matrix multiplication\ncomputation over `A_buf`{.sourceCode} and `B_buf`{.sourceCode} to\nproduce the product matrix `C_buf`{.sourceCode}. The last operation is a\ncast and copy back to DRAM, into results tensor `C`{.sourceCode}.\n\n![image](https://raw.githubusercontent.com/uwsampl/web-data/main/vta/tutorial/gemm_dataflow.png){.align-center}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Data Layout\n===========\n\nWe describe the placeholder tensors `A`{.sourceCode}, and\n`B`{.sourceCode} in a tiled data format to match the data layout\nrequirements imposed by the VTA tensor core.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "::: {.note}\n::: {.admonition-title}\nNote\n:::\n\n**Data Tiling**\n\nOne source of complexity when targeting accelerators is to make sure\nthat the data layout matches the layout imposed by the accelerator\ndesign. VTA is designed around a *tensor core* that performs, one\nmatrix-matrix operation per cycle between an activation matrix and a\nweight matrix, adding the result matrix to an accumulator matrix, as\nshown in the figure below.\n\n![image](https://raw.githubusercontent.com/uwsampl/web-data/main/vta/tutorial/tensor_core.png){.align-center\nwidth=\"480px\"}\n\nThe dimensions of that matrix-matrix multiplication are specified in the\n`vta_config.json`{.sourceCode} configuration file. The activation matrix\nhas a `(BATCH, BLOCK_IN)`{.sourceCode} shape and the transposed weight\nmatrix has a `(BLOCK_OUT, BLOCK_IN)`{.sourceCode} shape, thus inferring\nthat the resulting output matrix has a `(BATCH, BLOCK_OUT)`{.sourceCode}\nshape. Consequently input and output tensors processed by VTA need to be\ntiled according to these aforementioned dimension.\n\nThe diagram below shows the impact of data tiling on a matrix that is\noriginally of shape (4, 8). Tiling by a (2, 2) tile shape ensures that\ndata within each tile is contiguous. The resulting tiled tensor has a\nshape of (2, 4, 2, 2).\n\n![image](https://raw.githubusercontent.com/uwsampl/web-data/main/vta/tutorial/data_tiling.png){.align-center\nwidth=\"480px\"}\n:::\n\nWe first define the variables `m`{.sourceCode}, `n`{.sourceCode},\n`o`{.sourceCode} to represent the shape of the matrix multiplication.\nThese variables are multiplicative factors over the\n`BLOCK_OUT`{.sourceCode}, `BLOCK_IN`{.sourceCode}, and\n`BATCH`{.sourceCode} tensor dimensions respectively. By default, the\nconfiguration file sets `BATCH`{.sourceCode}, `BLOCK_IN`{.sourceCode},\nand `BLOCK_OUT`{.sourceCode} to be 1, 16 and 16 respectively\n(`BATCH`{.sourceCode} being set to 1 implies that our compute building\nblock is vector-matrix multiply).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "::: {.note}\n::: {.admonition-title}\nNote\n:::\n\n**Data Types**\n\nIt\\'s important to not only match the inner-tile dimension of VTA\\'s\ntensor core, but also to match the specific data types expected by VTA.\nVTA for now only supports fixed point data types, which integer width is\nspecified in the `vta_config.json`{.sourceCode} file by\n`INP_WIDTH`{.sourceCode} and `WGT_WIDTH`{.sourceCode} for the\nactivations and weights data types respectively. In addition, the\naccumulator data type integer width is specified by\n`ACC_WIDTH`{.sourceCode}.\n:::\n\nBy default, the configuration file sets `INP_WIDTH`{.sourceCode} and\n`WGT_WIDTH`{.sourceCode} to 8. The accumulator width\n`ACC_WIDTH`{.sourceCode} is set to 32, in order to avoid overflow during\naccumulation. As a result, `env.inp_dtype`{.sourceCode} and\n`env.wgt_dtype`{.sourceCode} are all narrow 8-bit integers, while\n`env.acc_dtype`{.sourceCode} is a standard 32-bit integer.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Output channel factor m - total 16x16=256 output channels\nm = 16\n# Input channel factor n - total 16x16=256 input channels\nn = 16\n# Batch factor o (we use single batch inference)\no = 1\n# A placeholder tensor in tiled data format\nA = te.placeholder((o, n, env.BATCH, env.BLOCK_IN), name=\"A\", dtype=env.inp_dtype)\n# B placeholder tensor in tiled data format\nB = te.placeholder((m, n, env.BLOCK_OUT, env.BLOCK_IN), name=\"B\", dtype=env.wgt_dtype)\n# A copy buffer\nA_buf = te.compute((o, n, env.BATCH, env.BLOCK_IN), lambda *i: A(*i), \"A_buf\")\n# B copy buffer\nB_buf = te.compute((m, n, env.BLOCK_OUT, env.BLOCK_IN), lambda *i: B(*i), \"B_buf\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Matrix Multiplication\n=====================\n\nNow we\\'re ready to describe the matrix multiplication result tensor\n`C`{.sourceCode}, with another compute operation. The compute function\ntakes the shape of the tensor, as well as a lambda function that\ndescribes the computation rule for each position of the tensor.\n\nIn order to implement matrix multiplication, the lambda function needs\nto include a reduction formula over the input channel dimension axes. To\ncreate a reduction formula, we can declare a reduction axis using\n`te.reduce_axis`{.sourceCode}, which takes in the range of reductions.\n`te.sum`{.sourceCode} takes in the expression to be reduced as well as\nthe reduction axes to compute the sum of value over all k in the\ndeclared ranges.\n\nNote that the reduction needs to be performed over 32-bit\n`env.acc_dtype`{.sourceCode} accumulator data types.\n\nNo computation happens during this phase, as we are only declaring how\nthe computation should be done.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Outer input feature reduction axis\nko = te.reduce_axis((0, n), name=\"ko\")\n# Inner input feature reduction axis\nki = te.reduce_axis((0, env.BLOCK_IN), name=\"ki\")\n# Describe the in-VTA matrix multiplication\nC_buf = te.compute(\n    (o, m, env.BATCH, env.BLOCK_OUT),\n    lambda bo, co, bi, ci: te.sum(\n        A_buf[bo, ko, bi, ki].astype(env.acc_dtype) * B_buf[co, ko, ci, ki].astype(env.acc_dtype),\n        axis=[ko, ki],\n    ),\n    name=\"C_buf\",\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Casting the Results\n===================\n\nAfter the computation is done, we\\'ll need to send the results computed\nby VTA back to main memory.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "::: {.note}\n::: {.admonition-title}\nNote\n:::\n\n**Memory Store Restrictions**\n\nOne specificity of VTA is that it only supports DRAM stores in the\nnarrow `env.inp_dtype`{.sourceCode} data type format. This lets us\nreduce the data footprint for memory transfers, but also lets us\nquantize the wide accumulator data type down to a data format that\nmatches the input activation data type. This means that in the context\nof neural network inference, the outputs of a given layer after\nactivation can be consumed directly by the next layer.\n:::\n\nWe perform one last typecast operation to the narrow input activation\ndata format.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Cast to output type, and send to main memory\nC = te.compute(\n    (o, m, env.BATCH, env.BLOCK_OUT), lambda *i: C_buf(*i).astype(env.inp_dtype), name=\"C\"\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This concludes the computation declaration part of this tutorial.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Scheduling the Computation\n==========================\n\nWhile the above lines describes the computation rule, we can obtain\n`C`{.sourceCode} in many ways. TVM asks the user to provide an\nimplementation of the computation called *schedule*.\n\nA schedule is a set of transformations to an original computation that\ntransforms the implementation of the computation without affecting\ncorrectness. This simple VTA programming tutorial aims to demonstrate\nbasic schedule transformations that will map the original schedule down\nto VTA hardware primitives.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Default Schedule\n================\n\nAfter we construct the schedule, by default the schedule computes\n`C`{.sourceCode} in the following way:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Let's take a look at the generated schedule\ns = te.create_schedule(C.op)\nprint(tvm.lower(s, [A, B, C], simple_mode=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Although this schedule makes sense, it won\\'t compile to VTA. In order\nto obtain correct code generation, we need to apply scheduling\nprimitives and code annotation that will transform the schedule into one\nthat can be directly lowered onto VTA hardware intrinsics. Those\ninclude:\n\n> -   DMA copy operations which will take globally-scoped tensors and\n>     copy those into locally-scoped tensors.\n> -   Tensor operations that will perform the matrix multiplication.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Buffer Scopes\n=============\n\nFirst, we set the scope of the buffers to tell TVM that these buffers\nwill be living in the VTA\\'s on-chip SRAM caches. Below, we tell TVM\nthat `A_buf`{.sourceCode}, `B_buf`{.sourceCode}, `C_buf`{.sourceCode}\nwill respectively live in VTA\\'s on-chip input, weight and accumulator\nmemory.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "::: {.note}\n::: {.admonition-title}\nNote\n:::\n\n**VTA\\'s On-Chip SRAMs**\n\nVTA has three different memory scopes, each corresponding to different\non-chip SRAM buffers.\n\n> -   `env.inp_scope`{.sourceCode}: Input buffer, which is a read-only\n>     SRAM buffer that stores input matrices of shape\n>     `(env.BATCH, env.BLOCK_IN)`{.sourceCode} of type\n>     `env.inp_dtype`{.sourceCode}. The input buffer contains [2 \\^\n>     LOG\\_INP\\_BUFF\\_SIZE]{.title-ref} matrix elements (as specified in\n>     the `vta_config.json`{.sourceCode} file).\n> -   `env.wgt_scope`{.sourceCode}: Weight buffer, which is a read-only\n>     SRAM buffer that stores weight matrices of shape\n>     `(env.BLOCK_OUT, env.BLOCK_IN)`{.sourceCode} of type\n>     `env.wgt_dtype`{.sourceCode}. The weight buffer contains [2 \\^\n>     LOG\\_WGT\\_BUFF\\_SIZE]{.title-ref} matrix elements.\n> -   `env.acc_scope`{.sourceCode}: Accumulator buffer, which is a\n>     read/write SRAM buffer that stores accumulator matrices of shape\n>     `(env.BATCH, env.BLOCK_OUT)`{.sourceCode} of type\n>     `env.acc_dtype`{.sourceCode}. The accumulator buffer is VTA\\'s\n>     general purpose register file: it holds both intermediate results\n>     of convolutions and matrix multiplications as well as intermediate\n>     results of pooling, batch normalization, and activation layers.\n>     The accumulator buffer contains [2 \\^\n>     LOG\\_ACC\\_BUFF\\_SIZE]{.title-ref} matrix elements.\n:::\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Set the intermediate tensor's scope to VTA's on-chip buffers\ns[A_buf].set_scope(env.inp_scope)\ns[B_buf].set_scope(env.wgt_scope)\ns[C_buf].set_scope(env.acc_scope)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "DMA Transfers\n=============\n\nWe need to schedule DMA transfers to move data living in DRAM to and\nfrom the VTA on-chip buffers. This can be achieved using the\n`compute_at`{.sourceCode} schedule primitive which nests the copying of\nthe buffers into the computation loop that performs the matrix\nmultiplication.\n\nWe insert `dma_copy`{.sourceCode} pragmas to indicate to the compiler\nthat the copy operations will be performed in bulk via DMA, which is\ncommon in hardware accelerators. Finally, we print the temporary\nschedule to observe the effects of moving the copy operations into the\nmatrix multiplication loop.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Move buffer copy into matrix multiply loop\ns[A_buf].compute_at(s[C_buf], ko)\ns[B_buf].compute_at(s[C_buf], ko)\n\n# Tag the buffer copies with the DMA pragma to insert a DMA transfer\ns[A_buf].pragma(s[A_buf].op.axis[0], env.dma_copy)\ns[B_buf].pragma(s[B_buf].op.axis[0], env.dma_copy)\ns[C].pragma(s[C].op.axis[0], env.dma_copy)\n\n# Let's take a look at the transformed schedule\nprint(tvm.lower(s, [A, B, C], simple_mode=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Tensorization\n=============\n\nThe last step of the schedule transformation consists in applying\n*tensorization* to our schedule. Tensorization is analogous to\nvectorization, but extends the concept to a higher-dimensional unit of\ncomputation. Consequently, tensorization imposes data layout constraints\nas discussed earlier when declaring the data layout input placeholders.\nWe\\'ve already arranged our tensors in a tiled format, so the next thing\nwe need to perform is loop reordering to accommodate for tensorization.\n\nHere we choose to move the outermost reduction axis all the way out.\nThis dictates that we first iterate over input channels, then batch\ndimensions, and finally output channels. Lastly, we apply the\ntensorization scheduling primitive `tensorize`{.sourceCode} along the\nouter axis of the inner-most matrix matrix multiplication tensor block.\nWe print the finalized schedule that is ready for code-generation by the\nVTA runtime JIT compiler.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "s[C_buf].reorder(\n    ko, s[C_buf].op.axis[0], s[C_buf].op.axis[1], s[C_buf].op.axis[2], s[C_buf].op.axis[3], ki\n)\ns[C_buf].tensorize(s[C_buf].op.axis[2], env.gemm)\n\n# Let's take a look at the finalized schedule\nprint(vta.lower(s, [A, B, C], simple_mode=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This concludes the scheduling portion of this tutorial.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "TVM Compilation\n===============\n\nAfter we have finished specifying the schedule, we can compile it into a\nTVM function.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Build GEMM VTA kernel\nmy_gemm = vta.build(\n    s, [A, B, C], tvm.target.Target(\"ext_dev\", host=env.target_host), name=\"my_gemm\"\n)\n\n# Write the compiled module into an object file.\ntemp = utils.tempdir()\nmy_gemm.save(temp.relpath(\"gemm.o\"))\n\n# Send the executable over RPC\nremote.upload(temp.relpath(\"gemm.o\"))\n\n# Load the compiled module\nf = remote.load_module(\"gemm.o\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Running the Function\n====================\n\nThe compiled TVM function uses a concise C API and can be invoked from\ncode language.\n\nTVM provides an array API in python to aid quick testing and\nprototyping. The array API is based on\n[DLPack](https://github.com/dmlc/dlpack) standard.\n\n-   We first create a remote context (for remote execution on the Pynq).\n-   Then `tvm.nd.array`{.sourceCode} formats the data accordingly.\n-   `f()`{.sourceCode} runs the actual computation.\n-   `numpy()`{.sourceCode} copies the result array back in a format that\n    can be interpreted.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Get the remote device context\nctx = remote.ext_dev(0)\n\n# Initialize the A and B arrays randomly in the int range of (-128, 128]\nA_orig = np.random.randint(-128, 128, size=(o * env.BATCH, n * env.BLOCK_IN)).astype(A.dtype)\nB_orig = np.random.randint(-128, 128, size=(m * env.BLOCK_OUT, n * env.BLOCK_IN)).astype(B.dtype)\n\n# Apply packing to the A and B arrays from a 2D to a 4D packed layout\nA_packed = A_orig.reshape(o, env.BATCH, n, env.BLOCK_IN).transpose((0, 2, 1, 3))\nB_packed = B_orig.reshape(m, env.BLOCK_OUT, n, env.BLOCK_IN).transpose((0, 2, 1, 3))\n\n# Format the input/output arrays with tvm.nd.array to the DLPack standard\nA_nd = tvm.nd.array(A_packed, ctx)\nB_nd = tvm.nd.array(B_packed, ctx)\nC_nd = tvm.nd.array(np.zeros((o, m, env.BATCH, env.BLOCK_OUT)).astype(C.dtype), ctx)\n\n# Clear stats\nif env.TARGET in [\"sim\", \"tsim\"]:\n    simulator.clear_stats()\n\n# Invoke the module to perform the computation\nf(A_nd, B_nd, C_nd)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Verifying Correctness\n=====================\n\nCompute the reference result with numpy and assert that the output of\nthe matrix multiplication indeed is correct\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Compute reference result with numpy\nC_ref = np.dot(A_orig.astype(env.acc_dtype), B_orig.T.astype(env.acc_dtype)).astype(C.dtype)\nC_ref = C_ref.reshape(o, env.BATCH, m, env.BLOCK_OUT).transpose((0, 2, 1, 3))\nnp.testing.assert_equal(C_ref, C_nd.numpy())\n\n# Print stats\nif env.TARGET in [\"sim\", \"tsim\"]:\n    sim_stats = simulator.stats()\n    print(\"Execution statistics:\")\n    for k, v in sim_stats.items():\n        print(\"\\t{:<16}: {:>16}\".format(k, v))\n\nprint(\"Successful matrix multiply test!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Summary\n=======\n\nThis tutorial showcases the TVM workflow to implement a simple matrix\nmultiplication example on VTA. The general workflow includes:\n\n-   Programming the FPGA with the VTA bitstream over RPC.\n-   Describing matrix multiplication via a series of computations.\n-   Describing how we want to perform the computation using schedule\n    primitives.\n-   Compiling the function to the VTA target.\n-   Running the compiled module and verifying it against a numpy\n    implementation.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}