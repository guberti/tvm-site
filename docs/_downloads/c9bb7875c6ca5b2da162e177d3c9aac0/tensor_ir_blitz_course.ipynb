{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%%bash\n# Installs the latest dev build of TVM from PyPI, with CUDA enabled. To use this,\n# you must request a Google Colab instance with a GPU by going to Runtime ->\n# Change runtime type -> Hardware accelerator -> GPU. If you wish to build from\n# source, see see https://tvm.apache.org/docs/install/from_source.html\npip install tlcpack-nightly-cu113 --pre -f https://tlcpack.ai/wheels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Blitz Course to TensorIR {#tir_blitz}\n========================\n\n**Author**: [Siyuan Feng](https://github.com/Hzfengsy)\n\nTensorIR is a domain specific language for deep learning programs\nserving two broad purposes:\n\n-   An implementation for transforming and optimizing programs on\n    various hardware backends.\n-   An abstraction for automatic \\_[tensorized]() program optimization.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import tvm\nfrom tvm.ir.module import IRModule\nfrom tvm.script import tir as T\nimport numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "IRModule\n========\n\nAn IRModule is the central data structure in TVM, which contains deep\nlearning programs. It is the basic object of interest of IR\ntransformation and model building.\n\n![image](https://raw.githubusercontent.com/tlc-pack/web-data/main/images/design/tvm_life_of_irmodule.png){.align-center\nwidth=\"85.0%\"}\n\nThis is the life cycle of an IRModule, which can be created from\nTVMScript. TensorIR schedule primitives and passes are two major ways to\ntransform an IRModule. Also, a sequence of transformations on an\nIRModule is acceptable. Note that we can print an IRModule at **ANY**\nstage to TVMScript. After all transformations and optimizations are\ncomplete, we can build the IRModule to a runnable module to deploy on\ntarget devices.\n\nBased on the design of TensorIR and IRModule, we are able to create a\nnew programming method:\n\n1.  Write a program by TVMScript in a python-AST based syntax.\n2.  Transform and optimize a program with python api.\n3.  Interactively inspect and try the performance with an imperative\n    style transformation API.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Create an IRModule\n==================\n\nIRModule can be created by writing TVMScript, which is a round-trippable\nsyntax for TVM IR.\n\nDifferent than creating a computational expression by Tensor Expression\n(`tutorial-tensor-expr-get-started`{.interpreted-text role=\"ref\"}),\nTensorIR allow users to program through TVMScript, a language embedded\nin python AST. The new method makes it possible to write complex\nprograms and further schedule and optimize it.\n\nFollowing is a simple example for vector addition.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "@tvm.script.ir_module\nclass MyModule:\n    @T.prim_func\n    def main(a: T.handle, b: T.handle):\n        # We exchange data between function by handles, which are similar to pointer.\n        T.func_attr({\"global_symbol\": \"main\", \"tir.noalias\": True})\n        # Create buffer from handles.\n        A = T.match_buffer(a, (8,), dtype=\"float32\")\n        B = T.match_buffer(b, (8,), dtype=\"float32\")\n        for i in range(8):\n            # A block is an abstraction for computation.\n            with T.block(\"B\"):\n                # Define a spatial block iterator and bind it to value i.\n                vi = T.axis.spatial(8, i)\n                B[vi] = A[vi] + 1.0\n\n\nir_module = MyModule\nprint(type(ir_module))\nprint(ir_module.script())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Besides, we can also use tensor expression DSL to write simple\noperators, and convert them to an IRModule.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from tvm import te\n\nA = te.placeholder((8,), dtype=\"float32\", name=\"A\")\nB = te.compute((8,), lambda *i: A(*i) + 1.0, name=\"B\")\nfunc = te.create_prim_func([A, B])\nir_module_from_te = IRModule({\"main\": func})\nprint(ir_module_from_te.script())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Build and Run an IRModule\n=========================\n\nWe can build the IRModule into a runnable module with specific target\nbackends.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "mod = tvm.build(ir_module, target=\"llvm\")  # The module for CPU backends.\nprint(type(mod))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Prepare the input array and output array, then run the module.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "a = tvm.nd.array(np.arange(8).astype(\"float32\"))\nb = tvm.nd.array(np.zeros((8,)).astype(\"float32\"))\nmod(a, b)\nprint(a)\nprint(b)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Transform an IRModule\n=====================\n\nThe IRModule is the central data structure for program optimization,\nwhich can be transformed by `Schedule`{.sourceCode}. A schedule contains\nmultiple primitive methods to interactively transform the program. Each\nprimitive transforms the program in certain ways to bring additional\nperformance optimizations.\n\n![image](https://raw.githubusercontent.com/tlc-pack/web-data/main/images/design/tvm_tensor_ir_opt_flow.png){.align-center\nwidth=\"100.0%\"}\n\nThe image above is a typical workflow for optimizing a tensor program.\nFirst, we need to create a schedule on the initial IRModule created from\neither TVMScript or Tensor Expression. Then, a sequence of schedule\nprimitives will help to improve the performance. And at last, we can\nlower and build it into a runnable module.\n\nHere we just demonstrate a very simple transformation. First we create\nschedule on the input [ir\\_module]{.title-ref}.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "sch = tvm.tir.Schedule(ir_module)\nprint(type(sch))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Tile the loop into 3 loops and print the result.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Get block by its name\nblock_b = sch.get_block(\"B\")\n# Get loops surrounding the block\n(i,) = sch.get_loops(block_b)\n# Tile the loop nesting.\ni_0, i_1, i_2 = sch.split(i, factors=[2, 2, 2])\nprint(sch.mod.script())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can also reorder the loops. Now we move loop [i\\_2]{.title-ref} to\noutside of [i\\_1]{.title-ref}.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "sch.reorder(i_0, i_2, i_1)\nprint(sch.mod.script())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Transform to a GPU program\n==========================\n\nIf we want to deploy models on GPUs, thread binding is necessary.\nFortunately, we can also use primitives and do incrementally\ntransformation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "sch.bind(i_0, \"blockIdx.x\")\nsch.bind(i_2, \"threadIdx.x\")\nprint(sch.mod.script())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "After binding the threads, now build the IRModule with\n`cuda`{.sourceCode} backends.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "ctx = tvm.cuda(0)\ncuda_mod = tvm.build(sch.mod, target=\"cuda\")\ncuda_a = tvm.nd.array(np.arange(8).astype(\"float32\"), ctx)\ncuda_b = tvm.nd.array(np.zeros((8,)).astype(\"float32\"), ctx)\ncuda_mod(cuda_a, cuda_b)\nprint(cuda_a)\nprint(cuda_b)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}